{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f16429ef",
   "metadata": {},
   "source": [
    "# Data functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6185885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d9959ba",
   "metadata": {
    "code_folding": [
     9,
     23,
     43
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "# !pip install pytorch_transformers\n",
    "from pytorch_transformers import BertTokenizer\n",
    "\n",
    "\n",
    "def pad_and_truncate(sequence, maxlen, dtype='int64', padding='post', truncating='post', value=0):\n",
    "    x = (np.ones(maxlen) * value).astype(dtype)\n",
    "    if truncating == 'pre':\n",
    "        trunc = sequence[-maxlen:]\n",
    "    else:\n",
    "        trunc = sequence[:maxlen]\n",
    "    trunc = np.asarray(trunc, dtype=dtype)\n",
    "    if padding == 'post':\n",
    "        x[:len(trunc)] = trunc\n",
    "    else:\n",
    "        x[-len(trunc):] = trunc\n",
    "    return x\n",
    "\n",
    "\n",
    "class Tokenizer4Bert:\n",
    "    def __init__(self, max_seq_len, pretrained_bert_name):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(pretrained_bert_name)\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def text_to_sequence(self, text, reverse=False, padding='post', truncating='post'):\n",
    "        sequence = self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(text))\n",
    "        if len(sequence) == 0:\n",
    "            sequence = [0]\n",
    "        if reverse:\n",
    "            sequence = sequence[::-1]\n",
    "        return pad_and_truncate(sequence, self.max_seq_len, padding=padding, truncating=truncating)\n",
    "\n",
    "    def id_to_sequence(self, sequence, reverse=False, padding='post', truncating='post'):\n",
    "        if len(sequence) == 0:\n",
    "            sequence = [0]\n",
    "        if reverse:\n",
    "            sequence = sequence[::-1]\n",
    "        return pad_and_truncate(sequence, self.max_seq_len, padding=padding, truncating=truncating)\n",
    "\n",
    "class DepInstanceParser():\n",
    "    def __init__(self, basicDependencies, tokens):\n",
    "        self.basicDependencies = basicDependencies\n",
    "        self.tokens = tokens\n",
    "        self.words = []\n",
    "        self.dep_governed_info = []\n",
    "        self.dep_parsing()\n",
    "\n",
    "\n",
    "    def dep_parsing(self):\n",
    "        if len(self.tokens) > 0:\n",
    "            words = []\n",
    "            for token in self.tokens:\n",
    "                token['word'] = token\n",
    "                words.append(self.change_word(token['word']))\n",
    "            dep_governed_info = [\n",
    "                {\"word\": word}\n",
    "                for i,word in enumerate(words)\n",
    "            ]\n",
    "            self.words = words\n",
    "        else:\n",
    "            dep_governed_info = [{}] * len(self.basicDependencies)\n",
    "        for dep in self.basicDependencies:\n",
    "            dependent_index = dep['dependent'] - 1\n",
    "            governed_index = dep['governor'] - 1\n",
    "            dep_governed_info[dependent_index] = {\n",
    "                \"governor\": governed_index,\n",
    "                \"dep\": dep['dep']\n",
    "            }\n",
    "        self.dep_governed_info = dep_governed_info\n",
    "\n",
    "    def change_word(self, word):\n",
    "        if \"-RRB-\" in word:\n",
    "            return word.replace(\"-RRB-\", \")\")\n",
    "        if \"-LRB-\" in word:\n",
    "            return word.replace(\"-LRB-\", \"(\")\n",
    "        return word\n",
    "\n",
    "    def get_first_order(self, direct=False):\n",
    "        dep_adj_matrix  = [[0] * len(self.dep_governed_info) for _ in range(len(self.dep_governed_info))]\n",
    "        dep_type_matrix = [[\"none\"] * len(self.dep_governed_info) for _ in range(len(self.dep_governed_info))]\n",
    "        # for i in range(len(self.dep_governed_info)):\n",
    "        #     dep_adj_matrix[i][i]  = 1\n",
    "        #     dep_type_matrix[i][i] = \"self_loop\"\n",
    "        for i, dep_info in enumerate(self.dep_governed_info):\n",
    "            governor = dep_info[\"governor\"]\n",
    "            dep_type = dep_info[\"dep\"]\n",
    "            dep_adj_matrix[i][governor] = 1\n",
    "            dep_adj_matrix[governor][i] = 1\n",
    "            dep_type_matrix[i][governor] = dep_type if direct is False else \"{}_in\".format(dep_type)\n",
    "            dep_type_matrix[governor][i] = dep_type if direct is False else \"{}_out\".format(dep_type)\n",
    "        return dep_adj_matrix, dep_type_matrix\n",
    "\n",
    "    def get_next_order(self, dep_adj_matrix, dep_type_matrix):\n",
    "        new_dep_adj_matrix = copy.deepcopy(dep_adj_matrix)\n",
    "        new_dep_type_matrix = copy.deepcopy(dep_type_matrix)\n",
    "        for target_index in range(len(dep_adj_matrix)):\n",
    "            for first_order_index in range(len(dep_adj_matrix[target_index])):\n",
    "                if dep_adj_matrix[target_index][first_order_index] == 0:\n",
    "                    continue\n",
    "                for second_order_index in range(len(dep_adj_matrix[first_order_index])):\n",
    "                    if dep_adj_matrix[first_order_index][second_order_index] == 0:\n",
    "                        continue\n",
    "                    if second_order_index == target_index:\n",
    "                        continue\n",
    "                    if new_dep_adj_matrix[target_index][second_order_index] == 1:\n",
    "                        continue\n",
    "                    new_dep_adj_matrix[target_index][second_order_index] = 1\n",
    "                    new_dep_type_matrix[target_index][second_order_index] = dep_type_matrix[first_order_index][second_order_index]\n",
    "        return new_dep_adj_matrix, new_dep_type_matrix\n",
    "\n",
    "    def get_second_order(self, direct=False):\n",
    "        dep_adj_matrix, dep_type_matrix = self.get_first_order(direct=direct)\n",
    "        return self.get_next_order(dep_adj_matrix, dep_type_matrix)\n",
    "\n",
    "    def get_third_order(self, direct=False):\n",
    "        dep_adj_matrix, dep_type_matrix = self.get_second_order(direct=direct)\n",
    "        return self.get_next_order(dep_adj_matrix, dep_type_matrix)\n",
    "\n",
    "    def search_dep_path(self, start_idx, end_idx, adj_max, dep_path_arr):\n",
    "        for next_id in range(len(adj_max[start_idx])):\n",
    "            if next_id in dep_path_arr or adj_max[start_idx][next_id] in [\"none\"]:\n",
    "                continue\n",
    "            if next_id == end_idx:\n",
    "                return 1, dep_path_arr + [next_id]\n",
    "            stat, dep_arr = self.search_dep_path(next_id, end_idx, adj_max, dep_path_arr + [next_id])\n",
    "            if stat == 1:\n",
    "                return stat, dep_arr\n",
    "        return 0, []\n",
    "\n",
    "    def get_dep_path(self, start_index, end_index, direct=False):\n",
    "        dep_adj_matrix, dep_type_matrix = self.get_first_order(direct=direct)\n",
    "        _, dep_path = self.search_dep_path(start_index, end_index, dep_type_matrix, [start_index])\n",
    "        return dep_path\n",
    "\n",
    "class ABSADataset(Dataset):\n",
    "    def __init__(self, datafile, tokenizer, opt, deptype2id=None, dep_order=\"first\"):\n",
    "        self.datafile = datafile\n",
    "        self.depfile = \"{}.dep\".format(datafile)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.opt = opt\n",
    "        self.deptype2id = deptype2id\n",
    "        self.dep_order = dep_order\n",
    "        self.textdata = ABSADataset.load_datafile(self.datafile)\n",
    "        self.depinfo = ABSADataset.load_depfile(self.depfile)\n",
    "        self.polarity2id = self.get_polarity2id()\n",
    "        self.feature = []\n",
    "        for sentence,depinfo in zip(self.textdata, self.depinfo):\n",
    "            self.feature.append(self.create_feature(sentence, depinfo, opt.print_sent))\n",
    "        print(self.feature[:1])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.feature[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feature)\n",
    "\n",
    "    def ws(self, text):\n",
    "        tokens = []\n",
    "        valid_ids = []\n",
    "        for i, word in enumerate(text):\n",
    "            if len(text) <= 0:\n",
    "                continue\n",
    "            token = self.tokenizer.tokenizer.tokenize(word)\n",
    "            tokens.extend(token)\n",
    "            for m in range(len(token)):\n",
    "                if m == 0:\n",
    "                    valid_ids.append(1)\n",
    "                else:\n",
    "                    valid_ids.append(0)\n",
    "        token_ids = self.tokenizer.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        return tokens, token_ids, valid_ids\n",
    "\n",
    "    def create_feature(self, sentence, depinfo, print_sent = False):\n",
    "        text_left, text_right, aspect, polarity = sentence\n",
    "\n",
    "        cls_id = self.tokenizer.tokenizer.vocab[\"[CLS]\"]\n",
    "        sep_id = self.tokenizer.tokenizer.vocab[\"[SEP]\"]\n",
    "\n",
    "        doc = text_left + \" \" + aspect + \" \" + text_right\n",
    "\n",
    "        left_tokens, left_token_ids, left_valid_ids = self.ws(text_left.split(\" \"))\n",
    "        right_tokens, right_token_ids, right_valid_ids = self.ws(text_right.split(\" \"))\n",
    "        aspect_tokens, aspect_token_ids, aspect_valid_ids = self.ws(aspect.split(\" \"))\n",
    "        tokens = left_tokens + aspect_tokens + right_tokens\n",
    "        input_ids = [cls_id] + left_token_ids + aspect_token_ids + right_token_ids + [sep_id] + aspect_token_ids + [sep_id]\n",
    "        valid_ids = [1] + left_valid_ids + aspect_valid_ids + right_valid_ids + [1] + aspect_valid_ids + [1]\n",
    "        mem_valid_ids = [0] + [0] * len(left_tokens) + [1] * len(aspect_tokens) + [0] * len(right_tokens) # aspect terms mask\n",
    "        segment_ids = [0] * (len(tokens) + 2) + [1] * (len(aspect_tokens)+1)\n",
    "\n",
    "        dep_instance_parser = DepInstanceParser(basicDependencies=depinfo, tokens=[])\n",
    "        if self.dep_order == \"first\":\n",
    "            dep_adj_matrix, dep_type_matrix = dep_instance_parser.get_first_order()\n",
    "        elif self.dep_order == \"second\":\n",
    "            dep_adj_matrix, dep_type_matrix = dep_instance_parser.get_second_order()\n",
    "        elif self.dep_order == \"third\":\n",
    "            dep_adj_matrix, dep_type_matrix = dep_instance_parser.get_third_order()\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "        token_head_list = []\n",
    "        for input_id, valid_id in zip(input_ids, valid_ids):\n",
    "            if input_id == cls_id:\n",
    "                continue\n",
    "            if input_id == sep_id:\n",
    "                break\n",
    "            if valid_id == 1:\n",
    "                token_head_list.append(input_id)\n",
    "\n",
    "        input_ids = self.tokenizer.id_to_sequence(input_ids)\n",
    "        valid_ids = self.tokenizer.id_to_sequence(valid_ids)\n",
    "        segment_ids = self.tokenizer.id_to_sequence(segment_ids)\n",
    "        mem_valid_ids = self.tokenizer.id_to_sequence(mem_valid_ids)\n",
    "\n",
    "        size = input_ids.shape[0]\n",
    "        \n",
    "        if print_sent:\n",
    "            print(doc)\n",
    "            print(len(dep_adj_matrix[0]))\n",
    "\n",
    "        # final_dep_adj_matrix = [[0] * size for _ in range(self.tokenizer.max_seq_len)]\n",
    "        # final_dep_value_matrix = [[0] * size for _ in range(self.tokenizer.max_seq_len)]\n",
    "        final_dep_adj_matrix = [[0] * size for _ in range(size)]\n",
    "        final_dep_value_matrix = [[0] * size for _ in range(size)]\n",
    "        for i in range(len(token_head_list)):\n",
    "            for j in range(len(dep_adj_matrix[i])):\n",
    "                if j >= size:\n",
    "                    break\n",
    "                final_dep_adj_matrix[i+1][j] = dep_adj_matrix[i][j]\n",
    "                final_dep_value_matrix[i+1][j] = self.deptype2id[dep_type_matrix[i][j]]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\":torch.tensor(input_ids),\n",
    "            \"valid_ids\":torch.tensor(valid_ids),\n",
    "            \"segment_ids\":torch.tensor(segment_ids),\n",
    "            \"mem_valid_ids\":torch.tensor(mem_valid_ids),\n",
    "            \"dep_adj_matrix\":torch.tensor(final_dep_adj_matrix),\n",
    "            \"dep_value_matrix\":torch.tensor(final_dep_value_matrix),\n",
    "            \"polarity\": self.polarity2id[polarity],\n",
    "            \"raw_text\": doc,\n",
    "            \"aspect\": aspect\n",
    "        }\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def load_depfile(filename):\n",
    "        data = []\n",
    "        with open(filename, 'r') as f:\n",
    "            dep_info = []\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if len(line) > 0:\n",
    "                    items = line.split(\"\\t\")\n",
    "                    dep_info.append({\n",
    "                        \"governor\": int(items[0]),\n",
    "                        \"dependent\": int(items[1]),\n",
    "                        \"dep\": items[2],\n",
    "                    })\n",
    "                else:\n",
    "                    if len(dep_info) > 0:\n",
    "                        data.append(dep_info)\n",
    "                        dep_info = []\n",
    "            if len(dep_info) > 0:\n",
    "                data.append(dep_info)\n",
    "                dep_info = []\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def load_datafile(filename):\n",
    "        data = []\n",
    "        with open(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            for i in range(0, len(lines), 3):\n",
    "                text_left, _, text_right = [s.lower().strip() for s in lines[i].partition(\"$T$\")]\n",
    "                aspect = lines[i + 1].lower().strip()\n",
    "                text_right = text_right.replace(\"$T$\", aspect)\n",
    "                polarity = lines[i + 2].strip()\n",
    "                data.append([text_left, text_right, aspect, polarity])\n",
    "\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def load_deptype_map(opt):\n",
    "        deptype_set = set()\n",
    "        for filename in [opt.train_file, opt.test_file, opt.val_file]:\n",
    "            filename = \"{}.dep\".format(filename)\n",
    "            if os.path.exists(filename) is False:\n",
    "                continue\n",
    "            data = ABSADataset.load_depfile(filename)\n",
    "            for dep_info in data:\n",
    "                for item in dep_info:\n",
    "                    deptype_set.add(item['dep'])\n",
    "        deptype_map = {\"none\": 0}\n",
    "        for deptype in sorted(deptype_set, key=lambda x:x):\n",
    "            deptype_map[deptype] = len(deptype_map)\n",
    "        return deptype_map\n",
    "\n",
    "    @staticmethod\n",
    "    def get_polarity2id():\n",
    "        polarity_label = [\"-1\",\"0\",\"1\"]\n",
    "        return dict([(label, idx) for idx,label in enumerate(polarity_label)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6bb2ba",
   "metadata": {},
   "source": [
    "# TGCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09478741",
   "metadata": {
    "code_folding": [
     84
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pytorch_transformers import BertPreTrainedModel,BertModel\n",
    "\n",
    "class GraphConvolution(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "    def forward(self, text, adj):\n",
    "        hidden = torch.matmul(text, self.weight)\n",
    "        denom = torch.sum(adj, dim=2, keepdim=True) + 1\n",
    "        output = torch.matmul(adj, hidden) / denom\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "class TypeGraphConvolution(nn.Module):\n",
    "    \"\"\"\n",
    "    TGCN Layer\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, embedding_dim, bias=True):\n",
    "        super(TypeGraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        self.dense = nn.Linear(embedding_dim, in_features, bias=False)\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "    def forward(self, text, adj, dep_embed):\n",
    "        batch_size, max_len, feat_dim = text.shape\n",
    "        val_us = text.unsqueeze(dim=2)\n",
    "        val_us = val_us.repeat(1, 1, max_len, 1)\n",
    "        val_sum = val_us + self.dense(dep_embed)\n",
    "        adj_us = adj.unsqueeze(dim=-1)\n",
    "        adj_us = adj_us.repeat(1, 1, 1, feat_dim)\n",
    "        hidden = torch.matmul(val_sum, self.weight)\n",
    "        output = hidden.transpose(1,2) * adj_us\n",
    "\n",
    "        output = torch.sum(output, dim=2)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "        \n",
    "class SemGraphConvolution(nn.Module):\n",
    "    \"\"\"\n",
    "    Semantic GCN layer with attention adjacency matrix \n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, attention_heads = 1, bias=True):\n",
    "        super(SemGraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "    def forward(self, text, adj):\n",
    "        hidden = torch.matmul(text, self.weight)\n",
    "        denom = torch.sum(adj, dim=2, keepdim=True) + 1\n",
    "        output = torch.matmul(adj, hidden) / denom\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "        \n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.query = nn.Linear(input_dim, input_dim)\n",
    "        self.key = nn.Linear(input_dim, input_dim)\n",
    "        self.value = nn.Linear(input_dim, input_dim)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        queries = self.query(x)\n",
    "        keys = self.key(x)\n",
    "        values = self.value(x)\n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2)) / (self.input_dim ** 0.5)\n",
    "        attention = self.softmax(scores)\n",
    "        return attention\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = self.clones(nn.Linear(d_model, d_model), 2)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, :, :query.size(1)]\n",
    "            mask = mask.unsqueeze(1)\n",
    "            \n",
    "        nbatches = query.size(0)\n",
    "        query, key = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "                             for l, x in zip(self.linears, (query, key))]\n",
    "        \n",
    "        attn = self.attention(query, key, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        return attn\n",
    "    \n",
    "\n",
    "    def attention(self, query, key, mask=None, dropout=None):\n",
    "        d_k = query.size(-1)\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        p_attn = F.softmax(scores, dim=-1)\n",
    "        if dropout is not None:\n",
    "            p_attn = dropout(p_attn)\n",
    "\n",
    "        return p_attn\n",
    "    \n",
    "    def clones(self, module, N):\n",
    "        return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "        \n",
    "\n",
    "class AsaTgcnSem(BertPreTrainedModel):\n",
    "    def __init__(self, config, modules, tokenizer, opt):\n",
    "#     use_ensemble = True, fusion_type = 'concat', dropout = 0.2, concat_dropout = 0.5,\n",
    "#                  cooc_path = 'cooc_matrix_ids.csv', cooc = None):\n",
    "        \"\"\"\n",
    "        modules: dictionary of form {'tgcn': bool, 'semgcn': bool, 'lexgcn': bool}\n",
    "        cooc: cooc matrix as dataframe preloaded into memory. if not passed as argument,\n",
    "        the matrix will be loaded from the specified path.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(AsaTgcnSem, self).__init__(config)\n",
    "        self.opt = opt\n",
    "        self.modules = opt.modules\n",
    "        self.use_tgcn, self.use_semgcn, self.use_lexgcn = opt.modules['tgcn'], opt.modules['semgcn'], opt.modules['lexgcn']\n",
    "        self.num_modules = sum((self.use_tgcn, self.use_semgcn, self.use_lexgcn))\n",
    "        self.use_ensemble = opt.use_ensemble\n",
    "        self.layer_number_tgcn = opt.num_layers['tgcn']\n",
    "        self.layer_number_sem = opt.num_layers['semgcn']\n",
    "        self.layer_number_lex = opt.num_layers['lexgcn']\n",
    "        assert self.use_tgcn or self.use_semgcn or self.use_lexgcn\n",
    "        assert opt.fusion_type == 'concat' or opt.fusion_type == 'gate'\n",
    "        self.fusion_type = opt.fusion_type\n",
    "        \n",
    "        self.num_labels = config.num_labels\n",
    "        self.num_types = config.num_types\n",
    "        \n",
    "        \n",
    "#         self.modules = modules\n",
    "#         self.use_tgcn, self.use_semgcn, self.use_lexgcn = modules['tgcn'], modules['semgcn'], modules['lexgcn']\n",
    "#         self.num_modules = sum((self.use_tgcn, self.use_semgcn, self.use_lexgcn))\n",
    "#         assert self.use_tgcn or self.use_semgcn or self.use_lexgcn\n",
    "#         assert fusion_type == 'concat' or fusion_type == 'gate'\n",
    "#         self.fusion_type = fusion_type\n",
    "#         self.config = config\n",
    "#         self.layer_number_tgcn = 3\n",
    "#         self.layer_number_sem = 2\n",
    "#         self.layer_number_lex = 2\n",
    "#         self.num_labels = config.num_labels\n",
    "#         self.num_types = config.num_types\n",
    "#         self.use_ensemble = use_ensemble\n",
    "        \n",
    "        self.bert = BertModel(config)\n",
    "        \n",
    "        if self.use_tgcn:\n",
    "            self.TGCNLayers = nn.ModuleList(([TypeGraphConvolution(config.hidden_size, config.hidden_size, config.hidden_size)\n",
    "                                             for _ in range(self.layer_number_tgcn)]))\n",
    "        if self.use_semgcn:\n",
    "            self.SemGCNLayers = nn.ModuleList(([GraphConvolution(config.hidden_size, config.hidden_size)\n",
    "                                            for _ in range(self.layer_number_sem)]))\n",
    "        if self.use_lexgcn:\n",
    "            self.LexGCNLayers = nn.ModuleList(([GraphConvolution(config.hidden_size, config.hidden_size)\n",
    "                                           for _ in range(self.layer_number_lex)]))\n",
    "        \n",
    "        if self.use_lexgcn:\n",
    "            if opt.cooc is not None:\n",
    "                self.cooc = opt.cooc\n",
    "            else:\n",
    "                self.cooc = pd.read_csv(opt.cooc_path, index_col=0)\n",
    "            self.cooc.index = self.cooc.index.astype(int)\n",
    "            self.cooc.columns = self.cooc.columns.astype(int)\n",
    "            \n",
    "            # Obsolete\n",
    "#             self.cooc_matrix = self.cooc.to_numpy()\n",
    "#             # Padding with 0's to deal with out-of-vocabulary words in test data\n",
    "#             self.cooc_matrix = np.pad(self.cooc_matrix, ((0, 1), (0, 1)), mode='constant')\n",
    "#             # Mapping token ids to indices of matrix\n",
    "#             self.id_to_index_map = {tokenizer.tokenizer.convert_tokens_to_ids(tokenizer.tokenizer.tokenize(w))[0]: i for i, w in enumerate(self.cooc.columns)}\n",
    "        \n",
    "        # multiplied by two if concat\n",
    "        if self.fusion_type == 'concat':\n",
    "            self.fc_single = nn.Linear(config.hidden_size*self.num_modules, self.num_labels)\n",
    "        elif self.fusion_type == 'gate':\n",
    "            self.fc_single = nn.Linear(config.hidden_size, self.num_labels)\n",
    "        \n",
    "        self.gate_weight = nn.Parameter(torch.FloatTensor(config.hidden_size, config.hidden_size * 2))\n",
    "        self.gate_bias = nn.Parameter(torch.FloatTensor(config.hidden_size))\n",
    "    \n",
    "        self.dropout = nn.Dropout(opt.dropout)\n",
    "        self.concat_dropout = nn.Dropout(opt.concat_dropout)\n",
    "        self.ensemble_linear_tgcn = nn.Linear(1, self.layer_number_tgcn)\n",
    "        self.ensemble_linear_semgcn = nn.Linear(1, self.layer_number_sem)\n",
    "        self.ensemble_linear_lexgcn = nn.Linear(1, self.layer_number_lex)\n",
    "        self.ensemble = nn.Parameter(torch.FloatTensor(3, 1))\n",
    "        self.dep_embedding = nn.Embedding(self.num_types, config.hidden_size, padding_idx=0)\n",
    "\n",
    "    def get_attention(self, val_out, dep_embed, adj):\n",
    "        batch_size, max_len, feat_dim = val_out.shape\n",
    "        val_us = val_out.unsqueeze(dim=2)\n",
    "        val_us = val_us.repeat(1,1,max_len,1)\n",
    "        val_cat = torch.cat((val_us, dep_embed), -1).float()\n",
    "        atten_expand = (val_cat * val_cat.transpose(1,2))\n",
    "\n",
    "        attention_score = torch.sum(atten_expand, dim=-1)\n",
    "        attention_score = attention_score / np.power(feat_dim, 0.5)\n",
    "        exp_attention_score = torch.exp(attention_score)\n",
    "        exp_attention_score = torch.mul(exp_attention_score, adj.float()) # mask\n",
    "        sum_attention_score = torch.sum(exp_attention_score, dim=-1).unsqueeze(dim=-1).repeat(1,1,max_len)\n",
    "\n",
    "        attention_score = torch.div(exp_attention_score, sum_attention_score + 1e-10)\n",
    "        if 'HalfTensor' in val_out.type():\n",
    "            attention_score = attention_score.half()\n",
    "\n",
    "        return attention_score\n",
    "    \n",
    "    def get_lex_adj(self, input_ids, batch_size, max_len):\n",
    "        # Initialize an empty adjacency tensor\n",
    "        adj_tensor = torch.zeros((batch_size, max_len, max_len))\n",
    "        \n",
    "#         for i, id_sequence in enumerate(input_ids):\n",
    "#             # Get word list\n",
    "#             num_words = int(torch.sum(id_sequence != 0))\n",
    "\n",
    "#             word_indices = []\n",
    "            \n",
    "#             for word_id in id_sequence:\n",
    "#                 word_id_int = int(word_id) # conver from torch.tensor to int\n",
    "#                 index = self.id_to_index_map[word_id_int] if word_id_int in self.id_to_index_map else -1\n",
    "#                 word_indices.append(index)\n",
    "# #             print('word indices: ', word_indices)\n",
    "# #             print('num words: ', num_words)\n",
    "# #             print('input ids: ', input_ids)\n",
    "            \n",
    "#             for j in range(num_words):\n",
    "#                 for k in range(num_words):\n",
    "#                     if j != k:\n",
    "#                         adj_tensor[i, j, k] = self.cooc_matrix[word_indices[j]][word_indices[k]]\n",
    "#                     else:\n",
    "#                         adj_tensor[i, j, k] = adj_tensor[i, j, k] / (2 * num_words)\n",
    "        \n",
    "        \n",
    "        # number of non-zero input_ids for each sentence\n",
    "        num_words = []\n",
    "        \n",
    "        # i refers to the sentence number \n",
    "        for i, id_sequence in enumerate(input_ids):\n",
    "            num_words.append(int(torch.sum(id_sequence != 0)))\n",
    "            \n",
    "            for j in range(num_words[i]):\n",
    "                for k in range(num_words[i]):\n",
    "                    if j != k:\n",
    "                        id_j, id_k = id_sequence[j].item(), id_sequence[k].item()\n",
    "                        if id_j in self.cooc and id_k in self.cooc:\n",
    "                            adj_tensor[i, j, k] = self.cooc[id_j][id_k]\n",
    "                        else:\n",
    "                            adj_tensor[i, j, k] = 0\n",
    "            \n",
    "            \n",
    "        # Calculate the sums of rows for each matrix\n",
    "        row_sums = adj_tensor.sum(dim=2, keepdim=True).repeat(1, 1, max_len)\n",
    "\n",
    "        # Calculate the sums of columns for each matrix\n",
    "        column_sums = adj_tensor.sum(dim=1, keepdim=True).repeat(1, max_len, 1)\n",
    "\n",
    "        # Create a diagonal mask for each matrix\n",
    "        diagonal_mask = torch.eye(adj_tensor.size(-1)).bool().unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "\n",
    "        total_sum = row_sums + column_sums\n",
    "\n",
    "        # Set the diagonal entries to the sum of all the row and column entries (will be averaged later)\n",
    "        res = torch.where(diagonal_mask, total_sum, adj_tensor)\n",
    "        \n",
    "        adj_tensor = adj_tensor + res\n",
    "        \n",
    "        # Average \n",
    "        \n",
    "        for i, num in enumerate(num_words):\n",
    "            # Divide diagonal elements by 2\n",
    "            diagonal = torch.diagonal(adj_tensor[i])\n",
    "            diagonal_divided = diagonal / num\n",
    "\n",
    "            # Assign divided diagonal elements back to the tensor\n",
    "            adj_tensor[i].diagonal().copy_(diagonal_divided)\n",
    "\n",
    "        return adj_tensor\n",
    "\n",
    "    def get_avarage(self, aspect_indices, x):\n",
    "        aspect_indices_us = torch.unsqueeze(aspect_indices, 2)\n",
    "        x_mask = x * aspect_indices_us\n",
    "        aspect_len = (aspect_indices_us != 0).sum(dim=1)\n",
    "        x_sum = x_mask.sum(dim=1)\n",
    "        x_av = torch.div(x_sum, aspect_len)\n",
    "\n",
    "        return x_av\n",
    "    \n",
    "    def set_dropout(self, dropout):\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        \n",
    "    def forward(self, input_ids, segment_ids, valid_ids, mem_valid_ids, dep_adj_matrix, dep_value_matrix):\n",
    "        # Generate sentence representation with BERT\n",
    "        sequence_output, pooled_output = self.bert(input_ids, segment_ids)\n",
    "        \n",
    "        # Dependency type embeddings\n",
    "        dep_embed = self.dep_embedding(dep_value_matrix)\n",
    "        \n",
    "        # Initializing valid output tensor (i.e. 0 for padding, only keeping representations of tokens in sentence)\n",
    "        batch_size, max_len, feat_dim = sequence_output.shape\n",
    "        valid_output = torch.zeros(batch_size, max_len, feat_dim, device=input_ids.device).type_as(sequence_output)\n",
    "        for i in range(batch_size):\n",
    "            temp = sequence_output[i][valid_ids[i] == 1]\n",
    "            valid_output[i][:temp.size(0)] = temp\n",
    "        valid_output = self.dropout(valid_output)\n",
    "\n",
    "        attention_score_for_output = [] # Useless code?\n",
    "        tgcn_layer_outputs = []\n",
    "        semgcn_layer_outputs = []\n",
    "        lexgcn_layer_outputs = []\n",
    "        seq_out_tgcn = valid_output\n",
    "        seq_out_semgcn = valid_output\n",
    "        seq_out_lexgcn = valid_output\n",
    "        if self.use_tgcn:\n",
    "            for tgcn in self.TGCNLayers:\n",
    "                # Computing attention\n",
    "                attention_score = self.get_attention(seq_out_tgcn, dep_embed, dep_adj_matrix)\n",
    "                attention_score_for_output.append(attention_score) # Useless code?\n",
    "\n",
    "                # Applying GCN layer\n",
    "                seq_out = F.relu(tgcn(seq_out_tgcn, attention_score, dep_embed))\n",
    "\n",
    "                # Saving layer output to be used for layer ensemble later\n",
    "                tgcn_layer_outputs.append(seq_out_tgcn)\n",
    "                \n",
    "            # Average aspect terms for each layer and combining into list \n",
    "            tgcn_layer_outputs_pool = [self.get_avarage(mem_valid_ids, x_out) for x_out in tgcn_layer_outputs]\n",
    "        \n",
    "        if self.use_semgcn:\n",
    "            for semgcn in self.SemGCNLayers:\n",
    "                # Computing attention\n",
    "                attn = MultiHeadAttention(1, feat_dim)\n",
    "                attn.to('cuda')\n",
    "                attn_tensor = attn(seq_out_semgcn, seq_out_semgcn)\n",
    "                attn_tensor = attn_tensor.squeeze(1)\n",
    "\n",
    "                # Applying GCN layer\n",
    "                seq_out_semgcn = F.relu(semgcn(seq_out_semgcn, attn_tensor))\n",
    "\n",
    "                # Saving layer output\n",
    "                semgcn_layer_outputs.append(seq_out_semgcn)\n",
    "                \n",
    "            # Average aspect terms for each layer and combining into list\n",
    "            semgcn_layer_outputs_pool = [self.get_avarage(mem_valid_ids, x_out) for x_out in semgcn_layer_outputs]\n",
    "            \n",
    "        \n",
    "        if self.use_lexgcn:\n",
    "            for lexgcn in self.LexGCNLayers:\n",
    "                # Compute adjaceny matrix\n",
    "                adj_tensor = self.get_lex_adj(input_ids, batch_size, max_len)\n",
    "                adj_tensor = adj_tensor.to('cuda')\n",
    "                # Applying GCN layer\n",
    "# #                 print(f'ADJ_TENSOR: {adj_tensor}')\n",
    "#                 print(seq_out_lexgcn.device, adj_tensor.device)\n",
    "                seq_out_lexgcn = F.relu(lexgcn(seq_out_lexgcn, adj_tensor))\n",
    "                \n",
    "                # Saving layer output\n",
    "                lexgcn_layer_outputs.append(seq_out_lexgcn)\n",
    "            \n",
    "            # Average aspect terms for each layer and combining into list\n",
    "            lexgcn_layer_outputs_pool = [self.get_avarage(mem_valid_ids, x_out) for x_out in lexgcn_layer_outputs]\n",
    "        \n",
    "        all_outputs = []\n",
    "        \n",
    "        if self.use_ensemble:\n",
    "            if self.use_tgcn:\n",
    "                # Layer ensemble for tgcn\n",
    "                tgcn_pool = torch.stack(tgcn_layer_outputs_pool, -1) # stacking layer outputs \n",
    "                ensemble_tgcn = torch.matmul(tgcn_pool, F.softmax(self.ensemble_linear_tgcn.weight, dim=0))\n",
    "                ensemble_tgcn = ensemble_tgcn.squeeze(dim=-1)\n",
    "                ensemble_tgcn = self.dropout(ensemble_tgcn)\n",
    "                all_outputs.append(ensemble_tgcn)\n",
    "            \n",
    "            if self.use_semgcn:\n",
    "                # Layer ensemble for semgcn\n",
    "                semgcn_pool = torch.stack(semgcn_layer_outputs_pool, -1)\n",
    "                ensemble_semgcn = torch.matmul(semgcn_pool, F.softmax(self.ensemble_linear_semgcn.weight, dim = 0))\n",
    "                ensemble_semgcn = ensemble_semgcn.squeeze(dim=-1)\n",
    "                ensemble_semgcn = self.dropout(ensemble_semgcn)\n",
    "                all_outputs.append(ensemble_semgcn)\n",
    "            \n",
    "            if self.use_lexgcn:\n",
    "            # Layer ensemble for lexgcn\n",
    "                lexgcn_pool = torch.stack(lexgcn_layer_outputs_pool, -1)\n",
    "                ensemble_lexgcn = torch.matmul(lexgcn_pool, F.softmax(self.ensemble_linear_lexgcn.weight, dim = 0))\n",
    "                ensemble_lexgcn = ensemble_lexgcn.squeeze(dim=-1)\n",
    "                ensemble_lexgcn = self.dropout(ensemble_lexgcn)\n",
    "                all_outputs.append(ensemble_lexgcn)\n",
    "            \n",
    "        else:\n",
    "            # Take only the last layer output\n",
    "            if self.use_tgcn:\n",
    "                ensemble_tgcn = tgcn_layer_outputs_pool[-1]\n",
    "                all_outputs.append(ensemble_tgcn)\n",
    "            if self.use_semgcn:\n",
    "                ensemble_semgcn = semgcn_layer_outputs_pool[-1]\n",
    "                all_outputs.append(ensemble_semgcn)\n",
    "            if self.use_lexgcn:\n",
    "                ensemble_lexgcn = lexgcn_layer_outputs_pool[-1]\n",
    "                all_outputs.append(ensemble_lexgcn)\n",
    "            \n",
    "        # Stacking module outputs\n",
    "        ensemble_out = torch.cat(all_outputs, dim=1)\n",
    "        \n",
    "        # gating only if 2 modules used\n",
    "        if self.fusion_type == 'gate' and self.num_modules == 2: \n",
    "            concatenated = torch.cat((ensemble_tgcn, ensemble_semgcn), dim=1) \n",
    "            g = torch.matmul(concatenated, self.gate_weight.t()) + self.gate_bias  # Compute W_g[h0 ; h1] + b_g\n",
    "            g = torch.sigmoid(g)\n",
    "            ensemble_out = g * ensemble_tgcn + (1 - g) * ensemble_semgcn\n",
    "          \n",
    "        # Additional dropout\n",
    "        if (self.num_modules == 2 and self.fusion_type == 'concat') or self.num_modules == 3:\n",
    "            ensemble_out = self.concat_dropout(ensemble_out)\n",
    "        output = self.fc_single(ensemble_out)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "class AsaTgcn(BertPreTrainedModel):\n",
    "    def __init__(self, config, dropout = 0.2):\n",
    "        super(AsaTgcn, self).__init__(config)\n",
    "        self.config = config\n",
    "        self.layer_number_tgcn = 3\n",
    "        self.num_labels = config.num_labels\n",
    "        self.num_types = config.num_types\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.TGCNLayers = nn.ModuleList(([TypeGraphConvolution(config.hidden_size, config.hidden_size, config.hidden_size)\n",
    "                                         for _ in range(self.layer_number_tgcn)]))\n",
    "        self.fc_single = nn.Linear(config.hidden_size, self.num_labels)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ensemble_linear_tgcn = nn.Linear(1, self.layer_number_tgcn)\n",
    "        self.ensemble = nn.Parameter(torch.FloatTensor(3, 1))\n",
    "        self.dep_embedding = nn.Embedding(self.num_types, config.hidden_size, padding_idx=0)\n",
    "\n",
    "    def get_attention(self, val_out, dep_embed, adj):\n",
    "        batch_size, max_len, feat_dim = val_out.shape\n",
    "        val_us = val_out.unsqueeze(dim=2)\n",
    "        val_us = val_us.repeat(1,1,max_len,1)\n",
    "        val_cat = torch.cat((val_us, dep_embed), -1).float()\n",
    "        atten_expand = (val_cat * val_cat.transpose(1,2))\n",
    "\n",
    "        attention_score = torch.sum(atten_expand, dim=-1)\n",
    "        attention_score = attention_score / np.power(feat_dim, 0.5)\n",
    "        exp_attention_score = torch.exp(attention_score)\n",
    "        exp_attention_score = torch.mul(exp_attention_score, adj.float()) # mask\n",
    "        sum_attention_score = torch.sum(exp_attention_score, dim=-1).unsqueeze(dim=-1).repeat(1,1,max_len)\n",
    "\n",
    "        attention_score = torch.div(exp_attention_score, sum_attention_score + 1e-10)\n",
    "        if 'HalfTensor' in val_out.type():\n",
    "            attention_score = attention_score.half()\n",
    "\n",
    "        return attention_score\n",
    "\n",
    "    def get_avarage(self, aspect_indices, x):\n",
    "        aspect_indices_us = torch.unsqueeze(aspect_indices, 2)\n",
    "        x_mask = x * aspect_indices_us\n",
    "        aspect_len = (aspect_indices_us != 0).sum(dim=1)\n",
    "        x_sum = x_mask.sum(dim=1)\n",
    "        x_av = torch.div(x_sum, aspect_len)\n",
    "        return x_av\n",
    "    \n",
    "    def set_dropout(self, dropout):\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_ids, segment_ids, valid_ids, mem_valid_ids, dep_adj_matrix, dep_value_matrix):\n",
    "        # Generate sentence representation with BERT\n",
    "        sequence_output, pooled_output = self.bert(input_ids, segment_ids)\n",
    "        \n",
    "        # Dependency type embeddings\n",
    "        dep_embed = self.dep_embedding(dep_value_matrix)\n",
    "        \n",
    "        # Initializing valid output tensor (i.e. 0 for padding, only keeping representations of tokens in sentence)\n",
    "        batch_size, max_len, feat_dim = sequence_output.shape\n",
    "        valid_output = torch.zeros(batch_size, max_len, feat_dim, device=input_ids.device).type_as(sequence_output)\n",
    "        for i in range(batch_size):\n",
    "            temp = sequence_output[i][valid_ids[i] == 1]\n",
    "            valid_output[i][:temp.size(0)] = temp\n",
    "        valid_output = self.dropout(valid_output)\n",
    "\n",
    "        attention_score_for_output = [] # Useless code?\n",
    "        tgcn_layer_outputs = []\n",
    "        semgcn_layer_outputs = []\n",
    "        seq_out_tgcn = valid_output\n",
    "        seq_out_semgcn = valid_output\n",
    "        for tgcn in self.TGCNLayers:\n",
    "            # Computing attention\n",
    "            attention_score = self.get_attention(seq_out_tgcn, dep_embed, dep_adj_matrix)\n",
    "            attention_score_for_output.append(attention_score) # Useless code?\n",
    "            \n",
    "            # Applying GCN layer\n",
    "            seq_out = F.relu(tgcn(seq_out_tgcn, attention_score, dep_embed))\n",
    "            \n",
    "            # Saving layer output to be used for layer ensemble later\n",
    "            tgcn_layer_outputs.append(seq_out_tgcn)\n",
    "        \n",
    "        # Average aspect terms for each layer and combining into list\n",
    "        tgcn_layer_outputs_pool = [self.get_avarage(mem_valid_ids, x_out) for x_out in tgcn_layer_outputs]\n",
    "        \n",
    "        # Layer ensemble for tgcn\n",
    "        tgcn_pool = torch.stack(tgcn_layer_outputs_pool, -1) # stacking layer outputs \n",
    "        ensemble_tgcn = torch.matmul(tgcn_pool, F.softmax(self.ensemble_linear_tgcn.weight, dim=0))\n",
    "        ensemble_tgcn = ensemble_tgcn.squeeze(dim=-1)\n",
    "        ensemble_tgcn = self.dropout(ensemble_tgcn)\n",
    "        \n",
    "        output = self.fc_single(ensemble_tgcn)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a68451",
   "metadata": {},
   "source": [
    "# Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ed06cd6",
   "metadata": {
    "code_folding": [
     308
    ]
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import argparse\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "from time import strftime, localtime\n",
    "import random\n",
    "import numpy as np\n",
    "import subprocess\n",
    "\n",
    "from pytorch_transformers import BertModel, BertConfig\n",
    "# from data_utils import Tokenizer4Bert, ABSADataset\n",
    "# from asa_tgcn_model import AsaTgcn\n",
    "\n",
    "# !pip install scikit-learn\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "\n",
    "CONFIG_NAME = 'config.json'\n",
    "WEIGHTS_NAME = 'pytorch_model.bin'\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "class Instructor:\n",
    "    def __init__(self, opt):\n",
    "        self.opt = opt\n",
    "        logger.info(opt)\n",
    "        deptype2id = ABSADataset.load_deptype_map(opt)\n",
    "        polarity2id = ABSADataset.get_polarity2id()\n",
    "        logger.info(deptype2id)\n",
    "        logger.info(polarity2id)\n",
    "        self.deptype2id = deptype2id\n",
    "        self.polarity2id = polarity2id\n",
    "        \n",
    "        self.vocab_path = os.path.join(opt.bert_model, 'vocab.txt')\n",
    "        self.tokenizer = Tokenizer4Bert(opt.max_seq_len, opt.bert_model)\n",
    "        config = BertConfig.from_json_file(os.path.join(opt.bert_model, CONFIG_NAME))\n",
    "        config.num_labels=opt.polarities_dim\n",
    "        config.num_types=len(self.deptype2id)\n",
    "        logger.info(config)\n",
    "        if opt.model_type == 'tgcn':\n",
    "            self.model = AsaTgcn.from_pretrained(opt.bert_model, config=config, dropout = opt.dropout)\n",
    "        else:\n",
    "            self.model = AsaTgcnSem.from_pretrained(opt.bert_model, config=config, modules = opt.modules,\n",
    "                                                    tokenizer = self.tokenizer, opt=self.opt) \n",
    "#                                                 use_ensemble = opt.use_ensemble,\n",
    "#                                                     fusion_type = opt.fusion_type, dropout = opt.dropout, \n",
    "#                                                     concat_dropout = opt.concat_dropout,\n",
    "#                                                    cooc_path = opt.cooc_path, cooc = opt.cooc)\n",
    "        self.model.set_dropout(opt.dropout)\n",
    "        self.model.to(opt.device)\n",
    "        \n",
    "        self.fulltrainset = ABSADataset(opt.train_file, self.tokenizer, self.opt, deptype2id=deptype2id)\n",
    "        self.trainset = ABSADataset(opt.train_file, self.tokenizer, self.opt, deptype2id=deptype2id)\n",
    "        self.testset = ABSADataset(opt.test_file, self.tokenizer, self.opt, deptype2id=deptype2id)\n",
    "        \n",
    "        \n",
    "        if os.path.exists(opt.val_file):\n",
    "            self.valset = ABSADataset(opt.val_file, self.tokenizer, self.opt, deptype2id=deptype2id)\n",
    "        elif opt.valset_ratio > 0:\n",
    "            valset_len = int(len(self.trainset) * opt.valset_ratio)\n",
    "            self.trainset, self.valset = random_split(self.trainset, (len(self.trainset)-valset_len, valset_len))\n",
    "        else:\n",
    "            self.valset = self.testset\n",
    "\n",
    "        if opt.device.type == 'cuda':\n",
    "            logger.info('cuda memory allocated: {}'.format(torch.cuda.memory_allocated(device=opt.device.index)))\n",
    "\n",
    "    def _print_args(self):\n",
    "        n_trainable_params, n_nontrainable_params = 0, 0\n",
    "        for p in self.model.parameters():\n",
    "            n_params = torch.prod(torch.tensor(p.shape))\n",
    "            if p.requires_grad:\n",
    "                n_trainable_params += n_params\n",
    "            else:\n",
    "                n_nontrainable_params += n_params\n",
    "        logger.info('n_trainable_params: {0}, n_nontrainable_params: {1}'.format(n_trainable_params, n_nontrainable_params))\n",
    "        logger.info('> training arguments:')\n",
    "        for arg in vars(self.opt):\n",
    "            logger.info('>>> {0}: {1}'.format(arg, getattr(self.opt, arg)))\n",
    "\n",
    "    def _reset_params(self):\n",
    "        for child in self.model.children():\n",
    "            if type(child) != BertModel:  # skip bert params\n",
    "                for p in child.parameters():\n",
    "                    if p.requires_grad:\n",
    "                        if len(p.shape) > 1:\n",
    "                            torch.nn.init.xavier_uniform_(p)\n",
    "                        else:\n",
    "                            stdv = 1. / math.sqrt(p.shape[0])\n",
    "                            torch.nn.init.uniform_(p, a=-stdv, b=stdv)\n",
    "\n",
    "    def save_model(self, save_path, model, args):\n",
    "        # Save a trained model, configuration and tokenizer\n",
    "        model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "        # If we save using the predefined names, we can load using `from_pretrained`\n",
    "        output_model_file = os.path.join(save_path, WEIGHTS_NAME)\n",
    "        output_config_file = os.path.join(save_path, CONFIG_NAME)\n",
    "        torch.save(model_to_save.state_dict(), output_model_file)\n",
    "\n",
    "        config = model_to_save.config\n",
    "        config.__dict__[\"deptype2id\"] = self.deptype2id\n",
    "        config.__dict__[\"polarity2id\"] = self.polarity2id\n",
    "        with open(output_config_file, \"w\", encoding='utf-8') as writer:\n",
    "            writer.write(config.to_json_string())\n",
    "        output_args_file = os.path.join(save_path, 'training_args.bin')\n",
    "        torch.save(args, output_args_file)\n",
    "        subprocess.run(['cp', self.vocab_path, os.path.join(save_path, 'vocab.txt')])\n",
    "\n",
    "    def _train(self, criterion, optimizer, train_data_loader, val_data_loader, test_data_loader):\n",
    "        max_val_acc = -1\n",
    "        max_val_f1 = -1\n",
    "        global_step = 0\n",
    "        path = None\n",
    "\n",
    "        model_home = self.opt.model_path \n",
    "#         model_home += '-' + strftime(\"%y%m%d-%H%M\", localtime())\n",
    "\n",
    "        results = {\"bert_model\": self.opt.bert_model, \"batch_size\": self.opt.batch_size,\n",
    "                   \"learning_rate\": self.opt.learning_rate, \"seed\": self.opt.seed,\n",
    "                  \"num_epoch\": self.opt.num_epoch, \"l2reg\": self.opt.l2reg,\n",
    "                  \"dropout\": self.opt.dropout}\n",
    "        for epoch in range(self.opt.num_epoch):\n",
    "            logger.info('>' * 100)\n",
    "            logger.info('epoch: {}'.format(epoch))\n",
    "            n_correct, n_total, loss_total = 0, 0, 0\n",
    "            self.model.train()\n",
    "            for i_batch, t_sample_batched in enumerate(train_data_loader):\n",
    "                global_step += 1\n",
    "                optimizer.zero_grad()\n",
    "                # t_sample_batched[\"raw_text\"],\n",
    "                outputs = self.model(t_sample_batched[\"input_ids\"].to(self.opt.device),\n",
    "                                     t_sample_batched[\"segment_ids\"].to(self.opt.device),\n",
    "                                     t_sample_batched[\"valid_ids\"].to(self.opt.device),\n",
    "                                     t_sample_batched[\"mem_valid_ids\"].to(self.opt.device),\n",
    "                                     t_sample_batched[\"dep_adj_matrix\"].to(self.opt.device),\n",
    "                                     t_sample_batched[\"dep_value_matrix\"].to(self.opt.device))\n",
    "\n",
    "                targets = t_sample_batched['polarity'].to(self.opt.device)\n",
    "\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                n_correct += (torch.argmax(outputs, -1) == targets).sum().item()\n",
    "                n_total += len(outputs)\n",
    "                loss_total += loss.item() * len(outputs)\n",
    "                if global_step % self.opt.log_step == 0:\n",
    "                    train_acc = n_correct / n_total\n",
    "                    train_loss = loss_total / n_total\n",
    "                    logger.info('epoch: {}, loss: {:.4f}, train acc: {:.4f}'.format(epoch, train_loss, train_acc))\n",
    "            val_acc, val_f1 = Instructor._evaluate_acc_f1(self.model, val_data_loader, device=self.opt.device)\n",
    "            logger.info('>epoch: {}, val_acc: {:.4f}, val_f1: {:.4f}'.format(epoch, val_acc, val_f1))\n",
    "            results[\"{}_val_acc\".format(epoch)] = val_acc\n",
    "            results[\"{}_val_f1\".format(epoch)] = val_f1\n",
    "            saving_path = os.path.join(model_home, \"epoch_{}\".format(epoch))\n",
    "            if not os.path.exists(saving_path):\n",
    "                os.makedirs(saving_path)\n",
    "            if val_acc > max_val_acc or (val_acc == max_val_acc and val_f1 > max_val_f1):\n",
    "                max_val_acc = val_acc\n",
    "                max_val_f1 = val_f1\n",
    "                \n",
    "                if opt.save_models == 'last':\n",
    "                    best_path = saving_path\n",
    "                    best_model = self.model\n",
    "                elif opt.save_models == 'all':\n",
    "                    self.save_model(saving_path, self.model, self.opt)\n",
    "                elif opt.save_models == 'none':\n",
    "                    pass \n",
    "\n",
    "                self.model.eval()\n",
    "                saving_path = os.path.join(model_home, \"epoch_{}_eval.txt\".format(epoch))\n",
    "                test_acc, test_f1 = self._evaluate_acc_f1(self.model, test_data_loader, device=self.opt.device,\n",
    "                                                          saving_path=saving_path)\n",
    "                logger.info('>> epoch: {}, test_acc: {:.4f}, test_f1: {:.4f}'.format(epoch, test_acc, test_f1))\n",
    "\n",
    "                results[\"max_val_acc\"] = max_val_acc\n",
    "                results[\"test_acc\"] = test_acc\n",
    "                results[\"test_f1\"] = test_f1\n",
    "            \n",
    "            output_eval_file = os.path.join(model_home, \"eval_results.txt\")\n",
    "            with open(output_eval_file, \"w\") as writer:\n",
    "                for k,v in results.items():\n",
    "                    writer.write(\"{}={}\\n\".format(k,v))\n",
    "        acc_file = os.path.join(model_home, \"acc-{:.4f}\".format(test_acc))\n",
    "        if opt.save_models == 'last':\n",
    "            self.save_model(best_path, best_model, self.opt)\n",
    "        with open(acc_file, 'w') as f:\n",
    "            f.write(f\"accuracy: {test_acc}\")\n",
    "        return max_val_acc, test_acc, test_f1\n",
    "\n",
    "    @staticmethod\n",
    "    def _evaluate_acc_f1(model, data_loader, device, saving_path=None):\n",
    "        n_correct, n_total = 0, 0\n",
    "        t_targets_all, t_outputs_all = None, None\n",
    "        model.eval()\n",
    "\n",
    "        saving_path_f = open(saving_path, 'w') if saving_path is not None else None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for t_batch, t_sample_batched in enumerate(data_loader):\n",
    "                t_targets = t_sample_batched['polarity'].to(device)\n",
    "                t_raw_texts = t_sample_batched['raw_text']\n",
    "                t_aspects = t_sample_batched['aspect']\n",
    "\n",
    "                t_outputs = model(t_sample_batched[\"input_ids\"].to(device),\n",
    "                                  t_sample_batched[\"segment_ids\"].to(device),\n",
    "                                  t_sample_batched[\"valid_ids\"].to(device),\n",
    "                                  t_sample_batched[\"mem_valid_ids\"].to(device),\n",
    "                                  t_sample_batched[\"dep_adj_matrix\"].to(device),\n",
    "                                  t_sample_batched[\"dep_value_matrix\"].to(device))\n",
    "\n",
    "                n_correct += (torch.argmax(t_outputs, -1) == t_targets).sum().item()\n",
    "                n_total += len(t_outputs)\n",
    "\n",
    "                if t_targets_all is None:\n",
    "                    t_targets_all = t_targets\n",
    "                    t_outputs_all = t_outputs\n",
    "                else:\n",
    "                    t_targets_all = torch.cat((t_targets_all, t_targets), dim=0)\n",
    "                    t_outputs_all = torch.cat((t_outputs_all, t_outputs), dim=0)\n",
    "\n",
    "                if saving_path_f is not None:\n",
    "                    for t_target, t_output, t_raw_text, t_aspect in zip(t_targets.detach().cpu().numpy(),\n",
    "                                                                        torch.argmax(t_outputs, -1).detach().cpu().numpy(),\n",
    "                                                                        t_raw_texts, t_aspects):\n",
    "                        saving_path_f.write(\"{}\\t{}\\t{}\\t{}\\n\".format(t_target, t_output, t_raw_text, t_aspect))\n",
    "\n",
    "        acc = n_correct / n_total\n",
    "        f1 = metrics.f1_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2], average='macro')\n",
    "        return acc, f1\n",
    "\n",
    "    def train(self):\n",
    "        # Loss and Optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        _params = filter(lambda p: p.requires_grad, self.model.parameters())\n",
    "        optimizer = torch.optim.Adam(_params, lr=self.opt.learning_rate, weight_decay=self.opt.l2reg)\n",
    "\n",
    "        train_data_loader = DataLoader(dataset=self.trainset, batch_size=self.opt.batch_size, shuffle=True)\n",
    "        test_data_loader = DataLoader(dataset=self.testset, batch_size=self.opt.batch_size, shuffle=False)\n",
    "        val_data_loader = DataLoader(dataset=self.valset, batch_size=self.opt.batch_size, shuffle=False)\n",
    "        full_train_data_loader = DataLoader(dataset = self.fulltrainset, batch_size = self.opt.batch_size, shuffle=True)\n",
    "\n",
    "        self._reset_params()\n",
    "        max_val_acc, test_acc, test_f1 = self._train(criterion, optimizer, train_data_loader, val_data_loader, test_data_loader)\n",
    "        return max_val_acc, test_acc, test_f1\n",
    "\n",
    "\n",
    "def test(opt):\n",
    "    logger.info(opt)\n",
    "    config = BertConfig.from_json_file(os.path.join(opt.model_path, CONFIG_NAME))\n",
    "    logger.info(config)\n",
    "\n",
    "    tokenizer = Tokenizer4Bert(opt.max_seq_len, opt.model_path)\n",
    "    if opt.model_type == 'tgcn':\n",
    "        model = AsaTgcn.from_pretrained(opt.model_path)\n",
    "    elif opt.model_type == 'tgcn+sem':\n",
    "        model = AsaTgcnSem.from_pretrained(opt.model_path)\n",
    "    model.set_dropout(opt.dropout)\n",
    "    model.to(opt.device)\n",
    "\n",
    "    deptype2id = config.deptype2id\n",
    "    logger.info(deptype2id)\n",
    "    testset = ABSADataset(opt.test_file, tokenizer, opt, deptype2id=deptype2id)\n",
    "    test_data_loader = DataLoader(dataset=testset, batch_size=opt.batch_size, shuffle=False)\n",
    "    test_acc, test_f1 = Instructor._evaluate_acc_f1(model, test_data_loader, device=opt.device)\n",
    "    logger.info('>> test_acc: {:.4f}, test_f1: {:.4f}'.format(test_acc, test_f1))\n",
    "\n",
    "\n",
    "def get_args(model_type = 'tgcn', # tgcn, tgcn+sem, tri_gcn\n",
    "             # Select which modules to use for hybrid model\n",
    "             tgcn = True,\n",
    "             semgcn = True, \n",
    "             lexgcn = True,\n",
    "             tgcn_layers = 3,\n",
    "             semgcn_layers = 2,\n",
    "             lexgcn_layers = 2,\n",
    "             path = None, \n",
    "             year='2015',\n",
    "             val_file='val.txt',\n",
    "             log = 'log',\n",
    "             bert_model='bert_large_uncased',\n",
    "             cooc_path = 'cooc_matrix.csv', # Path to co-occurrence matrix file\n",
    "             cooc = None, # Pandas DataFrame co-occurrence matrix. If not specified, it will be loaded from cooc_path\n",
    "             learning_rate=2e-5,\n",
    "             dropout=0.2,\n",
    "             concat_dropout = 0.5,\n",
    "             bert_dropout=0.2,\n",
    "             l2reg=0.01,\n",
    "             num_epoch=50,\n",
    "             batch_size=16,\n",
    "             log_step=5,\n",
    "             max_seq_len=100,\n",
    "             polarities_dim=3,\n",
    "             device='cuda',\n",
    "             seed=50,\n",
    "             valset_ratio=0.1,\n",
    "             do_train=True,\n",
    "             do_eval=True,\n",
    "             eval_epoch_num=0,\n",
    "             fusion_type = 'concat', # 'concat' or 'gate'\n",
    "             use_ensemble = True, \n",
    "            save_models='last',\n",
    "            print_sentences = False,\n",
    "             optim = 'adam'\n",
    "            ):\n",
    "    assert model_type == 'tgcn' or model_type == 'tgcn+sem' or model_type == 'tri_gcn'\n",
    "    opt = argparse.Namespace()\n",
    "    opt.model_type = model_type\n",
    "    opt.modules = {'tgcn': tgcn, 'semgcn': semgcn, 'lexgcn': lexgcn}\n",
    "    opt.num_layers = {'tgcn': tgcn_layers, 'semgcn': semgcn_layers, 'lexgcn': lexgcn_layers}\n",
    "    opt.year = year\n",
    "    fusion = \"\" if model_type == 'tgcn' else \"+\" + fusion_type\n",
    "    opt.train_file = f'data/train{year}restaurant.txt'\n",
    "    opt.test_file = f'data/test{year}restaurant.txt'\n",
    "    opt.model_path = f'test_models/{year}{model_type}{fusion}_seed{seed}_reg{l2reg}_drop{dropout}_cdrop{concat_dropout}_lr{learning_rate}_epochs{num_epoch}_{optim.lower()}'\n",
    "#     if model_type == 'tgcn':\n",
    "#         opt.model_path = f'models/rest_{year}/BERT.L_seed{seed}_reg{l2reg}_drop{dropout}_lr{learning_rate}_epochs{num_epoch}' \n",
    "#     elif model_type == 'tgcn+sem':\n",
    "#         opt.model_path = f'models/rest_{year}/{model_type}/{model_type}_seed{seed}_reg{l2reg}_drop{dropout}_lr{learning_rate}_epochs{num_epoch}'\n",
    "    if do_eval and not do_train:\n",
    "        opt.model_path += f'/epoch_{eval_epoch_num}'\n",
    "    if path:\n",
    "        opt.model_path = path\n",
    "    opt.val_file = val_file\n",
    "    opt.log = log\n",
    "    opt.bert_model = bert_model\n",
    "    opt.cooc_path = cooc_path\n",
    "    opt.cooc = cooc\n",
    "    opt.learning_rate = learning_rate\n",
    "    opt.dropout = dropout\n",
    "    opt.concat_dropout = concat_dropout\n",
    "    opt.bert_dropout = bert_dropout\n",
    "    opt.l2reg = l2reg\n",
    "    opt.num_epoch = num_epoch\n",
    "    opt.batch_size = batch_size\n",
    "    opt.log_step = log_step\n",
    "    opt.max_seq_len = max_seq_len\n",
    "    opt.polarities_dim = polarities_dim\n",
    "    opt.device = device\n",
    "    opt.seed = seed\n",
    "    opt.valset_ratio = valset_ratio\n",
    "    opt.do_train = do_train\n",
    "    opt.do_eval = do_eval\n",
    "    opt.eval_epoch_num = eval_epoch_num\n",
    "    opt.fusion_type = fusion_type\n",
    "    opt.use_ensemble = True\n",
    "    opt.save_models = save_models\n",
    "    opt.print_sent = print_sentences\n",
    "    opt.optim = optim\n",
    "    return opt\n",
    "\n",
    "\n",
    "def set_seed(opt):\n",
    "    if opt.seed is not None:\n",
    "        random.seed(opt.seed)\n",
    "        np.random.seed(opt.seed)\n",
    "        torch.manual_seed(opt.seed)\n",
    "        torch.cuda.manual_seed(opt.seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "        \n",
    "\n",
    "def main(opt):\n",
    "    opt = opt\n",
    "    set_seed(opt)\n",
    "\n",
    "    opt.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \\\n",
    "        if opt.device is None else torch.device(opt.device)\n",
    "    opt.n_gpu = torch.cuda.device_count()\n",
    "\n",
    "    if not os.path.exists(opt.log):\n",
    "        os.makedirs(opt.log)\n",
    "\n",
    "    log_file = '{}/log-{}.log'.format(opt.log, strftime(\"%y%m%d-%H%M\", localtime()))\n",
    "    logger.addHandler(logging.FileHandler(log_file))\n",
    "\n",
    "    if opt.do_train:\n",
    "        ins = Instructor(opt)\n",
    "        max_val_acc, test_acc, test_f1 = ins.train()\n",
    "    elif opt.do_eval:\n",
    "        test(opt)\n",
    "    \n",
    "    return max_val_acc, test_acc, test_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a91a3f6",
   "metadata": {},
   "source": [
    "### Loading cooc matrix so we don't have to load it for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e9e0f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cooc = pd.read_csv('cooc_matrix_ids.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5406b261",
   "metadata": {},
   "source": [
    "# Run program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a33c9d",
   "metadata": {},
   "source": [
    "Hyperparameter searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c5ee4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "csv_file = 'final_results.csv'\n",
    "\n",
    "res = {'2015': [], '2016': []}\n",
    "\n",
    "for i in range(40):\n",
    "    lr = np.random.choice(np.logspace(-6, -3))\n",
    "    d = np.random.choice([0.1, 0.2, 0.4])\n",
    "    cdrop = np.random.choice([0.1, 0.2, 0.4])\n",
    "    w_decay = np.random.choice(np.logspace(-5, -3))\n",
    "    seed = np.random.randint(1000)\n",
    "    year = '2016'\n",
    "    opt = get_args(batch_size = 16, seed = seed, dropout = d,\n",
    "                  l2reg = w_decay, learning_rate = lr, year = year,\n",
    "                  num_epoch = 20, model_type = 'tri_gcn', save_models = 'none', fusion_type = 'concat',\n",
    "                  concat_dropout = cdrop, cooc = cooc, tgcn = True, semgcn = True, lexgcn = True, use_ensemble = True,\n",
    "                  tgcn_layers = 2, semgcn_layers = 2, lexgcn_layers = 2, optim = 'adam')\n",
    "    opt.device = torch.device('cuda')\n",
    "    max_val_acc, test_acc, test_f1 = main(opt)\n",
    "    res[year].append((opt, max_val_acc, test_acc, test_f1))\n",
    "    try:\n",
    "        with open(csv_file, 'a', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow((year, max_val_acc, test_acc, test_f1, seed, lr, d, cdrop, w_decay))\n",
    "    finally:\n",
    "        print('FAILED SOME SHIT')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bc0ed2",
   "metadata": {},
   "source": [
    "Training with best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8edfdc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(model_type='tri_gcn', modules={'tgcn': True, 'semgcn': True, 'lexgcn': True}, num_layers={'tgcn': 2, 'semgcn': 2, 'lexgcn': 2}, year='2016', train_file='data/train2016restaurant.txt', test_file='data/test2016restaurant.txt', model_path='test_models/2016tri_gcn+concat_seed925_reg0.0005_drop0.4_cdrop0.4_lr2.5e-05_epochs20_adam', val_file='val.txt', log='log', bert_model='bert_large_uncased', cooc_path='cooc_matrix.csv', cooc=       100      1000  10000  10003     10005     10007     10009      1001  \\\n",
      "100    0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "1000   0.0  0.000000    0.0    0.0  0.000606  0.000303  0.000606  0.035120   \n",
      "10000  0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.333333   \n",
      "10003  0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "10005  0.0  0.333333    0.0    0.0  0.000000  0.166667  0.000000  0.166667   \n",
      "...    ...       ...    ...    ...       ...       ...       ...       ...   \n",
      "9992   0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "9993   0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "9996   0.0  0.500000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "9997   0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "9998   0.0  0.400000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "       10010  10015  ...      9986      9987       999  9990      9991  9992  \\\n",
      "100      0.0    0.0  ...  0.000000  0.000000  0.000000   0.0  0.000000   0.0   \n",
      "1000     0.0    0.0  ...  0.002422  0.000303  0.062368   0.0  0.000606   0.0   \n",
      "10000    0.0    0.0  ...  0.000000  0.000000  0.000000   0.0  0.000000   0.0   \n",
      "10003    0.0    0.0  ...  0.000000  0.000000  0.000000   0.0  0.000000   0.0   \n",
      "10005    0.0    0.0  ...  0.000000  0.000000  0.333333   0.0  0.000000   0.0   \n",
      "...      ...    ...  ...       ...       ...       ...   ...       ...   ...   \n",
      "9992     0.0    0.0  ...  0.000000  0.000000  0.000000   0.0  0.000000   0.0   \n",
      "9993     0.0    0.0  ...  0.000000  0.000000  0.000000   0.0  0.000000   0.0   \n",
      "9996     0.0    0.0  ...  0.000000  0.000000  0.000000   0.0  0.000000   0.0   \n",
      "9997     0.0    0.0  ...  0.000000  0.000000  0.000000   0.0  0.000000   0.0   \n",
      "9998     0.0    0.0  ...  0.000000  0.000000  0.000000   0.0  0.000000   0.0   \n",
      "\n",
      "       9993      9996  9997      9998  \n",
      "100     0.0  0.000000   0.0  0.000000  \n",
      "1000    0.0  0.000606   0.0  0.001211  \n",
      "10000   0.0  0.000000   0.0  0.000000  \n",
      "10003   0.0  0.000000   0.0  0.000000  \n",
      "10005   0.0  0.000000   0.0  0.000000  \n",
      "...     ...       ...   ...       ...  \n",
      "9992    0.0  0.000000   0.0  0.000000  \n",
      "9993    0.0  0.000000   0.0  0.000000  \n",
      "9996    0.0  0.000000   0.0  0.000000  \n",
      "9997    0.0  0.000000   0.0  0.000000  \n",
      "9998    0.0  0.000000   0.0  0.000000  \n",
      "\n",
      "[13075 rows x 13075 columns], learning_rate=2.5e-05, dropout=0.4, concat_dropout=0.4, bert_dropout=0.2, l2reg=0.0005, num_epoch=20, batch_size=16, log_step=5, max_seq_len=100, polarities_dim=3, device=device(type='cuda'), seed=925, valset_ratio=0.1, do_train=True, do_eval=True, eval_epoch_num=0, fusion_type='concat', use_ensemble=True, save_models='none', print_sent=False, optim='adam', n_gpu=1)\n",
      "{'none': 0, 'acl': 1, 'acl:relcl': 2, 'advcl': 3, 'advcl:relcl': 4, 'advmod': 5, 'amod': 6, 'appos': 7, 'aux': 8, 'aux:pass': 9, 'case': 10, 'cc': 11, 'cc:preconj': 12, 'ccomp': 13, 'compound': 14, 'compound:prt': 15, 'conj': 16, 'cop': 17, 'csubj': 18, 'dep': 19, 'det': 20, 'det:predet': 21, 'discourse': 22, 'dislocated': 23, 'expl': 24, 'fixed': 25, 'flat': 26, 'iobj': 27, 'list': 28, 'mark': 29, 'nmod': 30, 'nmod:npmod': 31, 'nmod:poss': 32, 'nmod:tmod': 33, 'nsubj': 34, 'nsubj:outer': 35, 'nsubj:pass': 36, 'nummod': 37, 'obj': 38, 'obl': 39, 'obl:agent': 40, 'obl:npmod': 41, 'obl:tmod': 42, 'parataxis': 43, 'punct': 44, 'reparandum': 45, 'root': 46, 'vocative': 47, 'xcomp': 48}\n",
      "{'-1': 0, '0': 1, '1': 2}\n",
      "Model name 'bert_large_uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming 'bert_large_uncased' is a path or url to a directory containing tokenizer files.\n",
      "Didn't find file bert_large_uncased/added_tokens.json. We won't load it.\n",
      "Didn't find file bert_large_uncased/special_tokens_map.json. We won't load it.\n",
      "Didn't find file bert_large_uncased/tokenizer_config.json. We won't load it.\n",
      "loading file bert_large_uncased/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "{\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 3,\n",
      "  \"num_types\": 49,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file bert_large_uncased/pytorch_model.bin\n",
      "Weights of AsaTgcnSem not initialized from pretrained model: ['gate_weight', 'gate_bias', 'ensemble', 'TGCNLayers.0.weight', 'TGCNLayers.0.bias', 'TGCNLayers.0.dense.weight', 'TGCNLayers.1.weight', 'TGCNLayers.1.bias', 'TGCNLayers.1.dense.weight', 'SemGCNLayers.0.weight', 'SemGCNLayers.0.bias', 'SemGCNLayers.1.weight', 'SemGCNLayers.1.bias', 'LexGCNLayers.0.weight', 'LexGCNLayers.0.bias', 'LexGCNLayers.1.weight', 'LexGCNLayers.1.bias', 'fc_single.weight', 'fc_single.bias', 'ensemble_linear_tgcn.weight', 'ensemble_linear_tgcn.bias', 'ensemble_linear_semgcn.weight', 'ensemble_linear_semgcn.bias', 'ensemble_linear_lexgcn.weight', 'ensemble_linear_lexgcn.bias', 'dep_embedding.weight']\n",
      "Weights from pretrained model not used in AsaTgcnSem: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "[{'input_ids': tensor([  101, 13325,  2013,  3025,  8466,  2023,  2109,  2000,  2022,  1037,\n",
      "         2204,  2173,  1010,  2021,  2025,  2151,  2936,  1012,   102,  2173,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'valid_ids': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'segment_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'mem_valid_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'dep_adj_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'dep_value_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'polarity': 0, 'raw_text': 'judging from previous posts this used to be a good place , but not any longer .', 'aspect': 'place'}]\n",
      "[{'input_ids': tensor([  101, 13325,  2013,  3025,  8466,  2023,  2109,  2000,  2022,  1037,\n",
      "         2204,  2173,  1010,  2021,  2025,  2151,  2936,  1012,   102,  2173,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'valid_ids': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'segment_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'mem_valid_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'dep_adj_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'dep_value_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'polarity': 0, 'raw_text': 'judging from previous posts this used to be a good place , but not any longer .', 'aspect': 'place'}]\n",
      "[{'input_ids': tensor([  101,  4240,  2428,  2204, 10514,  6182,  1012,   102, 10514,  6182,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'valid_ids': tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'segment_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'mem_valid_ids': tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'dep_adj_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'dep_value_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 5,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'polarity': 2, 'raw_text': 'serves really good sushi .', 'aspect': 'sushi'}]\n",
      "cuda memory allocated: 1383591936\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 0\n",
      "epoch: 0, loss: 1.0337, train acc: 0.5500\n",
      "epoch: 0, loss: 0.9878, train acc: 0.6000\n",
      "epoch: 0, loss: 0.9327, train acc: 0.6292\n",
      "epoch: 0, loss: 0.8777, train acc: 0.6438\n",
      "epoch: 0, loss: 0.8678, train acc: 0.6475\n",
      "epoch: 0, loss: 0.8840, train acc: 0.6354\n",
      "epoch: 0, loss: 0.8816, train acc: 0.6339\n",
      "epoch: 0, loss: 0.8772, train acc: 0.6266\n",
      "epoch: 0, loss: 0.8560, train acc: 0.6375\n",
      "epoch: 0, loss: 0.8457, train acc: 0.6462\n",
      "epoch: 0, loss: 0.8333, train acc: 0.6557\n",
      "epoch: 0, loss: 0.8214, train acc: 0.6594\n",
      "epoch: 0, loss: 0.8078, train acc: 0.6644\n",
      "epoch: 0, loss: 0.8048, train acc: 0.6571\n",
      "epoch: 0, loss: 0.7985, train acc: 0.6550\n",
      "epoch: 0, loss: 0.7901, train acc: 0.6617\n",
      "epoch: 0, loss: 0.7835, train acc: 0.6603\n",
      "epoch: 0, loss: 0.7790, train acc: 0.6604\n",
      "epoch: 0, loss: 0.7722, train acc: 0.6618\n",
      "epoch: 0, loss: 0.7801, train acc: 0.6613\n",
      "epoch: 0, loss: 0.7707, train acc: 0.6661\n",
      ">epoch: 0, val_acc: 0.7234, val_f1: 0.3983\n",
      ">> epoch: 0, test_acc: 0.8062, test_f1: 0.4837\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 1\n",
      "epoch: 1, loss: 0.5687, train acc: 0.8281\n",
      "epoch: 1, loss: 0.6061, train acc: 0.7639\n",
      "epoch: 1, loss: 0.5895, train acc: 0.7634\n",
      "epoch: 1, loss: 0.5821, train acc: 0.7599\n",
      "epoch: 1, loss: 0.5743, train acc: 0.7708\n",
      "epoch: 1, loss: 0.5435, train acc: 0.7866\n",
      "epoch: 1, loss: 0.5406, train acc: 0.7886\n",
      "epoch: 1, loss: 0.5255, train acc: 0.7917\n",
      "epoch: 1, loss: 0.5301, train acc: 0.7926\n",
      "epoch: 1, loss: 0.5303, train acc: 0.7959\n",
      "epoch: 1, loss: 0.5154, train acc: 0.8009\n",
      "epoch: 1, loss: 0.5066, train acc: 0.8051\n",
      "epoch: 1, loss: 0.4956, train acc: 0.8125\n",
      "epoch: 1, loss: 0.4907, train acc: 0.8170\n",
      "epoch: 1, loss: 0.4733, train acc: 0.8260\n",
      "epoch: 1, loss: 0.4688, train acc: 0.8267\n",
      "epoch: 1, loss: 0.4605, train acc: 0.8311\n",
      "epoch: 1, loss: 0.4658, train acc: 0.8294\n",
      "epoch: 1, loss: 0.4554, train acc: 0.8324\n",
      "epoch: 1, loss: 0.4549, train acc: 0.8327\n",
      "epoch: 1, loss: 0.4485, train acc: 0.8353\n",
      ">epoch: 1, val_acc: 0.8723, val_f1: 0.5736\n",
      ">> epoch: 1, test_acc: 0.8754, test_f1: 0.5691\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 2\n",
      "epoch: 2, loss: 0.1920, train acc: 0.9375\n",
      "epoch: 2, loss: 0.2537, train acc: 0.9062\n",
      "epoch: 2, loss: 0.2793, train acc: 0.8990\n",
      "epoch: 2, loss: 0.2749, train acc: 0.9028\n",
      "epoch: 2, loss: 0.2830, train acc: 0.9022\n",
      "epoch: 2, loss: 0.2802, train acc: 0.9018\n",
      "epoch: 2, loss: 0.2883, train acc: 0.9053\n",
      "epoch: 2, loss: 0.2800, train acc: 0.9079\n",
      "epoch: 2, loss: 0.2670, train acc: 0.9113\n",
      "epoch: 2, loss: 0.3002, train acc: 0.9023\n",
      "epoch: 2, loss: 0.3106, train acc: 0.8927\n",
      "epoch: 2, loss: 0.3082, train acc: 0.8933\n",
      "epoch: 2, loss: 0.3033, train acc: 0.8958\n",
      "epoch: 2, loss: 0.3090, train acc: 0.8952\n",
      "epoch: 2, loss: 0.3185, train acc: 0.8930\n",
      "epoch: 2, loss: 0.3177, train acc: 0.8926\n",
      "epoch: 2, loss: 0.3137, train acc: 0.8946\n",
      "epoch: 2, loss: 0.3111, train acc: 0.8942\n",
      "epoch: 2, loss: 0.3150, train acc: 0.8911\n",
      "epoch: 2, loss: 0.3235, train acc: 0.8865\n",
      "epoch: 2, loss: 0.3149, train acc: 0.8902\n",
      ">epoch: 2, val_acc: 0.8670, val_f1: 0.5696\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 3\n",
      "epoch: 3, loss: 0.2408, train acc: 0.9062\n",
      "epoch: 3, loss: 0.2421, train acc: 0.9107\n",
      "epoch: 3, loss: 0.1954, train acc: 0.9271\n",
      "epoch: 3, loss: 0.2267, train acc: 0.9154\n",
      "epoch: 3, loss: 0.2124, train acc: 0.9233\n",
      "epoch: 3, loss: 0.2230, train acc: 0.9167\n",
      "epoch: 3, loss: 0.2249, train acc: 0.9199\n",
      "epoch: 3, loss: 0.2231, train acc: 0.9189\n",
      "epoch: 3, loss: 0.2225, train acc: 0.9182\n",
      "epoch: 3, loss: 0.2180, train acc: 0.9202\n",
      "epoch: 3, loss: 0.2103, train acc: 0.9231\n",
      "epoch: 3, loss: 0.2033, train acc: 0.9254\n",
      "epoch: 3, loss: 0.1953, train acc: 0.9284\n",
      "epoch: 3, loss: 0.2042, train acc: 0.9282\n",
      "epoch: 3, loss: 0.1981, train acc: 0.9288\n",
      "epoch: 3, loss: 0.2017, train acc: 0.9286\n",
      "epoch: 3, loss: 0.2025, train acc: 0.9306\n",
      "epoch: 3, loss: 0.1991, train acc: 0.9318\n",
      "epoch: 3, loss: 0.2000, train acc: 0.9307\n",
      "epoch: 3, loss: 0.1962, train acc: 0.9323\n",
      "epoch: 3, loss: 0.1993, train acc: 0.9308\n",
      ">epoch: 3, val_acc: 0.8883, val_f1: 0.7019\n",
      ">> epoch: 3, test_acc: 0.8985, test_f1: 0.6751\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 4\n",
      "epoch: 4, loss: 0.2231, train acc: 0.9375\n",
      "epoch: 4, loss: 0.1376, train acc: 0.9479\n",
      "epoch: 4, loss: 0.1247, train acc: 0.9545\n",
      "epoch: 4, loss: 0.1118, train acc: 0.9609\n",
      "epoch: 4, loss: 0.1344, train acc: 0.9494\n",
      "epoch: 4, loss: 0.1483, train acc: 0.9423\n",
      "epoch: 4, loss: 0.1523, train acc: 0.9456\n",
      "epoch: 4, loss: 0.1766, train acc: 0.9410\n",
      "epoch: 4, loss: 0.1796, train acc: 0.9405\n",
      "epoch: 4, loss: 0.1726, train acc: 0.9429\n",
      "epoch: 4, loss: 0.1658, train acc: 0.9449\n",
      "epoch: 4, loss: 0.1760, train acc: 0.9420\n",
      "epoch: 4, loss: 0.1760, train acc: 0.9406\n",
      "epoch: 4, loss: 0.1744, train acc: 0.9403\n",
      "epoch: 4, loss: 0.1840, train acc: 0.9366\n",
      "epoch: 4, loss: 0.1866, train acc: 0.9350\n",
      "epoch: 4, loss: 0.1888, train acc: 0.9352\n",
      "epoch: 4, loss: 0.1918, train acc: 0.9346\n",
      "epoch: 4, loss: 0.1958, train acc: 0.9327\n",
      "epoch: 4, loss: 0.1908, train acc: 0.9342\n",
      "epoch: 4, loss: 0.1900, train acc: 0.9350\n",
      "epoch: 4, loss: 0.1899, train acc: 0.9362\n",
      ">epoch: 4, val_acc: 0.9043, val_f1: 0.7513\n",
      ">> epoch: 4, test_acc: 0.9077, test_f1: 0.7745\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 5\n",
      "epoch: 5, loss: 0.1238, train acc: 0.9625\n",
      "epoch: 5, loss: 0.1240, train acc: 0.9500\n",
      "epoch: 5, loss: 0.1151, train acc: 0.9500\n",
      "epoch: 5, loss: 0.0997, train acc: 0.9563\n",
      "epoch: 5, loss: 0.1117, train acc: 0.9550\n",
      "epoch: 5, loss: 0.1070, train acc: 0.9563\n",
      "epoch: 5, loss: 0.1248, train acc: 0.9536\n",
      "epoch: 5, loss: 0.1169, train acc: 0.9578\n",
      "epoch: 5, loss: 0.1112, train acc: 0.9597\n",
      "epoch: 5, loss: 0.1096, train acc: 0.9600\n",
      "epoch: 5, loss: 0.1096, train acc: 0.9614\n",
      "epoch: 5, loss: 0.1184, train acc: 0.9615\n",
      "epoch: 5, loss: 0.1158, train acc: 0.9606\n",
      "epoch: 5, loss: 0.1216, train acc: 0.9589\n",
      "epoch: 5, loss: 0.1247, train acc: 0.9583\n",
      "epoch: 5, loss: 0.1288, train acc: 0.9570\n",
      "epoch: 5, loss: 0.1306, train acc: 0.9566\n",
      "epoch: 5, loss: 0.1349, train acc: 0.9549\n",
      "epoch: 5, loss: 0.1354, train acc: 0.9539\n",
      "epoch: 5, loss: 0.1335, train acc: 0.9537\n",
      "epoch: 5, loss: 0.1340, train acc: 0.9542\n",
      ">epoch: 5, val_acc: 0.8830, val_f1: 0.7330\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 6\n",
      "epoch: 6, loss: 0.0820, train acc: 0.9688\n",
      "epoch: 6, loss: 0.0815, train acc: 0.9653\n",
      "epoch: 6, loss: 0.0745, train acc: 0.9732\n",
      "epoch: 6, loss: 0.1141, train acc: 0.9605\n",
      "epoch: 6, loss: 0.1137, train acc: 0.9609\n",
      "epoch: 6, loss: 0.1176, train acc: 0.9612\n",
      "epoch: 6, loss: 0.1104, train acc: 0.9632\n",
      "epoch: 6, loss: 0.1041, train acc: 0.9647\n",
      "epoch: 6, loss: 0.1089, train acc: 0.9631\n",
      "epoch: 6, loss: 0.1035, train acc: 0.9656\n",
      "epoch: 6, loss: 0.1089, train acc: 0.9641\n",
      "epoch: 6, loss: 0.1069, train acc: 0.9640\n",
      "epoch: 6, loss: 0.1051, train acc: 0.9639\n",
      "epoch: 6, loss: 0.1053, train acc: 0.9638\n",
      "epoch: 6, loss: 0.1038, train acc: 0.9645\n",
      "epoch: 6, loss: 0.1035, train acc: 0.9652\n",
      "epoch: 6, loss: 0.0993, train acc: 0.9673\n",
      "epoch: 6, loss: 0.0984, train acc: 0.9677\n",
      "epoch: 6, loss: 0.0984, train acc: 0.9674\n",
      "epoch: 6, loss: 0.1001, train acc: 0.9665\n",
      "epoch: 6, loss: 0.1004, train acc: 0.9663\n",
      ">epoch: 6, val_acc: 0.8989, val_f1: 0.7576\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 7\n",
      "epoch: 7, loss: 0.0732, train acc: 0.9583\n",
      "epoch: 7, loss: 0.1440, train acc: 0.9531\n",
      "epoch: 7, loss: 0.1017, train acc: 0.9663\n",
      "epoch: 7, loss: 0.1005, train acc: 0.9688\n",
      "epoch: 7, loss: 0.1144, train acc: 0.9674\n",
      "epoch: 7, loss: 0.1059, train acc: 0.9688\n",
      "epoch: 7, loss: 0.0939, train acc: 0.9735\n",
      "epoch: 7, loss: 0.0887, train acc: 0.9753\n",
      "epoch: 7, loss: 0.0822, train acc: 0.9767\n",
      "epoch: 7, loss: 0.0794, train acc: 0.9766\n",
      "epoch: 7, loss: 0.0791, train acc: 0.9764\n",
      "epoch: 7, loss: 0.0829, train acc: 0.9752\n",
      "epoch: 7, loss: 0.0871, train acc: 0.9712\n",
      "epoch: 7, loss: 0.0861, train acc: 0.9715\n",
      "epoch: 7, loss: 0.0900, train acc: 0.9692\n",
      "epoch: 7, loss: 0.0890, train acc: 0.9696\n",
      "epoch: 7, loss: 0.0856, train acc: 0.9706\n",
      "epoch: 7, loss: 0.0845, train acc: 0.9709\n",
      "epoch: 7, loss: 0.0830, train acc: 0.9711\n",
      "epoch: 7, loss: 0.0869, train acc: 0.9694\n",
      "epoch: 7, loss: 0.0879, train acc: 0.9703\n",
      ">epoch: 7, val_acc: 0.8989, val_f1: 0.7783\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 8\n",
      "epoch: 8, loss: 0.1023, train acc: 0.9688\n",
      "epoch: 8, loss: 0.0818, train acc: 0.9643\n",
      "epoch: 8, loss: 0.0643, train acc: 0.9740\n",
      "epoch: 8, loss: 0.0699, train acc: 0.9743\n",
      "epoch: 8, loss: 0.0731, train acc: 0.9688\n",
      "epoch: 8, loss: 0.0700, train acc: 0.9676\n",
      "epoch: 8, loss: 0.0652, train acc: 0.9707\n",
      "epoch: 8, loss: 0.0617, train acc: 0.9730\n",
      "epoch: 8, loss: 0.0729, train acc: 0.9717\n",
      "epoch: 8, loss: 0.0772, train acc: 0.9707\n",
      "epoch: 8, loss: 0.0772, train acc: 0.9712\n",
      "epoch: 8, loss: 0.0771, train acc: 0.9715\n",
      "epoch: 8, loss: 0.0756, train acc: 0.9718\n",
      "epoch: 8, loss: 0.0778, train acc: 0.9711\n",
      "epoch: 8, loss: 0.0745, train acc: 0.9731\n",
      "epoch: 8, loss: 0.0717, train acc: 0.9748\n",
      "epoch: 8, loss: 0.0688, train acc: 0.9764\n",
      "epoch: 8, loss: 0.0686, train acc: 0.9763\n",
      "epoch: 8, loss: 0.0690, train acc: 0.9769\n",
      "epoch: 8, loss: 0.0710, train acc: 0.9768\n",
      "epoch: 8, loss: 0.0693, train acc: 0.9773\n",
      ">epoch: 8, val_acc: 0.9043, val_f1: 0.7611\n",
      ">> epoch: 8, test_acc: 0.9108, test_f1: 0.7996\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 9\n",
      "epoch: 9, loss: 0.0318, train acc: 1.0000\n",
      "epoch: 9, loss: 0.0426, train acc: 0.9896\n",
      "epoch: 9, loss: 0.0379, train acc: 0.9886\n",
      "epoch: 9, loss: 0.0380, train acc: 0.9883\n",
      "epoch: 9, loss: 0.0566, train acc: 0.9792\n",
      "epoch: 9, loss: 0.0630, train acc: 0.9784\n",
      "epoch: 9, loss: 0.0571, train acc: 0.9819\n",
      "epoch: 9, loss: 0.0497, train acc: 0.9844\n",
      "epoch: 9, loss: 0.0470, train acc: 0.9848\n",
      "epoch: 9, loss: 0.0549, train acc: 0.9810\n",
      "epoch: 9, loss: 0.0549, train acc: 0.9804\n",
      "epoch: 9, loss: 0.0607, train acc: 0.9788\n",
      "epoch: 9, loss: 0.0591, train acc: 0.9775\n",
      "epoch: 9, loss: 0.0571, train acc: 0.9782\n",
      "epoch: 9, loss: 0.0654, train acc: 0.9754\n",
      "epoch: 9, loss: 0.0665, train acc: 0.9753\n",
      "epoch: 9, loss: 0.0667, train acc: 0.9745\n",
      "epoch: 9, loss: 0.0663, train acc: 0.9746\n",
      "epoch: 9, loss: 0.0698, train acc: 0.9725\n",
      "epoch: 9, loss: 0.0689, train acc: 0.9727\n",
      "epoch: 9, loss: 0.0682, train acc: 0.9728\n",
      "epoch: 9, loss: 0.0669, train acc: 0.9734\n",
      ">epoch: 9, val_acc: 0.9043, val_f1: 0.7495\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 10\n",
      "epoch: 10, loss: 0.0245, train acc: 0.9875\n",
      "epoch: 10, loss: 0.0659, train acc: 0.9750\n",
      "epoch: 10, loss: 0.0575, train acc: 0.9792\n",
      "epoch: 10, loss: 0.0537, train acc: 0.9812\n",
      "epoch: 10, loss: 0.0632, train acc: 0.9800\n",
      "epoch: 10, loss: 0.0538, train acc: 0.9833\n",
      "epoch: 10, loss: 0.0549, train acc: 0.9821\n",
      "epoch: 10, loss: 0.0572, train acc: 0.9828\n",
      "epoch: 10, loss: 0.0560, train acc: 0.9806\n",
      "epoch: 10, loss: 0.0558, train acc: 0.9800\n",
      "epoch: 10, loss: 0.0548, train acc: 0.9795\n",
      "epoch: 10, loss: 0.0561, train acc: 0.9792\n",
      "epoch: 10, loss: 0.0570, train acc: 0.9788\n",
      "epoch: 10, loss: 0.0581, train acc: 0.9786\n",
      "epoch: 10, loss: 0.0567, train acc: 0.9783\n",
      "epoch: 10, loss: 0.0576, train acc: 0.9781\n",
      "epoch: 10, loss: 0.0557, train acc: 0.9787\n",
      "epoch: 10, loss: 0.0576, train acc: 0.9785\n",
      "epoch: 10, loss: 0.0575, train acc: 0.9783\n",
      "epoch: 10, loss: 0.0570, train acc: 0.9781\n",
      "epoch: 10, loss: 0.0562, train acc: 0.9780\n",
      ">epoch: 10, val_acc: 0.9043, val_f1: 0.7837\n",
      ">> epoch: 10, test_acc: 0.8954, test_f1: 0.7384\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 11\n",
      "epoch: 11, loss: 0.0369, train acc: 0.9844\n",
      "epoch: 11, loss: 0.0275, train acc: 0.9861\n",
      "epoch: 11, loss: 0.0252, train acc: 0.9866\n",
      "epoch: 11, loss: 0.0315, train acc: 0.9868\n",
      "epoch: 11, loss: 0.0255, train acc: 0.9896\n",
      "epoch: 11, loss: 0.0215, train acc: 0.9914\n",
      "epoch: 11, loss: 0.0202, train acc: 0.9926\n",
      "epoch: 11, loss: 0.0192, train acc: 0.9920\n",
      "epoch: 11, loss: 0.0196, train acc: 0.9915\n",
      "epoch: 11, loss: 0.0203, train acc: 0.9911\n",
      "epoch: 11, loss: 0.0220, train acc: 0.9907\n",
      "epoch: 11, loss: 0.0203, train acc: 0.9915\n",
      "epoch: 11, loss: 0.0201, train acc: 0.9912\n",
      "epoch: 11, loss: 0.0202, train acc: 0.9909\n",
      "epoch: 11, loss: 0.0282, train acc: 0.9873\n",
      "epoch: 11, loss: 0.0299, train acc: 0.9866\n",
      "epoch: 11, loss: 0.0318, train acc: 0.9859\n",
      "epoch: 11, loss: 0.0366, train acc: 0.9838\n",
      "epoch: 11, loss: 0.0433, train acc: 0.9820\n",
      "epoch: 11, loss: 0.0432, train acc: 0.9817\n",
      "epoch: 11, loss: 0.0442, train acc: 0.9808\n",
      ">epoch: 11, val_acc: 0.8989, val_f1: 0.7783\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 12\n",
      "epoch: 12, loss: 0.0400, train acc: 0.9792\n",
      "epoch: 12, loss: 0.0255, train acc: 0.9844\n",
      "epoch: 12, loss: 0.0306, train acc: 0.9808\n",
      "epoch: 12, loss: 0.0391, train acc: 0.9792\n",
      "epoch: 12, loss: 0.0314, train acc: 0.9837\n",
      "epoch: 12, loss: 0.0276, train acc: 0.9866\n",
      "epoch: 12, loss: 0.0248, train acc: 0.9886\n",
      "epoch: 12, loss: 0.0252, train acc: 0.9885\n",
      "epoch: 12, loss: 0.0359, train acc: 0.9884\n",
      "epoch: 12, loss: 0.0400, train acc: 0.9857\n",
      "epoch: 12, loss: 0.0397, train acc: 0.9858\n",
      "epoch: 12, loss: 0.0416, train acc: 0.9849\n",
      "epoch: 12, loss: 0.0436, train acc: 0.9831\n",
      "epoch: 12, loss: 0.0426, train acc: 0.9835\n",
      "epoch: 12, loss: 0.0404, train acc: 0.9846\n",
      "epoch: 12, loss: 0.0446, train acc: 0.9816\n",
      "epoch: 12, loss: 0.0454, train acc: 0.9812\n",
      "epoch: 12, loss: 0.0510, train acc: 0.9794\n",
      "epoch: 12, loss: 0.0497, train acc: 0.9798\n",
      "epoch: 12, loss: 0.0475, train acc: 0.9809\n",
      "epoch: 12, loss: 0.0476, train acc: 0.9806\n",
      ">epoch: 12, val_acc: 0.9043, val_f1: 0.7661\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 13\n",
      "epoch: 13, loss: 0.0097, train acc: 1.0000\n",
      "epoch: 13, loss: 0.0042, train acc: 1.0000\n",
      "epoch: 13, loss: 0.0034, train acc: 1.0000\n",
      "epoch: 13, loss: 0.0285, train acc: 0.9926\n",
      "epoch: 13, loss: 0.0309, train acc: 0.9886\n",
      "epoch: 13, loss: 0.0263, train acc: 0.9907\n",
      "epoch: 13, loss: 0.0250, train acc: 0.9902\n",
      "epoch: 13, loss: 0.0225, train acc: 0.9916\n",
      "epoch: 13, loss: 0.0276, train acc: 0.9896\n",
      "epoch: 13, loss: 0.0282, train acc: 0.9894\n",
      "epoch: 13, loss: 0.0298, train acc: 0.9892\n",
      "epoch: 13, loss: 0.0291, train acc: 0.9890\n",
      "epoch: 13, loss: 0.0336, train acc: 0.9869\n",
      "epoch: 13, loss: 0.0342, train acc: 0.9869\n",
      "epoch: 13, loss: 0.0351, train acc: 0.9870\n",
      "epoch: 13, loss: 0.0354, train acc: 0.9870\n",
      "epoch: 13, loss: 0.0354, train acc: 0.9870\n",
      "epoch: 13, loss: 0.0380, train acc: 0.9849\n",
      "epoch: 13, loss: 0.0381, train acc: 0.9851\n",
      "epoch: 13, loss: 0.0402, train acc: 0.9839\n",
      "epoch: 13, loss: 0.0413, train acc: 0.9828\n",
      ">epoch: 13, val_acc: 0.8989, val_f1: 0.7550\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 14\n",
      "epoch: 14, loss: 0.0116, train acc: 1.0000\n",
      "epoch: 14, loss: 0.0161, train acc: 1.0000\n",
      "epoch: 14, loss: 0.0261, train acc: 0.9943\n",
      "epoch: 14, loss: 0.0222, train acc: 0.9961\n",
      "epoch: 14, loss: 0.0200, train acc: 0.9970\n",
      "epoch: 14, loss: 0.0316, train acc: 0.9880\n",
      "epoch: 14, loss: 0.0315, train acc: 0.9879\n",
      "epoch: 14, loss: 0.0297, train acc: 0.9878\n",
      "epoch: 14, loss: 0.0318, train acc: 0.9863\n",
      "epoch: 14, loss: 0.0447, train acc: 0.9823\n",
      "epoch: 14, loss: 0.0451, train acc: 0.9816\n",
      "epoch: 14, loss: 0.0420, train acc: 0.9821\n",
      "epoch: 14, loss: 0.0429, train acc: 0.9826\n",
      "epoch: 14, loss: 0.0411, train acc: 0.9830\n",
      "epoch: 14, loss: 0.0411, train acc: 0.9824\n",
      "epoch: 14, loss: 0.0410, train acc: 0.9827\n",
      "epoch: 14, loss: 0.0386, train acc: 0.9838\n",
      "epoch: 14, loss: 0.0382, train acc: 0.9833\n",
      "epoch: 14, loss: 0.0397, train acc: 0.9828\n",
      "epoch: 14, loss: 0.0396, train acc: 0.9824\n",
      "epoch: 14, loss: 0.0383, train acc: 0.9827\n",
      "epoch: 14, loss: 0.0399, train acc: 0.9823\n",
      ">epoch: 14, val_acc: 0.8989, val_f1: 0.7918\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 15\n",
      "epoch: 15, loss: 0.0228, train acc: 0.9875\n",
      "epoch: 15, loss: 0.0252, train acc: 0.9875\n",
      "epoch: 15, loss: 0.0253, train acc: 0.9875\n",
      "epoch: 15, loss: 0.0366, train acc: 0.9812\n",
      "epoch: 15, loss: 0.0361, train acc: 0.9800\n",
      "epoch: 15, loss: 0.0335, train acc: 0.9812\n",
      "epoch: 15, loss: 0.0332, train acc: 0.9821\n",
      "epoch: 15, loss: 0.0360, train acc: 0.9797\n",
      "epoch: 15, loss: 0.0337, train acc: 0.9806\n",
      "epoch: 15, loss: 0.0331, train acc: 0.9812\n",
      "epoch: 15, loss: 0.0326, train acc: 0.9818\n",
      "epoch: 15, loss: 0.0320, train acc: 0.9823\n",
      "epoch: 15, loss: 0.0313, train acc: 0.9827\n",
      "epoch: 15, loss: 0.0293, train acc: 0.9839\n",
      "epoch: 15, loss: 0.0292, train acc: 0.9850\n",
      "epoch: 15, loss: 0.0343, train acc: 0.9836\n",
      "epoch: 15, loss: 0.0324, train acc: 0.9846\n",
      "epoch: 15, loss: 0.0339, train acc: 0.9840\n",
      "epoch: 15, loss: 0.0337, train acc: 0.9842\n",
      "epoch: 15, loss: 0.0365, train acc: 0.9825\n",
      "epoch: 15, loss: 0.0389, train acc: 0.9815\n",
      ">epoch: 15, val_acc: 0.9043, val_f1: 0.7848\n",
      ">> epoch: 15, test_acc: 0.9000, test_f1: 0.7357\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 16\n",
      "epoch: 16, loss: 0.0960, train acc: 0.9531\n",
      "epoch: 16, loss: 0.0658, train acc: 0.9722\n",
      "epoch: 16, loss: 0.0533, train acc: 0.9777\n",
      "epoch: 16, loss: 0.0400, train acc: 0.9836\n",
      "epoch: 16, loss: 0.0352, train acc: 0.9870\n",
      "epoch: 16, loss: 0.0315, train acc: 0.9892\n",
      "epoch: 16, loss: 0.0315, train acc: 0.9890\n",
      "epoch: 16, loss: 0.0286, train acc: 0.9904\n",
      "epoch: 16, loss: 0.0330, train acc: 0.9872\n",
      "epoch: 16, loss: 0.0383, train acc: 0.9834\n",
      "epoch: 16, loss: 0.0362, train acc: 0.9850\n",
      "epoch: 16, loss: 0.0363, train acc: 0.9841\n",
      "epoch: 16, loss: 0.0346, train acc: 0.9854\n",
      "epoch: 16, loss: 0.0360, train acc: 0.9846\n",
      "epoch: 16, loss: 0.0427, train acc: 0.9823\n",
      "epoch: 16, loss: 0.0464, train acc: 0.9810\n",
      "epoch: 16, loss: 0.0451, train acc: 0.9814\n",
      "epoch: 16, loss: 0.0474, train acc: 0.9810\n",
      "epoch: 16, loss: 0.0470, train acc: 0.9814\n",
      "epoch: 16, loss: 0.0493, train acc: 0.9811\n",
      "epoch: 16, loss: 0.0503, train acc: 0.9814\n",
      ">epoch: 16, val_acc: 0.8830, val_f1: 0.7282\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 17\n",
      "epoch: 17, loss: 0.0757, train acc: 0.9583\n",
      "epoch: 17, loss: 0.0446, train acc: 0.9766\n",
      "epoch: 17, loss: 0.0346, train acc: 0.9808\n",
      "epoch: 17, loss: 0.0477, train acc: 0.9792\n",
      "epoch: 17, loss: 0.0515, train acc: 0.9755\n",
      "epoch: 17, loss: 0.0525, train acc: 0.9754\n",
      "epoch: 17, loss: 0.0537, train acc: 0.9754\n",
      "epoch: 17, loss: 0.0550, train acc: 0.9770\n",
      "epoch: 17, loss: 0.0542, train acc: 0.9767\n",
      "epoch: 17, loss: 0.0600, train acc: 0.9779\n",
      "epoch: 17, loss: 0.0646, train acc: 0.9752\n",
      "epoch: 17, loss: 0.0684, train acc: 0.9731\n",
      "epoch: 17, loss: 0.0663, train acc: 0.9732\n",
      "epoch: 17, loss: 0.0655, train acc: 0.9733\n",
      "epoch: 17, loss: 0.0661, train acc: 0.9726\n",
      "epoch: 17, loss: 0.0656, train acc: 0.9728\n",
      "epoch: 17, loss: 0.0649, train acc: 0.9729\n",
      "epoch: 17, loss: 0.0644, train acc: 0.9737\n",
      "epoch: 17, loss: 0.0652, train acc: 0.9731\n",
      "epoch: 17, loss: 0.0635, train acc: 0.9732\n",
      "epoch: 17, loss: 0.0716, train acc: 0.9691\n",
      ">epoch: 17, val_acc: 0.8564, val_f1: 0.7008\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 18\n",
      "epoch: 18, loss: 0.1343, train acc: 0.9688\n",
      "epoch: 18, loss: 0.2566, train acc: 0.9464\n",
      "epoch: 18, loss: 0.1844, train acc: 0.9635\n",
      "epoch: 18, loss: 0.1642, train acc: 0.9596\n",
      "epoch: 18, loss: 0.1397, train acc: 0.9631\n",
      "epoch: 18, loss: 0.1264, train acc: 0.9630\n",
      "epoch: 18, loss: 0.1184, train acc: 0.9648\n",
      "epoch: 18, loss: 0.1047, train acc: 0.9679\n",
      "epoch: 18, loss: 0.1322, train acc: 0.9628\n",
      "epoch: 18, loss: 0.1256, train acc: 0.9628\n",
      "epoch: 18, loss: 0.1270, train acc: 0.9591\n",
      "epoch: 18, loss: 0.1510, train acc: 0.9539\n",
      "epoch: 18, loss: 0.1626, train acc: 0.9486\n",
      "epoch: 18, loss: 0.1547, train acc: 0.9506\n",
      "epoch: 18, loss: 0.1528, train acc: 0.9505\n",
      "epoch: 18, loss: 0.1475, train acc: 0.9505\n",
      "epoch: 18, loss: 0.1443, train acc: 0.9512\n",
      "epoch: 18, loss: 0.1424, train acc: 0.9526\n",
      "epoch: 18, loss: 0.1436, train acc: 0.9531\n",
      "epoch: 18, loss: 0.1436, train acc: 0.9530\n",
      "epoch: 18, loss: 0.1462, train acc: 0.9522\n",
      ">epoch: 18, val_acc: 0.8883, val_f1: 0.7473\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 19\n",
      "epoch: 19, loss: 0.0120, train acc: 1.0000\n",
      "epoch: 19, loss: 0.0174, train acc: 1.0000\n",
      "epoch: 19, loss: 0.0688, train acc: 0.9659\n",
      "epoch: 19, loss: 0.1540, train acc: 0.9453\n",
      "epoch: 19, loss: 0.1513, train acc: 0.9405\n",
      "epoch: 19, loss: 0.1550, train acc: 0.9423\n",
      "epoch: 19, loss: 0.2963, train acc: 0.8871\n",
      "epoch: 19, loss: 0.3894, train acc: 0.8542\n",
      "epoch: 19, loss: 0.4708, train acc: 0.8155\n",
      "epoch: 19, loss: 0.5207, train acc: 0.7894\n",
      "epoch: 19, loss: 0.5471, train acc: 0.7819\n",
      "epoch: 19, loss: 0.5634, train acc: 0.7690\n",
      "epoch: 19, loss: 0.5834, train acc: 0.7551\n",
      "epoch: 19, loss: 0.6016, train acc: 0.7415\n",
      "epoch: 19, loss: 0.6209, train acc: 0.7350\n",
      "epoch: 19, loss: 0.6356, train acc: 0.7262\n",
      "epoch: 19, loss: 0.6502, train acc: 0.7168\n",
      "epoch: 19, loss: 0.6525, train acc: 0.7151\n",
      "epoch: 19, loss: 0.6725, train acc: 0.7102\n",
      "epoch: 19, loss: 0.6742, train acc: 0.7077\n",
      "epoch: 19, loss: 0.6806, train acc: 0.7036\n",
      "epoch: 19, loss: 0.6893, train acc: 0.6974\n",
      ">epoch: 19, val_acc: 0.7074, val_f1: 0.2762\n",
      "Namespace(model_type='tri_gcn', modules={'tgcn': True, 'semgcn': True, 'lexgcn': True}, num_layers={'tgcn': 2, 'semgcn': 2, 'lexgcn': 2}, year='2016', train_file='data/train2016restaurant.txt', test_file='data/test2016restaurant.txt', model_path='test_models/2016tri_gcn+concat_seed382_reg0.0005_drop0.4_cdrop0.4_lr2.5e-05_epochs20_adam', val_file='val.txt', log='log', bert_model='bert_large_uncased', cooc_path='cooc_matrix.csv', cooc=       100       1000   10000  10003     10005     10007     10009     1001   \\\n",
      "100      0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "1000     0.0  0.000000    0.0    0.0  0.000606  0.000303  0.000606  0.035120   \n",
      "10000    0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.333333   \n",
      "10003    0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "10005    0.0  0.333333    0.0    0.0  0.000000  0.166667  0.000000  0.166667   \n",
      "...      ...       ...    ...    ...       ...       ...       ...       ...   \n",
      "9992     0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "9993     0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "9996     0.0  0.500000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "9997     0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "9998     0.0  0.400000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "       10010  10015  ...     9986      9987      999    9990      9991   \\\n",
      "100      0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "1000     0.0    0.0  ...  0.002422  0.000303  0.062368    0.0  0.000606   \n",
      "10000    0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "10003    0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "10005    0.0    0.0  ...  0.000000  0.000000  0.333333    0.0  0.000000   \n",
      "...      ...    ...  ...       ...       ...       ...    ...       ...   \n",
      "9992     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "9993     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "9996     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "9997     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "9998     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "\n",
      "       9992   9993      9996   9997      9998   \n",
      "100      0.0    0.0  0.000000    0.0  0.000000  \n",
      "1000     0.0    0.0  0.000606    0.0  0.001211  \n",
      "10000    0.0    0.0  0.000000    0.0  0.000000  \n",
      "10003    0.0    0.0  0.000000    0.0  0.000000  \n",
      "10005    0.0    0.0  0.000000    0.0  0.000000  \n",
      "...      ...    ...       ...    ...       ...  \n",
      "9992     0.0    0.0  0.000000    0.0  0.000000  \n",
      "9993     0.0    0.0  0.000000    0.0  0.000000  \n",
      "9996     0.0    0.0  0.000000    0.0  0.000000  \n",
      "9997     0.0    0.0  0.000000    0.0  0.000000  \n",
      "9998     0.0    0.0  0.000000    0.0  0.000000  \n",
      "\n",
      "[13075 rows x 13075 columns], learning_rate=2.5e-05, dropout=0.4, concat_dropout=0.4, bert_dropout=0.2, l2reg=0.0005, num_epoch=20, batch_size=16, log_step=5, max_seq_len=100, polarities_dim=3, device=device(type='cuda'), seed=382, valset_ratio=0.1, do_train=True, do_eval=True, eval_epoch_num=0, fusion_type='concat', use_ensemble=True, save_models='none', print_sent=False, optim='adam', n_gpu=1)\n",
      "{'none': 0, 'acl': 1, 'acl:relcl': 2, 'advcl': 3, 'advcl:relcl': 4, 'advmod': 5, 'amod': 6, 'appos': 7, 'aux': 8, 'aux:pass': 9, 'case': 10, 'cc': 11, 'cc:preconj': 12, 'ccomp': 13, 'compound': 14, 'compound:prt': 15, 'conj': 16, 'cop': 17, 'csubj': 18, 'dep': 19, 'det': 20, 'det:predet': 21, 'discourse': 22, 'dislocated': 23, 'expl': 24, 'fixed': 25, 'flat': 26, 'iobj': 27, 'list': 28, 'mark': 29, 'nmod': 30, 'nmod:npmod': 31, 'nmod:poss': 32, 'nmod:tmod': 33, 'nsubj': 34, 'nsubj:outer': 35, 'nsubj:pass': 36, 'nummod': 37, 'obj': 38, 'obl': 39, 'obl:agent': 40, 'obl:npmod': 41, 'obl:tmod': 42, 'parataxis': 43, 'punct': 44, 'reparandum': 45, 'root': 46, 'vocative': 47, 'xcomp': 48}\n",
      "{'-1': 0, '0': 1, '1': 2}\n",
      "Model name 'bert_large_uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming 'bert_large_uncased' is a path or url to a directory containing tokenizer files.\n",
      "Didn't find file bert_large_uncased/added_tokens.json. We won't load it.\n",
      "Didn't find file bert_large_uncased/special_tokens_map.json. We won't load it.\n",
      "Didn't find file bert_large_uncased/tokenizer_config.json. We won't load it.\n",
      "loading file bert_large_uncased/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "{\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 3,\n",
      "  \"num_types\": 49,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file bert_large_uncased/pytorch_model.bin\n",
      "Weights of AsaTgcnSem not initialized from pretrained model: ['gate_weight', 'gate_bias', 'ensemble', 'TGCNLayers.0.weight', 'TGCNLayers.0.bias', 'TGCNLayers.0.dense.weight', 'TGCNLayers.1.weight', 'TGCNLayers.1.bias', 'TGCNLayers.1.dense.weight', 'SemGCNLayers.0.weight', 'SemGCNLayers.0.bias', 'SemGCNLayers.1.weight', 'SemGCNLayers.1.bias', 'LexGCNLayers.0.weight', 'LexGCNLayers.0.bias', 'LexGCNLayers.1.weight', 'LexGCNLayers.1.bias', 'fc_single.weight', 'fc_single.bias', 'ensemble_linear_tgcn.weight', 'ensemble_linear_tgcn.bias', 'ensemble_linear_semgcn.weight', 'ensemble_linear_semgcn.bias', 'ensemble_linear_lexgcn.weight', 'ensemble_linear_lexgcn.bias', 'dep_embedding.weight']\n",
      "Weights from pretrained model not used in AsaTgcnSem: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "[{'input_ids': tensor([  101, 13325,  2013,  3025,  8466,  2023,  2109,  2000,  2022,  1037,\n",
      "         2204,  2173,  1010,  2021,  2025,  2151,  2936,  1012,   102,  2173,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'valid_ids': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'segment_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'mem_valid_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'dep_adj_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'dep_value_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'polarity': 0, 'raw_text': 'judging from previous posts this used to be a good place , but not any longer .', 'aspect': 'place'}]\n",
      "[{'input_ids': tensor([  101, 13325,  2013,  3025,  8466,  2023,  2109,  2000,  2022,  1037,\n",
      "         2204,  2173,  1010,  2021,  2025,  2151,  2936,  1012,   102,  2173,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'valid_ids': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'segment_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'mem_valid_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'dep_adj_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'dep_value_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'polarity': 0, 'raw_text': 'judging from previous posts this used to be a good place , but not any longer .', 'aspect': 'place'}]\n",
      "[{'input_ids': tensor([  101,  4240,  2428,  2204, 10514,  6182,  1012,   102, 10514,  6182,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'valid_ids': tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'segment_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'mem_valid_ids': tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'dep_adj_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'dep_value_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 5,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'polarity': 2, 'raw_text': 'serves really good sushi .', 'aspect': 'sushi'}]\n",
      "cuda memory allocated: 1400893440\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 0\n",
      "epoch: 0, loss: 1.0030, train acc: 0.5250\n",
      "epoch: 0, loss: 0.9074, train acc: 0.5938\n",
      "epoch: 0, loss: 0.9251, train acc: 0.5958\n",
      "epoch: 0, loss: 0.9452, train acc: 0.5781\n",
      "epoch: 0, loss: 0.9293, train acc: 0.5675\n",
      "epoch: 0, loss: 0.8985, train acc: 0.5958\n",
      "epoch: 0, loss: 0.8966, train acc: 0.5946\n",
      "epoch: 0, loss: 0.8843, train acc: 0.6016\n",
      "epoch: 0, loss: 0.9043, train acc: 0.6000\n",
      "epoch: 0, loss: 0.9239, train acc: 0.5863\n",
      "epoch: 0, loss: 0.9228, train acc: 0.5898\n",
      "epoch: 0, loss: 0.9320, train acc: 0.5927\n",
      "epoch: 0, loss: 0.9259, train acc: 0.5990\n",
      "epoch: 0, loss: 0.9166, train acc: 0.6018\n",
      "epoch: 0, loss: 0.9121, train acc: 0.6067\n",
      "epoch: 0, loss: 0.9057, train acc: 0.6102\n",
      "epoch: 0, loss: 0.8923, train acc: 0.6147\n",
      "epoch: 0, loss: 0.8830, train acc: 0.6194\n",
      "epoch: 0, loss: 0.8731, train acc: 0.6263\n",
      "epoch: 0, loss: 0.8671, train acc: 0.6319\n",
      "epoch: 0, loss: 0.8599, train acc: 0.6363\n",
      ">epoch: 0, val_acc: 0.7021, val_f1: 0.2750\n",
      ">> epoch: 0, test_acc: 0.7431, test_f1: 0.2842\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 1\n",
      "epoch: 1, loss: 0.7972, train acc: 0.7344\n",
      "epoch: 1, loss: 0.7974, train acc: 0.7014\n",
      "epoch: 1, loss: 0.7996, train acc: 0.6786\n",
      "epoch: 1, loss: 0.7944, train acc: 0.6645\n",
      "epoch: 1, loss: 0.8126, train acc: 0.6458\n",
      "epoch: 1, loss: 0.7951, train acc: 0.6466\n",
      "epoch: 1, loss: 0.7796, train acc: 0.6599\n",
      "epoch: 1, loss: 0.7807, train acc: 0.6651\n",
      "epoch: 1, loss: 0.7732, train acc: 0.6705\n",
      "epoch: 1, loss: 0.7730, train acc: 0.6645\n",
      "epoch: 1, loss: 0.7603, train acc: 0.6690\n",
      "epoch: 1, loss: 0.7626, train acc: 0.6684\n",
      "epoch: 1, loss: 0.7575, train acc: 0.6719\n",
      "epoch: 1, loss: 0.7600, train acc: 0.6676\n",
      "epoch: 1, loss: 0.7631, train acc: 0.6681\n",
      "epoch: 1, loss: 0.7699, train acc: 0.6638\n",
      "epoch: 1, loss: 0.7715, train acc: 0.6652\n",
      "epoch: 1, loss: 0.7790, train acc: 0.6636\n",
      "epoch: 1, loss: 0.7839, train acc: 0.6602\n",
      "epoch: 1, loss: 0.7819, train acc: 0.6616\n",
      "epoch: 1, loss: 0.7801, train acc: 0.6647\n",
      ">epoch: 1, val_acc: 0.7021, val_f1: 0.2750\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 2\n",
      "epoch: 2, loss: 0.6774, train acc: 0.7292\n",
      "epoch: 2, loss: 0.7957, train acc: 0.6953\n",
      "epoch: 2, loss: 0.8451, train acc: 0.6538\n",
      "epoch: 2, loss: 0.8449, train acc: 0.6458\n",
      "epoch: 2, loss: 0.8244, train acc: 0.6495\n",
      "epoch: 2, loss: 0.8244, train acc: 0.6607\n",
      "epoch: 2, loss: 0.8213, train acc: 0.6572\n",
      "epoch: 2, loss: 0.8311, train acc: 0.6530\n",
      "epoch: 2, loss: 0.8099, train acc: 0.6570\n",
      "epoch: 2, loss: 0.7985, train acc: 0.6615\n",
      "epoch: 2, loss: 0.7874, train acc: 0.6686\n",
      "epoch: 2, loss: 0.7977, train acc: 0.6713\n",
      "epoch: 2, loss: 0.7958, train acc: 0.6726\n",
      "epoch: 2, loss: 0.7954, train acc: 0.6710\n",
      "epoch: 2, loss: 0.7931, train acc: 0.6712\n",
      "epoch: 2, loss: 0.7883, train acc: 0.6747\n",
      "epoch: 2, loss: 0.7788, train acc: 0.6807\n",
      "epoch: 2, loss: 0.7707, train acc: 0.6847\n",
      "epoch: 2, loss: 0.7635, train acc: 0.6875\n",
      "epoch: 2, loss: 0.7786, train acc: 0.6805\n",
      "epoch: 2, loss: 0.7789, train acc: 0.6760\n",
      ">epoch: 2, val_acc: 0.7021, val_f1: 0.2750\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 3\n",
      "epoch: 3, loss: 0.7584, train acc: 0.6562\n",
      "epoch: 3, loss: 0.6906, train acc: 0.7143\n",
      "epoch: 3, loss: 0.7213, train acc: 0.6979\n",
      "epoch: 3, loss: 0.7097, train acc: 0.7132\n",
      "epoch: 3, loss: 0.7323, train acc: 0.6960\n",
      "epoch: 3, loss: 0.7320, train acc: 0.6944\n",
      "epoch: 3, loss: 0.7449, train acc: 0.6895\n",
      "epoch: 3, loss: 0.7575, train acc: 0.6757\n",
      "epoch: 3, loss: 0.7641, train acc: 0.6771\n",
      "epoch: 3, loss: 0.7544, train acc: 0.6795\n",
      "epoch: 3, loss: 0.7565, train acc: 0.6815\n",
      "epoch: 3, loss: 0.7571, train acc: 0.6831\n",
      "epoch: 3, loss: 0.7539, train acc: 0.6875\n",
      "epoch: 3, loss: 0.7594, train acc: 0.6838\n",
      "epoch: 3, loss: 0.7554, train acc: 0.6892\n",
      "epoch: 3, loss: 0.7518, train acc: 0.6924\n",
      "epoch: 3, loss: 0.7558, train acc: 0.6905\n",
      "epoch: 3, loss: 0.7577, train acc: 0.6861\n",
      "epoch: 3, loss: 0.7496, train acc: 0.6902\n",
      "epoch: 3, loss: 0.7552, train acc: 0.6862\n",
      "epoch: 3, loss: 0.7509, train acc: 0.6906\n",
      ">epoch: 3, val_acc: 0.7021, val_f1: 0.2750\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 4\n",
      "epoch: 4, loss: 0.7623, train acc: 0.6250\n",
      "epoch: 4, loss: 0.8246, train acc: 0.6562\n",
      "epoch: 4, loss: 0.8015, train acc: 0.6761\n",
      "epoch: 4, loss: 0.7998, train acc: 0.6602\n",
      "epoch: 4, loss: 0.7989, train acc: 0.6577\n",
      "epoch: 4, loss: 0.7784, train acc: 0.6538\n",
      "epoch: 4, loss: 0.7540, train acc: 0.6714\n",
      "epoch: 4, loss: 0.7487, train acc: 0.6719\n",
      "epoch: 4, loss: 0.7533, train acc: 0.6814\n",
      "epoch: 4, loss: 0.7562, train acc: 0.6848\n",
      "epoch: 4, loss: 0.7475, train acc: 0.6900\n",
      "epoch: 4, loss: 0.7481, train acc: 0.6875\n",
      "epoch: 4, loss: 0.7473, train acc: 0.6895\n",
      "epoch: 4, loss: 0.7502, train acc: 0.6847\n",
      "epoch: 4, loss: 0.7582, train acc: 0.6778\n",
      "epoch: 4, loss: 0.7570, train acc: 0.6801\n",
      "epoch: 4, loss: 0.7593, train acc: 0.6759\n",
      "epoch: 4, loss: 0.7538, train acc: 0.6802\n",
      "epoch: 4, loss: 0.7524, train acc: 0.6813\n",
      "epoch: 4, loss: 0.7553, train acc: 0.6777\n",
      "epoch: 4, loss: 0.7512, train acc: 0.6832\n",
      "epoch: 4, loss: 0.7522, train acc: 0.6820\n",
      ">epoch: 4, val_acc: 0.7021, val_f1: 0.2750\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 5\n",
      "epoch: 5, loss: 0.6996, train acc: 0.7500\n",
      "epoch: 5, loss: 0.6832, train acc: 0.7500\n",
      "epoch: 5, loss: 0.7746, train acc: 0.7000\n",
      "epoch: 5, loss: 0.7662, train acc: 0.7000\n",
      "epoch: 5, loss: 0.7943, train acc: 0.6875\n",
      "epoch: 5, loss: 0.7955, train acc: 0.6896\n",
      "epoch: 5, loss: 0.7846, train acc: 0.6911\n",
      "epoch: 5, loss: 0.7819, train acc: 0.6875\n",
      "epoch: 5, loss: 0.7853, train acc: 0.6819\n",
      "epoch: 5, loss: 0.7866, train acc: 0.6837\n",
      "epoch: 5, loss: 0.7900, train acc: 0.6773\n",
      "epoch: 5, loss: 0.7865, train acc: 0.6813\n",
      "epoch: 5, loss: 0.7758, train acc: 0.6865\n",
      "epoch: 5, loss: 0.7749, train acc: 0.6884\n",
      "epoch: 5, loss: 0.7731, train acc: 0.6900\n",
      "epoch: 5, loss: 0.7773, train acc: 0.6867\n",
      "epoch: 5, loss: 0.7750, train acc: 0.6801\n",
      "epoch: 5, loss: 0.7760, train acc: 0.6799\n",
      "epoch: 5, loss: 0.7763, train acc: 0.6783\n",
      "epoch: 5, loss: 0.7709, train acc: 0.6825\n",
      "epoch: 5, loss: 0.7615, train acc: 0.6875\n",
      ">epoch: 5, val_acc: 0.7021, val_f1: 0.2750\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 6\n",
      "epoch: 6, loss: 0.7787, train acc: 0.7031\n",
      "epoch: 6, loss: 0.7938, train acc: 0.6806\n",
      "epoch: 6, loss: 0.7877, train acc: 0.6786\n",
      "epoch: 6, loss: 0.7613, train acc: 0.6842\n",
      "epoch: 6, loss: 0.7750, train acc: 0.6823\n",
      "epoch: 6, loss: 0.7534, train acc: 0.6875\n",
      "epoch: 6, loss: 0.7472, train acc: 0.6912\n",
      "epoch: 6, loss: 0.7425, train acc: 0.6923\n",
      "epoch: 6, loss: 0.7401, train acc: 0.6932\n",
      "epoch: 6, loss: 0.7460, train acc: 0.6849\n",
      "epoch: 6, loss: 0.7556, train acc: 0.6794\n",
      "epoch: 6, loss: 0.7560, train acc: 0.6748\n",
      "epoch: 6, loss: 0.7619, train acc: 0.6748\n",
      "epoch: 6, loss: 0.7458, train acc: 0.6875\n",
      "epoch: 6, loss: 0.7557, train acc: 0.6850\n",
      "epoch: 6, loss: 0.7558, train acc: 0.6851\n",
      "epoch: 6, loss: 0.7558, train acc: 0.6868\n",
      "epoch: 6, loss: 0.7582, train acc: 0.6826\n",
      "epoch: 6, loss: 0.7595, train acc: 0.6815\n",
      "epoch: 6, loss: 0.7545, train acc: 0.6824\n",
      "epoch: 6, loss: 0.7545, train acc: 0.6833\n",
      ">epoch: 6, val_acc: 0.7021, val_f1: 0.2750\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 7\n",
      "epoch: 7, loss: 0.8641, train acc: 0.6250\n",
      "epoch: 7, loss: 0.8449, train acc: 0.6328\n",
      "epoch: 7, loss: 0.8149, train acc: 0.6442\n",
      "epoch: 7, loss: 0.7856, train acc: 0.6736\n",
      "epoch: 7, loss: 0.7612, train acc: 0.6902\n",
      "epoch: 7, loss: 0.7732, train acc: 0.6853\n",
      "epoch: 7, loss: 0.7612, train acc: 0.6875\n",
      "epoch: 7, loss: 0.7551, train acc: 0.6924\n",
      "epoch: 7, loss: 0.7510, train acc: 0.6962\n",
      "epoch: 7, loss: 0.7495, train acc: 0.6979\n",
      "epoch: 7, loss: 0.7495, train acc: 0.6969\n",
      "epoch: 7, loss: 0.7361, train acc: 0.6950\n",
      "epoch: 7, loss: 0.7377, train acc: 0.6915\n",
      "epoch: 7, loss: 0.7350, train acc: 0.6903\n",
      "epoch: 7, loss: 0.7361, train acc: 0.6858\n",
      "epoch: 7, loss: 0.7376, train acc: 0.6843\n",
      "epoch: 7, loss: 0.7405, train acc: 0.6807\n",
      "epoch: 7, loss: 0.7416, train acc: 0.6839\n",
      "epoch: 7, loss: 0.7377, train acc: 0.6848\n",
      "epoch: 7, loss: 0.7425, train acc: 0.6862\n",
      "epoch: 7, loss: 0.7378, train acc: 0.6881\n",
      ">epoch: 7, val_acc: 0.7021, val_f1: 0.2750\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 8\n",
      "epoch: 8, loss: 0.8967, train acc: 0.5938\n",
      "epoch: 8, loss: 0.7456, train acc: 0.6607\n",
      "epoch: 8, loss: 0.7275, train acc: 0.6927\n",
      "epoch: 8, loss: 0.6773, train acc: 0.7206\n",
      "epoch: 8, loss: 0.6734, train acc: 0.7159\n",
      "epoch: 8, loss: 0.7180, train acc: 0.6759\n",
      "epoch: 8, loss: 0.6850, train acc: 0.6953\n",
      "epoch: 8, loss: 0.6811, train acc: 0.7010\n",
      "epoch: 8, loss: 0.6666, train acc: 0.7113\n",
      "epoch: 8, loss: 0.6542, train acc: 0.7048\n",
      "epoch: 8, loss: 0.6422, train acc: 0.7151\n",
      "epoch: 8, loss: 0.6311, train acc: 0.7226\n",
      "epoch: 8, loss: 0.6580, train acc: 0.7208\n",
      "epoch: 8, loss: 0.6651, train acc: 0.7183\n",
      "epoch: 8, loss: 0.6722, train acc: 0.7118\n",
      "epoch: 8, loss: 0.6695, train acc: 0.7127\n",
      "epoch: 8, loss: 0.6823, train acc: 0.7073\n",
      "epoch: 8, loss: 0.6787, train acc: 0.7091\n",
      "epoch: 8, loss: 0.6628, train acc: 0.7154\n",
      "epoch: 8, loss: 0.6651, train acc: 0.7139\n",
      "epoch: 8, loss: 0.6594, train acc: 0.7169\n",
      ">epoch: 8, val_acc: 0.8245, val_f1: 0.5433\n",
      ">> epoch: 8, test_acc: 0.7815, test_f1: 0.5067\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 9\n",
      "epoch: 9, loss: 0.5659, train acc: 0.7500\n",
      "epoch: 9, loss: 0.5299, train acc: 0.7500\n",
      "epoch: 9, loss: 0.5209, train acc: 0.7386\n",
      "epoch: 9, loss: 0.5237, train acc: 0.7383\n",
      "epoch: 9, loss: 0.5368, train acc: 0.7440\n",
      "epoch: 9, loss: 0.5146, train acc: 0.7500\n",
      "epoch: 9, loss: 0.5168, train acc: 0.7500\n",
      "epoch: 9, loss: 0.5121, train acc: 0.7569\n",
      "epoch: 9, loss: 0.5124, train acc: 0.7607\n",
      "epoch: 9, loss: 0.5155, train acc: 0.7649\n",
      "epoch: 9, loss: 0.5214, train acc: 0.7659\n",
      "epoch: 9, loss: 0.5288, train acc: 0.7623\n",
      "epoch: 9, loss: 0.5131, train acc: 0.7756\n",
      "epoch: 9, loss: 0.5067, train acc: 0.7822\n",
      "epoch: 9, loss: 0.4963, train acc: 0.7879\n",
      "epoch: 9, loss: 0.4908, train acc: 0.7936\n",
      "epoch: 9, loss: 0.4874, train acc: 0.7971\n",
      "epoch: 9, loss: 0.4914, train acc: 0.7958\n",
      "epoch: 9, loss: 0.4918, train acc: 0.7946\n",
      "epoch: 9, loss: 0.4875, train acc: 0.7995\n",
      "epoch: 9, loss: 0.4757, train acc: 0.8057\n",
      "epoch: 9, loss: 0.4728, train acc: 0.8073\n",
      ">epoch: 9, val_acc: 0.8245, val_f1: 0.5141\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 10\n",
      "epoch: 10, loss: 0.5844, train acc: 0.8375\n",
      "epoch: 10, loss: 0.4576, train acc: 0.8500\n",
      "epoch: 10, loss: 0.4011, train acc: 0.8667\n",
      "epoch: 10, loss: 0.3951, train acc: 0.8625\n",
      "epoch: 10, loss: 0.3837, train acc: 0.8650\n",
      "epoch: 10, loss: 0.3688, train acc: 0.8729\n",
      "epoch: 10, loss: 0.3446, train acc: 0.8804\n",
      "epoch: 10, loss: 0.3522, train acc: 0.8797\n",
      "epoch: 10, loss: 0.3415, train acc: 0.8833\n",
      "epoch: 10, loss: 0.3260, train acc: 0.8912\n",
      "epoch: 10, loss: 0.3365, train acc: 0.8875\n",
      "epoch: 10, loss: 0.3237, train acc: 0.8917\n",
      "epoch: 10, loss: 0.3340, train acc: 0.8875\n",
      "epoch: 10, loss: 0.3372, train acc: 0.8866\n",
      "epoch: 10, loss: 0.3365, train acc: 0.8875\n",
      "epoch: 10, loss: 0.3351, train acc: 0.8875\n",
      "epoch: 10, loss: 0.3357, train acc: 0.8882\n",
      "epoch: 10, loss: 0.3331, train acc: 0.8882\n",
      "epoch: 10, loss: 0.3329, train acc: 0.8868\n",
      "epoch: 10, loss: 0.3358, train acc: 0.8856\n",
      "epoch: 10, loss: 0.3478, train acc: 0.8804\n",
      ">epoch: 10, val_acc: 0.8883, val_f1: 0.5870\n",
      ">> epoch: 10, test_acc: 0.8615, test_f1: 0.5570\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 11\n",
      "epoch: 11, loss: 0.2898, train acc: 0.9375\n",
      "epoch: 11, loss: 0.3045, train acc: 0.9028\n",
      "epoch: 11, loss: 0.2636, train acc: 0.9196\n",
      "epoch: 11, loss: 0.2881, train acc: 0.9178\n",
      "epoch: 11, loss: 0.2912, train acc: 0.9062\n",
      "epoch: 11, loss: 0.2608, train acc: 0.9159\n",
      "epoch: 11, loss: 0.2659, train acc: 0.9136\n",
      "epoch: 11, loss: 0.2753, train acc: 0.9038\n",
      "epoch: 11, loss: 0.2774, train acc: 0.9048\n",
      "epoch: 11, loss: 0.2920, train acc: 0.8954\n",
      "epoch: 11, loss: 0.2941, train acc: 0.8958\n",
      "epoch: 11, loss: 0.2972, train acc: 0.8972\n",
      "epoch: 11, loss: 0.2885, train acc: 0.9014\n",
      "epoch: 11, loss: 0.2975, train acc: 0.8958\n",
      "epoch: 11, loss: 0.2904, train acc: 0.8978\n",
      "epoch: 11, loss: 0.2924, train acc: 0.8979\n",
      "epoch: 11, loss: 0.2893, train acc: 0.8973\n",
      "epoch: 11, loss: 0.2907, train acc: 0.8940\n",
      "epoch: 11, loss: 0.2880, train acc: 0.8956\n",
      "epoch: 11, loss: 0.2860, train acc: 0.8958\n",
      "epoch: 11, loss: 0.2867, train acc: 0.8948\n",
      ">epoch: 11, val_acc: 0.8564, val_f1: 0.5589\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 12\n",
      "epoch: 12, loss: 0.2980, train acc: 0.8542\n",
      "epoch: 12, loss: 0.2528, train acc: 0.9062\n",
      "epoch: 12, loss: 0.2451, train acc: 0.9038\n",
      "epoch: 12, loss: 0.2471, train acc: 0.9062\n",
      "epoch: 12, loss: 0.2465, train acc: 0.8967\n",
      "epoch: 12, loss: 0.2596, train acc: 0.8996\n",
      "epoch: 12, loss: 0.2545, train acc: 0.9034\n",
      "epoch: 12, loss: 0.2385, train acc: 0.9079\n",
      "epoch: 12, loss: 0.2308, train acc: 0.9128\n",
      "epoch: 12, loss: 0.2373, train acc: 0.9128\n",
      "epoch: 12, loss: 0.2323, train acc: 0.9127\n",
      "epoch: 12, loss: 0.2373, train acc: 0.9127\n",
      "epoch: 12, loss: 0.2434, train acc: 0.9067\n",
      "epoch: 12, loss: 0.2484, train acc: 0.9035\n",
      "epoch: 12, loss: 0.2467, train acc: 0.9024\n",
      "epoch: 12, loss: 0.2428, train acc: 0.9046\n",
      "epoch: 12, loss: 0.2460, train acc: 0.9021\n",
      "epoch: 12, loss: 0.2427, train acc: 0.9041\n",
      "epoch: 12, loss: 0.2388, train acc: 0.9059\n",
      "epoch: 12, loss: 0.2363, train acc: 0.9056\n",
      "epoch: 12, loss: 0.2386, train acc: 0.9047\n",
      ">epoch: 12, val_acc: 0.8670, val_f1: 0.5650\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 13\n",
      "epoch: 13, loss: 0.1085, train acc: 0.9688\n",
      "epoch: 13, loss: 0.1449, train acc: 0.9554\n",
      "epoch: 13, loss: 0.1691, train acc: 0.9375\n",
      "epoch: 13, loss: 0.1512, train acc: 0.9485\n",
      "epoch: 13, loss: 0.1547, train acc: 0.9460\n",
      "epoch: 13, loss: 0.1770, train acc: 0.9398\n",
      "epoch: 13, loss: 0.1889, train acc: 0.9316\n",
      "epoch: 13, loss: 0.2037, train acc: 0.9257\n",
      "epoch: 13, loss: 0.2138, train acc: 0.9226\n",
      "epoch: 13, loss: 0.2171, train acc: 0.9189\n",
      "epoch: 13, loss: 0.2137, train acc: 0.9183\n",
      "epoch: 13, loss: 0.2126, train acc: 0.9178\n",
      "epoch: 13, loss: 0.2089, train acc: 0.9163\n",
      "epoch: 13, loss: 0.2065, train acc: 0.9151\n",
      "epoch: 13, loss: 0.2057, train acc: 0.9158\n",
      "epoch: 13, loss: 0.2053, train acc: 0.9156\n",
      "epoch: 13, loss: 0.2029, train acc: 0.9169\n",
      "epoch: 13, loss: 0.2032, train acc: 0.9181\n",
      "epoch: 13, loss: 0.2046, train acc: 0.9171\n",
      "epoch: 13, loss: 0.2029, train acc: 0.9188\n",
      "epoch: 13, loss: 0.2022, train acc: 0.9203\n",
      ">epoch: 13, val_acc: 0.8404, val_f1: 0.6328\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 14\n",
      "epoch: 14, loss: 0.2472, train acc: 0.9375\n",
      "epoch: 14, loss: 0.2056, train acc: 0.9271\n",
      "epoch: 14, loss: 0.1839, train acc: 0.9261\n",
      "epoch: 14, loss: 0.2009, train acc: 0.9180\n",
      "epoch: 14, loss: 0.1740, train acc: 0.9286\n",
      "epoch: 14, loss: 0.1653, train acc: 0.9351\n",
      "epoch: 14, loss: 0.1604, train acc: 0.9355\n",
      "epoch: 14, loss: 0.1766, train acc: 0.9306\n",
      "epoch: 14, loss: 0.1760, train acc: 0.9299\n",
      "epoch: 14, loss: 0.1732, train acc: 0.9280\n",
      "epoch: 14, loss: 0.1705, train acc: 0.9289\n",
      "epoch: 14, loss: 0.1706, train acc: 0.9297\n",
      "epoch: 14, loss: 0.1702, train acc: 0.9293\n",
      "epoch: 14, loss: 0.1755, train acc: 0.9290\n",
      "epoch: 14, loss: 0.1702, train acc: 0.9305\n",
      "epoch: 14, loss: 0.1741, train acc: 0.9276\n",
      "epoch: 14, loss: 0.1736, train acc: 0.9282\n",
      "epoch: 14, loss: 0.1735, train acc: 0.9273\n",
      "epoch: 14, loss: 0.1711, train acc: 0.9299\n",
      "epoch: 14, loss: 0.1712, train acc: 0.9297\n",
      "epoch: 14, loss: 0.1722, train acc: 0.9301\n",
      "epoch: 14, loss: 0.1705, train acc: 0.9320\n",
      ">epoch: 14, val_acc: 0.8617, val_f1: 0.6794\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 15\n",
      "epoch: 15, loss: 0.0732, train acc: 0.9875\n",
      "epoch: 15, loss: 0.1173, train acc: 0.9688\n",
      "epoch: 15, loss: 0.1091, train acc: 0.9667\n",
      "epoch: 15, loss: 0.1301, train acc: 0.9625\n",
      "epoch: 15, loss: 0.1295, train acc: 0.9600\n",
      "epoch: 15, loss: 0.1365, train acc: 0.9563\n",
      "epoch: 15, loss: 0.1277, train acc: 0.9607\n",
      "epoch: 15, loss: 0.1315, train acc: 0.9578\n",
      "epoch: 15, loss: 0.1254, train acc: 0.9583\n",
      "epoch: 15, loss: 0.1309, train acc: 0.9575\n",
      "epoch: 15, loss: 0.1258, train acc: 0.9580\n",
      "epoch: 15, loss: 0.1297, train acc: 0.9552\n",
      "epoch: 15, loss: 0.1273, train acc: 0.9567\n",
      "epoch: 15, loss: 0.1307, train acc: 0.9563\n",
      "epoch: 15, loss: 0.1373, train acc: 0.9525\n",
      "epoch: 15, loss: 0.1380, train acc: 0.9523\n",
      "epoch: 15, loss: 0.1426, train acc: 0.9507\n",
      "epoch: 15, loss: 0.1479, train acc: 0.9472\n",
      "epoch: 15, loss: 0.1531, train acc: 0.9447\n",
      "epoch: 15, loss: 0.1593, train acc: 0.9419\n",
      "epoch: 15, loss: 0.1620, train acc: 0.9405\n",
      ">epoch: 15, val_acc: 0.8723, val_f1: 0.6442\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 16\n",
      "epoch: 16, loss: 0.2832, train acc: 0.8750\n",
      "epoch: 16, loss: 0.2886, train acc: 0.8819\n",
      "epoch: 16, loss: 0.2522, train acc: 0.8973\n",
      "epoch: 16, loss: 0.2336, train acc: 0.9112\n",
      "epoch: 16, loss: 0.2438, train acc: 0.9115\n",
      "epoch: 16, loss: 0.2428, train acc: 0.9116\n",
      "epoch: 16, loss: 0.2370, train acc: 0.9173\n",
      "epoch: 16, loss: 0.2299, train acc: 0.9151\n",
      "epoch: 16, loss: 0.2178, train acc: 0.9190\n",
      "epoch: 16, loss: 0.2249, train acc: 0.9196\n",
      "epoch: 16, loss: 0.2188, train acc: 0.9248\n",
      "epoch: 16, loss: 0.2168, train acc: 0.9248\n",
      "epoch: 16, loss: 0.2050, train acc: 0.9287\n",
      "epoch: 16, loss: 0.2105, train acc: 0.9293\n",
      "epoch: 16, loss: 0.2021, train acc: 0.9333\n",
      "epoch: 16, loss: 0.2020, train acc: 0.9335\n",
      "epoch: 16, loss: 0.1965, train acc: 0.9345\n",
      "epoch: 16, loss: 0.1914, train acc: 0.9361\n",
      "epoch: 16, loss: 0.1948, train acc: 0.9362\n",
      "epoch: 16, loss: 0.1974, train acc: 0.9343\n",
      "epoch: 16, loss: 0.1944, train acc: 0.9357\n",
      ">epoch: 16, val_acc: 0.8830, val_f1: 0.6925\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 17\n",
      "epoch: 17, loss: 0.1192, train acc: 0.9583\n",
      "epoch: 17, loss: 0.1483, train acc: 0.9375\n",
      "epoch: 17, loss: 0.1511, train acc: 0.9375\n",
      "epoch: 17, loss: 0.1361, train acc: 0.9514\n",
      "epoch: 17, loss: 0.1659, train acc: 0.9429\n",
      "epoch: 17, loss: 0.1517, train acc: 0.9464\n",
      "epoch: 17, loss: 0.1408, train acc: 0.9508\n",
      "epoch: 17, loss: 0.1421, train acc: 0.9457\n",
      "epoch: 17, loss: 0.1459, train acc: 0.9433\n",
      "epoch: 17, loss: 0.1428, train acc: 0.9453\n",
      "epoch: 17, loss: 0.1459, train acc: 0.9446\n",
      "epoch: 17, loss: 0.1392, train acc: 0.9483\n",
      "epoch: 17, loss: 0.1451, train acc: 0.9474\n",
      "epoch: 17, loss: 0.1481, train acc: 0.9439\n",
      "epoch: 17, loss: 0.1530, train acc: 0.9401\n",
      "epoch: 17, loss: 0.1484, train acc: 0.9423\n",
      "epoch: 17, loss: 0.1442, train acc: 0.9443\n",
      "epoch: 17, loss: 0.1603, train acc: 0.9411\n",
      "epoch: 17, loss: 0.1673, train acc: 0.9382\n",
      "epoch: 17, loss: 0.1716, train acc: 0.9375\n",
      "epoch: 17, loss: 0.1780, train acc: 0.9333\n",
      ">epoch: 17, val_acc: 0.8723, val_f1: 0.6948\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 18\n",
      "epoch: 18, loss: 0.0667, train acc: 1.0000\n",
      "epoch: 18, loss: 0.1581, train acc: 0.9286\n",
      "epoch: 18, loss: 0.1860, train acc: 0.9323\n",
      "epoch: 18, loss: 0.1942, train acc: 0.9191\n",
      "epoch: 18, loss: 0.1761, train acc: 0.9318\n",
      "epoch: 18, loss: 0.1533, train acc: 0.9398\n",
      "epoch: 18, loss: 0.1537, train acc: 0.9355\n",
      "epoch: 18, loss: 0.1627, train acc: 0.9324\n",
      "epoch: 18, loss: 0.1809, train acc: 0.9286\n",
      "epoch: 18, loss: 0.2011, train acc: 0.9215\n",
      "epoch: 18, loss: 0.2082, train acc: 0.9195\n",
      "epoch: 18, loss: 0.2174, train acc: 0.9167\n",
      "epoch: 18, loss: 0.2255, train acc: 0.9153\n",
      "epoch: 18, loss: 0.2420, train acc: 0.9086\n",
      "epoch: 18, loss: 0.2499, train acc: 0.9097\n",
      "epoch: 18, loss: 0.2626, train acc: 0.9018\n",
      "epoch: 18, loss: 0.2578, train acc: 0.9017\n",
      "epoch: 18, loss: 0.2710, train acc: 0.9001\n",
      "epoch: 18, loss: 0.2667, train acc: 0.9029\n",
      "epoch: 18, loss: 0.2640, train acc: 0.9034\n",
      "epoch: 18, loss: 0.2618, train acc: 0.9026\n",
      ">epoch: 18, val_acc: 0.8085, val_f1: 0.5042\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 19\n",
      "epoch: 19, loss: 0.2388, train acc: 0.8750\n",
      "epoch: 19, loss: 0.1466, train acc: 0.9375\n",
      "epoch: 19, loss: 0.1752, train acc: 0.9148\n",
      "epoch: 19, loss: 0.1525, train acc: 0.9258\n",
      "epoch: 19, loss: 0.1345, train acc: 0.9405\n",
      "epoch: 19, loss: 0.1448, train acc: 0.9423\n",
      "epoch: 19, loss: 0.1876, train acc: 0.9375\n",
      "epoch: 19, loss: 0.1736, train acc: 0.9427\n",
      "epoch: 19, loss: 0.1632, train acc: 0.9466\n",
      "epoch: 19, loss: 0.1613, train acc: 0.9484\n",
      "epoch: 19, loss: 0.1658, train acc: 0.9485\n",
      "epoch: 19, loss: 0.1823, train acc: 0.9442\n",
      "epoch: 19, loss: 0.2004, train acc: 0.9395\n",
      "epoch: 19, loss: 0.2064, train acc: 0.9366\n",
      "epoch: 19, loss: 0.2317, train acc: 0.9305\n",
      "epoch: 19, loss: 0.2457, train acc: 0.9235\n",
      "epoch: 19, loss: 0.2702, train acc: 0.9190\n",
      "epoch: 19, loss: 0.2786, train acc: 0.9193\n",
      "epoch: 19, loss: 0.2839, train acc: 0.9155\n",
      "epoch: 19, loss: 0.2979, train acc: 0.9108\n",
      "epoch: 19, loss: 0.3130, train acc: 0.9072\n",
      "epoch: 19, loss: 0.3177, train acc: 0.9054\n",
      ">epoch: 19, val_acc: 0.8085, val_f1: 0.5005\n",
      "Namespace(model_type='tri_gcn', modules={'tgcn': True, 'semgcn': True, 'lexgcn': True}, num_layers={'tgcn': 2, 'semgcn': 2, 'lexgcn': 2}, year='2016', train_file='data/train2016restaurant.txt', test_file='data/test2016restaurant.txt', model_path='test_models/2016tri_gcn+concat_seed395_reg0.0005_drop0.4_cdrop0.4_lr2.5e-05_epochs20_adam', val_file='val.txt', log='log', bert_model='bert_large_uncased', cooc_path='cooc_matrix.csv', cooc=       100       1000   10000  10003     10005     10007     10009     1001   \\\n",
      "100      0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "1000     0.0  0.000000    0.0    0.0  0.000606  0.000303  0.000606  0.035120   \n",
      "10000    0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.333333   \n",
      "10003    0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "10005    0.0  0.333333    0.0    0.0  0.000000  0.166667  0.000000  0.166667   \n",
      "...      ...       ...    ...    ...       ...       ...       ...       ...   \n",
      "9992     0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "9993     0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "9996     0.0  0.500000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "9997     0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "9998     0.0  0.400000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "       10010  10015  ...     9986      9987      999    9990      9991   \\\n",
      "100      0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "1000     0.0    0.0  ...  0.002422  0.000303  0.062368    0.0  0.000606   \n",
      "10000    0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "10003    0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "10005    0.0    0.0  ...  0.000000  0.000000  0.333333    0.0  0.000000   \n",
      "...      ...    ...  ...       ...       ...       ...    ...       ...   \n",
      "9992     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "9993     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "9996     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "9997     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "9998     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "\n",
      "       9992   9993      9996   9997      9998   \n",
      "100      0.0    0.0  0.000000    0.0  0.000000  \n",
      "1000     0.0    0.0  0.000606    0.0  0.001211  \n",
      "10000    0.0    0.0  0.000000    0.0  0.000000  \n",
      "10003    0.0    0.0  0.000000    0.0  0.000000  \n",
      "10005    0.0    0.0  0.000000    0.0  0.000000  \n",
      "...      ...    ...       ...    ...       ...  \n",
      "9992     0.0    0.0  0.000000    0.0  0.000000  \n",
      "9993     0.0    0.0  0.000000    0.0  0.000000  \n",
      "9996     0.0    0.0  0.000000    0.0  0.000000  \n",
      "9997     0.0    0.0  0.000000    0.0  0.000000  \n",
      "9998     0.0    0.0  0.000000    0.0  0.000000  \n",
      "\n",
      "[13075 rows x 13075 columns], learning_rate=2.5e-05, dropout=0.4, concat_dropout=0.4, bert_dropout=0.2, l2reg=0.0005, num_epoch=20, batch_size=16, log_step=5, max_seq_len=100, polarities_dim=3, device=device(type='cuda'), seed=395, valset_ratio=0.1, do_train=True, do_eval=True, eval_epoch_num=0, fusion_type='concat', use_ensemble=True, save_models='none', print_sent=False, optim='adam', n_gpu=1)\n",
      "{'none': 0, 'acl': 1, 'acl:relcl': 2, 'advcl': 3, 'advcl:relcl': 4, 'advmod': 5, 'amod': 6, 'appos': 7, 'aux': 8, 'aux:pass': 9, 'case': 10, 'cc': 11, 'cc:preconj': 12, 'ccomp': 13, 'compound': 14, 'compound:prt': 15, 'conj': 16, 'cop': 17, 'csubj': 18, 'dep': 19, 'det': 20, 'det:predet': 21, 'discourse': 22, 'dislocated': 23, 'expl': 24, 'fixed': 25, 'flat': 26, 'iobj': 27, 'list': 28, 'mark': 29, 'nmod': 30, 'nmod:npmod': 31, 'nmod:poss': 32, 'nmod:tmod': 33, 'nsubj': 34, 'nsubj:outer': 35, 'nsubj:pass': 36, 'nummod': 37, 'obj': 38, 'obl': 39, 'obl:agent': 40, 'obl:npmod': 41, 'obl:tmod': 42, 'parataxis': 43, 'punct': 44, 'reparandum': 45, 'root': 46, 'vocative': 47, 'xcomp': 48}\n",
      "{'-1': 0, '0': 1, '1': 2}\n",
      "Model name 'bert_large_uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming 'bert_large_uncased' is a path or url to a directory containing tokenizer files.\n",
      "Didn't find file bert_large_uncased/added_tokens.json. We won't load it.\n",
      "Didn't find file bert_large_uncased/special_tokens_map.json. We won't load it.\n",
      "Didn't find file bert_large_uncased/tokenizer_config.json. We won't load it.\n",
      "loading file bert_large_uncased/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "{\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 3,\n",
      "  \"num_types\": 49,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file bert_large_uncased/pytorch_model.bin\n",
      "Weights of AsaTgcnSem not initialized from pretrained model: ['gate_weight', 'gate_bias', 'ensemble', 'TGCNLayers.0.weight', 'TGCNLayers.0.bias', 'TGCNLayers.0.dense.weight', 'TGCNLayers.1.weight', 'TGCNLayers.1.bias', 'TGCNLayers.1.dense.weight', 'SemGCNLayers.0.weight', 'SemGCNLayers.0.bias', 'SemGCNLayers.1.weight', 'SemGCNLayers.1.bias', 'LexGCNLayers.0.weight', 'LexGCNLayers.0.bias', 'LexGCNLayers.1.weight', 'LexGCNLayers.1.bias', 'fc_single.weight', 'fc_single.bias', 'ensemble_linear_tgcn.weight', 'ensemble_linear_tgcn.bias', 'ensemble_linear_semgcn.weight', 'ensemble_linear_semgcn.bias', 'ensemble_linear_lexgcn.weight', 'ensemble_linear_lexgcn.bias', 'dep_embedding.weight']\n",
      "Weights from pretrained model not used in AsaTgcnSem: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "[{'input_ids': tensor([  101, 13325,  2013,  3025,  8466,  2023,  2109,  2000,  2022,  1037,\n",
      "         2204,  2173,  1010,  2021,  2025,  2151,  2936,  1012,   102,  2173,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'valid_ids': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'segment_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'mem_valid_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'dep_adj_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'dep_value_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'polarity': 0, 'raw_text': 'judging from previous posts this used to be a good place , but not any longer .', 'aspect': 'place'}]\n",
      "[{'input_ids': tensor([  101, 13325,  2013,  3025,  8466,  2023,  2109,  2000,  2022,  1037,\n",
      "         2204,  2173,  1010,  2021,  2025,  2151,  2936,  1012,   102,  2173,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'valid_ids': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'segment_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'mem_valid_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'dep_adj_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'dep_value_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'polarity': 0, 'raw_text': 'judging from previous posts this used to be a good place , but not any longer .', 'aspect': 'place'}]\n",
      "[{'input_ids': tensor([  101,  4240,  2428,  2204, 10514,  6182,  1012,   102, 10514,  6182,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'valid_ids': tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'segment_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'mem_valid_ids': tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'dep_adj_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'dep_value_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 5,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'polarity': 2, 'raw_text': 'serves really good sushi .', 'aspect': 'sushi'}]\n",
      "cuda memory allocated: 1400893440\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 0\n",
      "epoch: 0, loss: 0.9516, train acc: 0.5750\n",
      "epoch: 0, loss: 0.9118, train acc: 0.6375\n",
      "epoch: 0, loss: 0.9037, train acc: 0.6708\n",
      "epoch: 0, loss: 0.9117, train acc: 0.6375\n",
      "epoch: 0, loss: 0.9359, train acc: 0.6275\n",
      "epoch: 0, loss: 0.9190, train acc: 0.6375\n",
      "epoch: 0, loss: 0.9455, train acc: 0.6214\n",
      "epoch: 0, loss: 0.9596, train acc: 0.6156\n",
      "epoch: 0, loss: 0.9605, train acc: 0.6028\n",
      "epoch: 0, loss: 0.9553, train acc: 0.6025\n",
      "epoch: 0, loss: 0.9556, train acc: 0.6057\n",
      "epoch: 0, loss: 0.9439, train acc: 0.6073\n",
      "epoch: 0, loss: 0.9367, train acc: 0.6077\n",
      "epoch: 0, loss: 0.9361, train acc: 0.6116\n",
      "epoch: 0, loss: 0.9321, train acc: 0.6150\n",
      "epoch: 0, loss: 0.9142, train acc: 0.6250\n",
      "epoch: 0, loss: 0.9084, train acc: 0.6279\n",
      "epoch: 0, loss: 0.9039, train acc: 0.6285\n",
      "epoch: 0, loss: 0.9006, train acc: 0.6296\n",
      "epoch: 0, loss: 0.8978, train acc: 0.6312\n",
      "epoch: 0, loss: 0.9030, train acc: 0.6232\n",
      ">epoch: 0, val_acc: 0.6915, val_f1: 0.3080\n",
      ">> epoch: 0, test_acc: 0.7385, test_f1: 0.3356\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 1\n",
      "epoch: 1, loss: 0.5860, train acc: 0.7656\n",
      "epoch: 1, loss: 0.7900, train acc: 0.6875\n",
      "epoch: 1, loss: 0.7883, train acc: 0.6607\n",
      "epoch: 1, loss: 0.8119, train acc: 0.6711\n",
      "epoch: 1, loss: 0.7898, train acc: 0.6719\n",
      "epoch: 1, loss: 0.7813, train acc: 0.6724\n",
      "epoch: 1, loss: 0.7632, train acc: 0.6912\n",
      "epoch: 1, loss: 0.7536, train acc: 0.6955\n",
      "epoch: 1, loss: 0.7470, train acc: 0.7017\n",
      "epoch: 1, loss: 0.7436, train acc: 0.7041\n",
      "epoch: 1, loss: 0.7473, train acc: 0.6991\n",
      "epoch: 1, loss: 0.7559, train acc: 0.6875\n",
      "epoch: 1, loss: 0.7493, train acc: 0.6875\n",
      "epoch: 1, loss: 0.7433, train acc: 0.6920\n",
      "epoch: 1, loss: 0.7441, train acc: 0.6951\n",
      "epoch: 1, loss: 0.7468, train acc: 0.6938\n",
      "epoch: 1, loss: 0.7491, train acc: 0.6927\n",
      "epoch: 1, loss: 0.7498, train acc: 0.6917\n",
      "epoch: 1, loss: 0.7452, train acc: 0.6902\n",
      "epoch: 1, loss: 0.7449, train acc: 0.6919\n",
      "epoch: 1, loss: 0.7502, train acc: 0.6905\n",
      ">epoch: 1, val_acc: 0.7394, val_f1: 0.3676\n",
      ">> epoch: 1, test_acc: 0.7646, test_f1: 0.3717\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 2\n",
      "epoch: 2, loss: 0.6948, train acc: 0.7083\n",
      "epoch: 2, loss: 0.6625, train acc: 0.7500\n",
      "epoch: 2, loss: 0.6495, train acc: 0.7404\n",
      "epoch: 2, loss: 0.6514, train acc: 0.7361\n",
      "epoch: 2, loss: 0.6297, train acc: 0.7391\n",
      "epoch: 2, loss: 0.6166, train acc: 0.7500\n",
      "epoch: 2, loss: 0.6506, train acc: 0.7348\n",
      "epoch: 2, loss: 0.6162, train acc: 0.7500\n",
      "epoch: 2, loss: 0.6069, train acc: 0.7558\n",
      "epoch: 2, loss: 0.5940, train acc: 0.7604\n",
      "epoch: 2, loss: 0.5879, train acc: 0.7642\n",
      "epoch: 2, loss: 0.5878, train acc: 0.7683\n",
      "epoch: 2, loss: 0.5800, train acc: 0.7748\n",
      "epoch: 2, loss: 0.5632, train acc: 0.7803\n",
      "epoch: 2, loss: 0.5561, train acc: 0.7860\n",
      "epoch: 2, loss: 0.5502, train acc: 0.7893\n",
      "epoch: 2, loss: 0.5507, train acc: 0.7869\n",
      "epoch: 2, loss: 0.5437, train acc: 0.7905\n",
      "epoch: 2, loss: 0.5476, train acc: 0.7897\n",
      "epoch: 2, loss: 0.5434, train acc: 0.7908\n",
      "epoch: 2, loss: 0.5406, train acc: 0.7931\n",
      ">epoch: 2, val_acc: 0.8777, val_f1: 0.5761\n",
      ">> epoch: 2, test_acc: 0.8615, test_f1: 0.5579\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 3\n",
      "epoch: 3, loss: 0.4027, train acc: 0.8750\n",
      "epoch: 3, loss: 0.4689, train acc: 0.8393\n",
      "epoch: 3, loss: 0.4547, train acc: 0.8438\n",
      "epoch: 3, loss: 0.4226, train acc: 0.8493\n",
      "epoch: 3, loss: 0.3886, train acc: 0.8693\n",
      "epoch: 3, loss: 0.3881, train acc: 0.8657\n",
      "epoch: 3, loss: 0.3831, train acc: 0.8711\n",
      "epoch: 3, loss: 0.3919, train acc: 0.8716\n",
      "epoch: 3, loss: 0.3751, train acc: 0.8780\n",
      "epoch: 3, loss: 0.3688, train acc: 0.8777\n",
      "epoch: 3, loss: 0.3616, train acc: 0.8786\n",
      "epoch: 3, loss: 0.3654, train acc: 0.8794\n",
      "epoch: 3, loss: 0.3612, train acc: 0.8810\n",
      "epoch: 3, loss: 0.3501, train acc: 0.8834\n",
      "epoch: 3, loss: 0.3566, train acc: 0.8793\n",
      "epoch: 3, loss: 0.3466, train acc: 0.8839\n",
      "epoch: 3, loss: 0.3507, train acc: 0.8834\n",
      "epoch: 3, loss: 0.3478, train acc: 0.8851\n",
      "epoch: 3, loss: 0.3449, train acc: 0.8859\n",
      "epoch: 3, loss: 0.3487, train acc: 0.8853\n",
      "epoch: 3, loss: 0.3528, train acc: 0.8836\n",
      ">epoch: 3, val_acc: 0.8883, val_f1: 0.6563\n",
      ">> epoch: 3, test_acc: 0.8600, test_f1: 0.5480\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 4\n",
      "epoch: 4, loss: 0.3886, train acc: 0.8750\n",
      "epoch: 4, loss: 0.2946, train acc: 0.8750\n",
      "epoch: 4, loss: 0.2993, train acc: 0.8864\n",
      "epoch: 4, loss: 0.3382, train acc: 0.8711\n",
      "epoch: 4, loss: 0.3391, train acc: 0.8750\n",
      "epoch: 4, loss: 0.3227, train acc: 0.8774\n",
      "epoch: 4, loss: 0.3190, train acc: 0.8831\n",
      "epoch: 4, loss: 0.3013, train acc: 0.8924\n",
      "epoch: 4, loss: 0.3068, train acc: 0.8918\n",
      "epoch: 4, loss: 0.3015, train acc: 0.8940\n",
      "epoch: 4, loss: 0.2926, train acc: 0.8958\n",
      "epoch: 4, loss: 0.2936, train acc: 0.8929\n",
      "epoch: 4, loss: 0.2962, train acc: 0.8945\n",
      "epoch: 4, loss: 0.2884, train acc: 0.8977\n",
      "epoch: 4, loss: 0.2951, train acc: 0.8961\n",
      "epoch: 4, loss: 0.2920, train acc: 0.8972\n",
      "epoch: 4, loss: 0.3012, train acc: 0.8943\n",
      "epoch: 4, loss: 0.2957, train acc: 0.8961\n",
      "epoch: 4, loss: 0.2882, train acc: 0.8990\n",
      "epoch: 4, loss: 0.2884, train acc: 0.8997\n",
      "epoch: 4, loss: 0.2859, train acc: 0.9016\n",
      "epoch: 4, loss: 0.2821, train acc: 0.9037\n",
      ">epoch: 4, val_acc: 0.9043, val_f1: 0.6030\n",
      ">> epoch: 4, test_acc: 0.8738, test_f1: 0.5676\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 5\n",
      "epoch: 5, loss: 0.1093, train acc: 0.9750\n",
      "epoch: 5, loss: 0.0978, train acc: 0.9688\n",
      "epoch: 5, loss: 0.1752, train acc: 0.9458\n",
      "epoch: 5, loss: 0.1776, train acc: 0.9406\n",
      "epoch: 5, loss: 0.1968, train acc: 0.9350\n",
      "epoch: 5, loss: 0.1906, train acc: 0.9354\n",
      "epoch: 5, loss: 0.1834, train acc: 0.9375\n",
      "epoch: 5, loss: 0.1855, train acc: 0.9359\n",
      "epoch: 5, loss: 0.1826, train acc: 0.9319\n",
      "epoch: 5, loss: 0.2038, train acc: 0.9250\n",
      "epoch: 5, loss: 0.1991, train acc: 0.9295\n",
      "epoch: 5, loss: 0.1995, train acc: 0.9313\n",
      "epoch: 5, loss: 0.1998, train acc: 0.9308\n",
      "epoch: 5, loss: 0.1994, train acc: 0.9313\n",
      "epoch: 5, loss: 0.2074, train acc: 0.9283\n",
      "epoch: 5, loss: 0.2044, train acc: 0.9305\n",
      "epoch: 5, loss: 0.2055, train acc: 0.9309\n",
      "epoch: 5, loss: 0.2015, train acc: 0.9319\n",
      "epoch: 5, loss: 0.2054, train acc: 0.9303\n",
      "epoch: 5, loss: 0.2055, train acc: 0.9306\n",
      "epoch: 5, loss: 0.2060, train acc: 0.9315\n",
      ">epoch: 5, val_acc: 0.9149, val_f1: 0.7840\n",
      ">> epoch: 5, test_acc: 0.8985, test_f1: 0.7031\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 6\n",
      "epoch: 6, loss: 0.2002, train acc: 0.9219\n",
      "epoch: 6, loss: 0.1828, train acc: 0.9306\n",
      "epoch: 6, loss: 0.1574, train acc: 0.9420\n",
      "epoch: 6, loss: 0.1498, train acc: 0.9474\n",
      "epoch: 6, loss: 0.1499, train acc: 0.9505\n",
      "epoch: 6, loss: 0.1443, train acc: 0.9526\n",
      "epoch: 6, loss: 0.1523, train acc: 0.9485\n",
      "epoch: 6, loss: 0.1608, train acc: 0.9439\n",
      "epoch: 6, loss: 0.1648, train acc: 0.9403\n",
      "epoch: 6, loss: 0.1591, train acc: 0.9426\n",
      "epoch: 6, loss: 0.1590, train acc: 0.9398\n",
      "epoch: 6, loss: 0.1590, train acc: 0.9375\n",
      "epoch: 6, loss: 0.1581, train acc: 0.9395\n",
      "epoch: 6, loss: 0.1667, train acc: 0.9366\n",
      "epoch: 6, loss: 0.1673, train acc: 0.9367\n",
      "epoch: 6, loss: 0.1605, train acc: 0.9391\n",
      "epoch: 6, loss: 0.1632, train acc: 0.9390\n",
      "epoch: 6, loss: 0.1617, train acc: 0.9403\n",
      "epoch: 6, loss: 0.1625, train acc: 0.9402\n",
      "epoch: 6, loss: 0.1596, train acc: 0.9407\n",
      "epoch: 6, loss: 0.1595, train acc: 0.9399\n",
      ">epoch: 6, val_acc: 0.9149, val_f1: 0.7960\n",
      ">> epoch: 6, test_acc: 0.8862, test_f1: 0.7172\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 7\n",
      "epoch: 7, loss: 0.0648, train acc: 0.9792\n",
      "epoch: 7, loss: 0.0927, train acc: 0.9688\n",
      "epoch: 7, loss: 0.0846, train acc: 0.9663\n",
      "epoch: 7, loss: 0.0924, train acc: 0.9688\n",
      "epoch: 7, loss: 0.0874, train acc: 0.9728\n",
      "epoch: 7, loss: 0.1049, train acc: 0.9643\n",
      "epoch: 7, loss: 0.0940, train acc: 0.9678\n",
      "epoch: 7, loss: 0.1013, train acc: 0.9655\n",
      "epoch: 7, loss: 0.0988, train acc: 0.9666\n",
      "epoch: 7, loss: 0.0989, train acc: 0.9648\n",
      "epoch: 7, loss: 0.0947, train acc: 0.9670\n",
      "epoch: 7, loss: 0.0940, train acc: 0.9666\n",
      "epoch: 7, loss: 0.1038, train acc: 0.9643\n",
      "epoch: 7, loss: 0.1091, train acc: 0.9605\n",
      "epoch: 7, loss: 0.1040, train acc: 0.9632\n",
      "epoch: 7, loss: 0.1043, train acc: 0.9623\n",
      "epoch: 7, loss: 0.1083, train acc: 0.9616\n",
      "epoch: 7, loss: 0.1157, train acc: 0.9595\n",
      "epoch: 7, loss: 0.1184, train acc: 0.9570\n",
      "epoch: 7, loss: 0.1219, train acc: 0.9554\n",
      "epoch: 7, loss: 0.1208, train acc: 0.9563\n",
      ">epoch: 7, val_acc: 0.9043, val_f1: 0.6577\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 8\n",
      "epoch: 8, loss: 0.0873, train acc: 0.9688\n",
      "epoch: 8, loss: 0.1064, train acc: 0.9554\n",
      "epoch: 8, loss: 0.0975, train acc: 0.9635\n",
      "epoch: 8, loss: 0.0926, train acc: 0.9632\n",
      "epoch: 8, loss: 0.0819, train acc: 0.9659\n",
      "epoch: 8, loss: 0.0964, train acc: 0.9583\n",
      "epoch: 8, loss: 0.0880, train acc: 0.9629\n",
      "epoch: 8, loss: 0.1039, train acc: 0.9595\n",
      "epoch: 8, loss: 0.1058, train acc: 0.9613\n",
      "epoch: 8, loss: 0.0983, train acc: 0.9654\n",
      "epoch: 8, loss: 0.1006, train acc: 0.9639\n",
      "epoch: 8, loss: 0.0956, train acc: 0.9660\n",
      "epoch: 8, loss: 0.0979, train acc: 0.9657\n",
      "epoch: 8, loss: 0.1050, train acc: 0.9646\n",
      "epoch: 8, loss: 0.1062, train acc: 0.9627\n",
      "epoch: 8, loss: 0.1071, train acc: 0.9635\n",
      "epoch: 8, loss: 0.1081, train acc: 0.9611\n",
      "epoch: 8, loss: 0.1132, train acc: 0.9598\n",
      "epoch: 8, loss: 0.1097, train acc: 0.9613\n",
      "epoch: 8, loss: 0.1106, train acc: 0.9620\n",
      "epoch: 8, loss: 0.1101, train acc: 0.9620\n",
      ">epoch: 8, val_acc: 0.9255, val_f1: 0.8083\n",
      ">> epoch: 8, test_acc: 0.8923, test_f1: 0.7428\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 9\n",
      "epoch: 9, loss: 0.0469, train acc: 1.0000\n",
      "epoch: 9, loss: 0.0384, train acc: 0.9896\n",
      "epoch: 9, loss: 0.0722, train acc: 0.9659\n",
      "epoch: 9, loss: 0.0561, train acc: 0.9766\n",
      "epoch: 9, loss: 0.0868, train acc: 0.9702\n",
      "epoch: 9, loss: 0.0836, train acc: 0.9712\n",
      "epoch: 9, loss: 0.0709, train acc: 0.9758\n",
      "epoch: 9, loss: 0.0765, train acc: 0.9722\n",
      "epoch: 9, loss: 0.0762, train acc: 0.9726\n",
      "epoch: 9, loss: 0.0745, train acc: 0.9728\n",
      "epoch: 9, loss: 0.0762, train acc: 0.9718\n",
      "epoch: 9, loss: 0.0739, train acc: 0.9721\n",
      "epoch: 9, loss: 0.0802, train acc: 0.9682\n",
      "epoch: 9, loss: 0.0889, train acc: 0.9650\n",
      "epoch: 9, loss: 0.0898, train acc: 0.9648\n",
      "epoch: 9, loss: 0.0893, train acc: 0.9646\n",
      "epoch: 9, loss: 0.0902, train acc: 0.9645\n",
      "epoch: 9, loss: 0.0888, train acc: 0.9658\n",
      "epoch: 9, loss: 0.0859, train acc: 0.9677\n",
      "epoch: 9, loss: 0.0867, train acc: 0.9674\n",
      "epoch: 9, loss: 0.0901, train acc: 0.9672\n",
      "epoch: 9, loss: 0.0925, train acc: 0.9669\n",
      ">epoch: 9, val_acc: 0.9309, val_f1: 0.8354\n",
      ">> epoch: 9, test_acc: 0.8938, test_f1: 0.7527\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 10\n",
      "epoch: 10, loss: 0.0328, train acc: 0.9875\n",
      "epoch: 10, loss: 0.0716, train acc: 0.9688\n",
      "epoch: 10, loss: 0.0526, train acc: 0.9792\n",
      "epoch: 10, loss: 0.0712, train acc: 0.9750\n",
      "epoch: 10, loss: 0.0676, train acc: 0.9775\n",
      "epoch: 10, loss: 0.0664, train acc: 0.9750\n",
      "epoch: 10, loss: 0.0664, train acc: 0.9750\n",
      "epoch: 10, loss: 0.0626, train acc: 0.9766\n",
      "epoch: 10, loss: 0.0686, train acc: 0.9722\n",
      "epoch: 10, loss: 0.0719, train acc: 0.9700\n",
      "epoch: 10, loss: 0.0693, train acc: 0.9716\n",
      "epoch: 10, loss: 0.0674, train acc: 0.9719\n",
      "epoch: 10, loss: 0.0697, train acc: 0.9702\n",
      "epoch: 10, loss: 0.0725, train acc: 0.9705\n",
      "epoch: 10, loss: 0.0744, train acc: 0.9692\n",
      "epoch: 10, loss: 0.0705, train acc: 0.9711\n",
      "epoch: 10, loss: 0.0680, train acc: 0.9728\n",
      "epoch: 10, loss: 0.0687, train acc: 0.9729\n",
      "epoch: 10, loss: 0.0683, train acc: 0.9737\n",
      "epoch: 10, loss: 0.0707, train acc: 0.9719\n",
      "epoch: 10, loss: 0.0705, train acc: 0.9720\n",
      ">epoch: 10, val_acc: 0.8936, val_f1: 0.7268\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 11\n",
      "epoch: 11, loss: 0.0553, train acc: 0.9844\n",
      "epoch: 11, loss: 0.1390, train acc: 0.9583\n",
      "epoch: 11, loss: 0.1263, train acc: 0.9554\n",
      "epoch: 11, loss: 0.1110, train acc: 0.9605\n",
      "epoch: 11, loss: 0.0949, train acc: 0.9661\n",
      "epoch: 11, loss: 0.0852, train acc: 0.9698\n",
      "epoch: 11, loss: 0.0800, train acc: 0.9724\n",
      "epoch: 11, loss: 0.0859, train acc: 0.9663\n",
      "epoch: 11, loss: 0.0799, train acc: 0.9688\n",
      "epoch: 11, loss: 0.0731, train acc: 0.9719\n",
      "epoch: 11, loss: 0.0756, train acc: 0.9722\n",
      "epoch: 11, loss: 0.0768, train acc: 0.9735\n",
      "epoch: 11, loss: 0.0797, train acc: 0.9727\n",
      "epoch: 11, loss: 0.0781, train acc: 0.9719\n",
      "epoch: 11, loss: 0.0782, train acc: 0.9721\n",
      "epoch: 11, loss: 0.0776, train acc: 0.9723\n",
      "epoch: 11, loss: 0.0799, train acc: 0.9702\n",
      "epoch: 11, loss: 0.0843, train acc: 0.9705\n",
      "epoch: 11, loss: 0.0818, train acc: 0.9714\n",
      "epoch: 11, loss: 0.0831, train acc: 0.9710\n",
      "epoch: 11, loss: 0.0812, train acc: 0.9718\n",
      ">epoch: 11, val_acc: 0.9149, val_f1: 0.7772\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 12\n",
      "epoch: 12, loss: 0.0103, train acc: 1.0000\n",
      "epoch: 12, loss: 0.0208, train acc: 0.9922\n",
      "epoch: 12, loss: 0.0404, train acc: 0.9760\n",
      "epoch: 12, loss: 0.0446, train acc: 0.9792\n",
      "epoch: 12, loss: 0.0483, train acc: 0.9755\n",
      "epoch: 12, loss: 0.0400, train acc: 0.9799\n",
      "epoch: 12, loss: 0.0370, train acc: 0.9811\n",
      "epoch: 12, loss: 0.0400, train acc: 0.9819\n",
      "epoch: 12, loss: 0.0416, train acc: 0.9811\n",
      "epoch: 12, loss: 0.0428, train acc: 0.9818\n",
      "epoch: 12, loss: 0.0406, train acc: 0.9835\n",
      "epoch: 12, loss: 0.0453, train acc: 0.9828\n",
      "epoch: 12, loss: 0.0421, train acc: 0.9841\n",
      "epoch: 12, loss: 0.0448, train acc: 0.9825\n",
      "epoch: 12, loss: 0.0518, train acc: 0.9820\n",
      "epoch: 12, loss: 0.0582, train acc: 0.9808\n",
      "epoch: 12, loss: 0.0567, train acc: 0.9812\n",
      "epoch: 12, loss: 0.0579, train acc: 0.9801\n",
      "epoch: 12, loss: 0.0647, train acc: 0.9772\n",
      "epoch: 12, loss: 0.0677, train acc: 0.9770\n",
      "epoch: 12, loss: 0.0715, train acc: 0.9733\n",
      ">epoch: 12, val_acc: 0.9096, val_f1: 0.7494\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 13\n",
      "epoch: 13, loss: 0.0230, train acc: 1.0000\n",
      "epoch: 13, loss: 0.0313, train acc: 0.9911\n",
      "epoch: 13, loss: 0.0239, train acc: 0.9948\n",
      "epoch: 13, loss: 0.0232, train acc: 0.9926\n",
      "epoch: 13, loss: 0.0236, train acc: 0.9915\n",
      "epoch: 13, loss: 0.0196, train acc: 0.9931\n",
      "epoch: 13, loss: 0.0216, train acc: 0.9941\n",
      "epoch: 13, loss: 0.0236, train acc: 0.9932\n",
      "epoch: 13, loss: 0.0285, train acc: 0.9911\n",
      "epoch: 13, loss: 0.0313, train acc: 0.9907\n",
      "epoch: 13, loss: 0.0409, train acc: 0.9880\n",
      "epoch: 13, loss: 0.0462, train acc: 0.9857\n",
      "epoch: 13, loss: 0.0468, train acc: 0.9859\n",
      "epoch: 13, loss: 0.0468, train acc: 0.9841\n",
      "epoch: 13, loss: 0.0466, train acc: 0.9844\n",
      "epoch: 13, loss: 0.0491, train acc: 0.9830\n",
      "epoch: 13, loss: 0.0493, train acc: 0.9817\n",
      "epoch: 13, loss: 0.0517, train acc: 0.9799\n",
      "epoch: 13, loss: 0.0519, train acc: 0.9796\n",
      "epoch: 13, loss: 0.0524, train acc: 0.9800\n",
      "epoch: 13, loss: 0.0519, train acc: 0.9798\n",
      ">epoch: 13, val_acc: 0.9255, val_f1: 0.7965\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 14\n",
      "epoch: 14, loss: 0.1197, train acc: 0.9375\n",
      "epoch: 14, loss: 0.0535, train acc: 0.9688\n",
      "epoch: 14, loss: 0.0327, train acc: 0.9830\n",
      "epoch: 14, loss: 0.0309, train acc: 0.9844\n",
      "epoch: 14, loss: 0.0361, train acc: 0.9792\n",
      "epoch: 14, loss: 0.0355, train acc: 0.9808\n",
      "epoch: 14, loss: 0.0581, train acc: 0.9758\n",
      "epoch: 14, loss: 0.0538, train acc: 0.9774\n",
      "epoch: 14, loss: 0.0544, train acc: 0.9771\n",
      "epoch: 14, loss: 0.0588, train acc: 0.9755\n",
      "epoch: 14, loss: 0.0557, train acc: 0.9767\n",
      "epoch: 14, loss: 0.0530, train acc: 0.9777\n",
      "epoch: 14, loss: 0.0526, train acc: 0.9775\n",
      "epoch: 14, loss: 0.0558, train acc: 0.9754\n",
      "epoch: 14, loss: 0.0541, train acc: 0.9762\n",
      "epoch: 14, loss: 0.0527, train acc: 0.9770\n",
      "epoch: 14, loss: 0.0571, train acc: 0.9753\n",
      "epoch: 14, loss: 0.0577, train acc: 0.9753\n",
      "epoch: 14, loss: 0.0580, train acc: 0.9753\n",
      "epoch: 14, loss: 0.0637, train acc: 0.9733\n",
      "epoch: 14, loss: 0.0643, train acc: 0.9734\n",
      "epoch: 14, loss: 0.0666, train acc: 0.9722\n",
      ">epoch: 14, val_acc: 0.9043, val_f1: 0.7721\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 15\n",
      "epoch: 15, loss: 0.0502, train acc: 0.9875\n",
      "epoch: 15, loss: 0.0545, train acc: 0.9812\n",
      "epoch: 15, loss: 0.0673, train acc: 0.9708\n",
      "epoch: 15, loss: 0.0974, train acc: 0.9719\n",
      "epoch: 15, loss: 0.0865, train acc: 0.9725\n",
      "epoch: 15, loss: 0.0820, train acc: 0.9708\n",
      "epoch: 15, loss: 0.0775, train acc: 0.9732\n",
      "epoch: 15, loss: 0.0714, train acc: 0.9766\n",
      "epoch: 15, loss: 0.0718, train acc: 0.9764\n",
      "epoch: 15, loss: 0.0732, train acc: 0.9775\n",
      "epoch: 15, loss: 0.0696, train acc: 0.9784\n",
      "epoch: 15, loss: 0.0731, train acc: 0.9771\n",
      "epoch: 15, loss: 0.0768, train acc: 0.9750\n",
      "epoch: 15, loss: 0.0747, train acc: 0.9750\n",
      "epoch: 15, loss: 0.0734, train acc: 0.9750\n",
      "epoch: 15, loss: 0.0713, train acc: 0.9750\n",
      "epoch: 15, loss: 0.0707, train acc: 0.9757\n",
      "epoch: 15, loss: 0.0688, train acc: 0.9757\n",
      "epoch: 15, loss: 0.0678, train acc: 0.9757\n",
      "epoch: 15, loss: 0.0651, train acc: 0.9762\n",
      "epoch: 15, loss: 0.0673, train acc: 0.9750\n",
      ">epoch: 15, val_acc: 0.8989, val_f1: 0.7004\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 16\n",
      "epoch: 16, loss: 0.0713, train acc: 0.9531\n",
      "epoch: 16, loss: 0.0435, train acc: 0.9792\n",
      "epoch: 16, loss: 0.0620, train acc: 0.9688\n",
      "epoch: 16, loss: 0.0482, train acc: 0.9770\n",
      "epoch: 16, loss: 0.0566, train acc: 0.9740\n",
      "epoch: 16, loss: 0.0569, train acc: 0.9763\n",
      "epoch: 16, loss: 0.0567, train acc: 0.9761\n",
      "epoch: 16, loss: 0.0555, train acc: 0.9760\n",
      "epoch: 16, loss: 0.0562, train acc: 0.9759\n",
      "epoch: 16, loss: 0.0548, train acc: 0.9758\n",
      "epoch: 16, loss: 0.0526, train acc: 0.9769\n",
      "epoch: 16, loss: 0.0563, train acc: 0.9756\n",
      "epoch: 16, loss: 0.0551, train acc: 0.9756\n",
      "epoch: 16, loss: 0.0531, train acc: 0.9764\n",
      "epoch: 16, loss: 0.0607, train acc: 0.9747\n",
      "epoch: 16, loss: 0.0663, train acc: 0.9739\n",
      "epoch: 16, loss: 0.0701, train acc: 0.9717\n",
      "epoch: 16, loss: 0.0720, train acc: 0.9712\n",
      "epoch: 16, loss: 0.0744, train acc: 0.9707\n",
      "epoch: 16, loss: 0.0730, train acc: 0.9710\n",
      "epoch: 16, loss: 0.0753, train acc: 0.9700\n",
      ">epoch: 16, val_acc: 0.8989, val_f1: 0.7314\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 17\n",
      "epoch: 17, loss: 0.1058, train acc: 0.9375\n",
      "epoch: 17, loss: 0.0566, train acc: 0.9688\n",
      "epoch: 17, loss: 0.0573, train acc: 0.9712\n",
      "epoch: 17, loss: 0.0571, train acc: 0.9722\n",
      "epoch: 17, loss: 0.0554, train acc: 0.9728\n",
      "epoch: 17, loss: 0.0487, train acc: 0.9754\n",
      "epoch: 17, loss: 0.0441, train acc: 0.9773\n",
      "epoch: 17, loss: 0.0398, train acc: 0.9786\n",
      "epoch: 17, loss: 0.0389, train acc: 0.9782\n",
      "epoch: 17, loss: 0.0479, train acc: 0.9766\n",
      "epoch: 17, loss: 0.0512, train acc: 0.9741\n",
      "epoch: 17, loss: 0.0527, train acc: 0.9731\n",
      "epoch: 17, loss: 0.0570, train acc: 0.9722\n",
      "epoch: 17, loss: 0.0560, train acc: 0.9733\n",
      "epoch: 17, loss: 0.0538, train acc: 0.9752\n",
      "epoch: 17, loss: 0.0533, train acc: 0.9752\n",
      "epoch: 17, loss: 0.0538, train acc: 0.9759\n",
      "epoch: 17, loss: 0.0510, train acc: 0.9773\n",
      "epoch: 17, loss: 0.0499, train acc: 0.9778\n",
      "epoch: 17, loss: 0.0567, train acc: 0.9770\n",
      "epoch: 17, loss: 0.0598, train acc: 0.9751\n",
      ">epoch: 17, val_acc: 0.8936, val_f1: 0.7266\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 18\n",
      "epoch: 18, loss: 0.0068, train acc: 1.0000\n",
      "epoch: 18, loss: 0.0452, train acc: 0.9732\n",
      "epoch: 18, loss: 0.0462, train acc: 0.9792\n",
      "epoch: 18, loss: 0.0497, train acc: 0.9779\n",
      "epoch: 18, loss: 0.0628, train acc: 0.9688\n",
      "epoch: 18, loss: 0.0531, train acc: 0.9745\n",
      "epoch: 18, loss: 0.0517, train acc: 0.9746\n",
      "epoch: 18, loss: 0.0536, train acc: 0.9747\n",
      "epoch: 18, loss: 0.0525, train acc: 0.9747\n",
      "epoch: 18, loss: 0.0504, train acc: 0.9761\n",
      "epoch: 18, loss: 0.0464, train acc: 0.9784\n",
      "epoch: 18, loss: 0.0499, train acc: 0.9759\n",
      "epoch: 18, loss: 0.0500, train acc: 0.9758\n",
      "epoch: 18, loss: 0.0469, train acc: 0.9776\n",
      "epoch: 18, loss: 0.0481, train acc: 0.9766\n",
      "epoch: 18, loss: 0.0501, train acc: 0.9756\n",
      "epoch: 18, loss: 0.0481, train acc: 0.9771\n",
      "epoch: 18, loss: 0.0481, train acc: 0.9770\n",
      "epoch: 18, loss: 0.0457, train acc: 0.9783\n",
      "epoch: 18, loss: 0.0456, train acc: 0.9781\n",
      "epoch: 18, loss: 0.0434, train acc: 0.9792\n",
      ">epoch: 18, val_acc: 0.8936, val_f1: 0.7553\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 19\n",
      "epoch: 19, loss: 0.0010, train acc: 1.0000\n",
      "epoch: 19, loss: 0.0493, train acc: 0.9688\n",
      "epoch: 19, loss: 0.0391, train acc: 0.9773\n",
      "epoch: 19, loss: 0.0287, train acc: 0.9844\n",
      "epoch: 19, loss: 0.0420, train acc: 0.9792\n",
      "epoch: 19, loss: 0.0405, train acc: 0.9784\n",
      "epoch: 19, loss: 0.0422, train acc: 0.9758\n",
      "epoch: 19, loss: 0.0462, train acc: 0.9757\n",
      "epoch: 19, loss: 0.0418, train acc: 0.9787\n",
      "epoch: 19, loss: 0.0385, train acc: 0.9796\n",
      "epoch: 19, loss: 0.0367, train acc: 0.9804\n",
      "epoch: 19, loss: 0.0370, train acc: 0.9799\n",
      "epoch: 19, loss: 0.0389, train acc: 0.9795\n",
      "epoch: 19, loss: 0.0371, train acc: 0.9801\n",
      "epoch: 19, loss: 0.0348, train acc: 0.9815\n",
      "epoch: 19, loss: 0.0334, train acc: 0.9819\n",
      "epoch: 19, loss: 0.0344, train acc: 0.9823\n",
      "epoch: 19, loss: 0.0326, train acc: 0.9833\n",
      "epoch: 19, loss: 0.0341, train acc: 0.9828\n",
      "epoch: 19, loss: 0.0358, train acc: 0.9811\n",
      "epoch: 19, loss: 0.0355, train acc: 0.9814\n",
      "epoch: 19, loss: 0.0372, train acc: 0.9805\n",
      ">epoch: 19, val_acc: 0.8989, val_f1: 0.7604\n",
      "Namespace(model_type='tri_gcn', modules={'tgcn': True, 'semgcn': True, 'lexgcn': True}, num_layers={'tgcn': 2, 'semgcn': 2, 'lexgcn': 2}, year='2016', train_file='data/train2016restaurant.txt', test_file='data/test2016restaurant.txt', model_path='test_models/2016tri_gcn+concat_seed372_reg0.0005_drop0.4_cdrop0.4_lr2.5e-05_epochs20_adam', val_file='val.txt', log='log', bert_model='bert_large_uncased', cooc_path='cooc_matrix.csv', cooc=       100       1000   10000  10003     10005     10007     10009     1001   \\\n",
      "100      0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "1000     0.0  0.000000    0.0    0.0  0.000606  0.000303  0.000606  0.035120   \n",
      "10000    0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.333333   \n",
      "10003    0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "10005    0.0  0.333333    0.0    0.0  0.000000  0.166667  0.000000  0.166667   \n",
      "...      ...       ...    ...    ...       ...       ...       ...       ...   \n",
      "9992     0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "9993     0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "9996     0.0  0.500000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "9997     0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "9998     0.0  0.400000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "       10010  10015  ...     9986      9987      999    9990      9991   \\\n",
      "100      0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "1000     0.0    0.0  ...  0.002422  0.000303  0.062368    0.0  0.000606   \n",
      "10000    0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "10003    0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "10005    0.0    0.0  ...  0.000000  0.000000  0.333333    0.0  0.000000   \n",
      "...      ...    ...  ...       ...       ...       ...    ...       ...   \n",
      "9992     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "9993     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "9996     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "9997     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "9998     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "\n",
      "       9992   9993      9996   9997      9998   \n",
      "100      0.0    0.0  0.000000    0.0  0.000000  \n",
      "1000     0.0    0.0  0.000606    0.0  0.001211  \n",
      "10000    0.0    0.0  0.000000    0.0  0.000000  \n",
      "10003    0.0    0.0  0.000000    0.0  0.000000  \n",
      "10005    0.0    0.0  0.000000    0.0  0.000000  \n",
      "...      ...    ...       ...    ...       ...  \n",
      "9992     0.0    0.0  0.000000    0.0  0.000000  \n",
      "9993     0.0    0.0  0.000000    0.0  0.000000  \n",
      "9996     0.0    0.0  0.000000    0.0  0.000000  \n",
      "9997     0.0    0.0  0.000000    0.0  0.000000  \n",
      "9998     0.0    0.0  0.000000    0.0  0.000000  \n",
      "\n",
      "[13075 rows x 13075 columns], learning_rate=2.5e-05, dropout=0.4, concat_dropout=0.4, bert_dropout=0.2, l2reg=0.0005, num_epoch=20, batch_size=16, log_step=5, max_seq_len=100, polarities_dim=3, device=device(type='cuda'), seed=372, valset_ratio=0.1, do_train=True, do_eval=True, eval_epoch_num=0, fusion_type='concat', use_ensemble=True, save_models='none', print_sent=False, optim='adam', n_gpu=1)\n",
      "{'none': 0, 'acl': 1, 'acl:relcl': 2, 'advcl': 3, 'advcl:relcl': 4, 'advmod': 5, 'amod': 6, 'appos': 7, 'aux': 8, 'aux:pass': 9, 'case': 10, 'cc': 11, 'cc:preconj': 12, 'ccomp': 13, 'compound': 14, 'compound:prt': 15, 'conj': 16, 'cop': 17, 'csubj': 18, 'dep': 19, 'det': 20, 'det:predet': 21, 'discourse': 22, 'dislocated': 23, 'expl': 24, 'fixed': 25, 'flat': 26, 'iobj': 27, 'list': 28, 'mark': 29, 'nmod': 30, 'nmod:npmod': 31, 'nmod:poss': 32, 'nmod:tmod': 33, 'nsubj': 34, 'nsubj:outer': 35, 'nsubj:pass': 36, 'nummod': 37, 'obj': 38, 'obl': 39, 'obl:agent': 40, 'obl:npmod': 41, 'obl:tmod': 42, 'parataxis': 43, 'punct': 44, 'reparandum': 45, 'root': 46, 'vocative': 47, 'xcomp': 48}\n",
      "{'-1': 0, '0': 1, '1': 2}\n",
      "Model name 'bert_large_uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming 'bert_large_uncased' is a path or url to a directory containing tokenizer files.\n",
      "Didn't find file bert_large_uncased/added_tokens.json. We won't load it.\n",
      "Didn't find file bert_large_uncased/special_tokens_map.json. We won't load it.\n",
      "Didn't find file bert_large_uncased/tokenizer_config.json. We won't load it.\n",
      "loading file bert_large_uncased/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "{\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 3,\n",
      "  \"num_types\": 49,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file bert_large_uncased/pytorch_model.bin\n",
      "Weights of AsaTgcnSem not initialized from pretrained model: ['gate_weight', 'gate_bias', 'ensemble', 'TGCNLayers.0.weight', 'TGCNLayers.0.bias', 'TGCNLayers.0.dense.weight', 'TGCNLayers.1.weight', 'TGCNLayers.1.bias', 'TGCNLayers.1.dense.weight', 'SemGCNLayers.0.weight', 'SemGCNLayers.0.bias', 'SemGCNLayers.1.weight', 'SemGCNLayers.1.bias', 'LexGCNLayers.0.weight', 'LexGCNLayers.0.bias', 'LexGCNLayers.1.weight', 'LexGCNLayers.1.bias', 'fc_single.weight', 'fc_single.bias', 'ensemble_linear_tgcn.weight', 'ensemble_linear_tgcn.bias', 'ensemble_linear_semgcn.weight', 'ensemble_linear_semgcn.bias', 'ensemble_linear_lexgcn.weight', 'ensemble_linear_lexgcn.bias', 'dep_embedding.weight']\n",
      "Weights from pretrained model not used in AsaTgcnSem: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "[{'input_ids': tensor([  101, 13325,  2013,  3025,  8466,  2023,  2109,  2000,  2022,  1037,\n",
      "         2204,  2173,  1010,  2021,  2025,  2151,  2936,  1012,   102,  2173,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'valid_ids': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'segment_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'mem_valid_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'dep_adj_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'dep_value_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'polarity': 0, 'raw_text': 'judging from previous posts this used to be a good place , but not any longer .', 'aspect': 'place'}]\n",
      "[{'input_ids': tensor([  101, 13325,  2013,  3025,  8466,  2023,  2109,  2000,  2022,  1037,\n",
      "         2204,  2173,  1010,  2021,  2025,  2151,  2936,  1012,   102,  2173,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'valid_ids': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'segment_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'mem_valid_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'dep_adj_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'dep_value_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'polarity': 0, 'raw_text': 'judging from previous posts this used to be a good place , but not any longer .', 'aspect': 'place'}]\n",
      "[{'input_ids': tensor([  101,  4240,  2428,  2204, 10514,  6182,  1012,   102, 10514,  6182,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'valid_ids': tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'segment_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'mem_valid_ids': tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'dep_adj_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'dep_value_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 5,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'polarity': 2, 'raw_text': 'serves really good sushi .', 'aspect': 'sushi'}]\n",
      "cuda memory allocated: 1400893440\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 0\n",
      "epoch: 0, loss: 1.0208, train acc: 0.5000\n",
      "epoch: 0, loss: 0.9161, train acc: 0.5813\n",
      "epoch: 0, loss: 0.8404, train acc: 0.6333\n",
      "epoch: 0, loss: 0.8469, train acc: 0.6562\n",
      "epoch: 0, loss: 0.8519, train acc: 0.6550\n",
      "epoch: 0, loss: 0.8609, train acc: 0.6500\n",
      "epoch: 0, loss: 0.8319, train acc: 0.6625\n",
      "epoch: 0, loss: 0.8624, train acc: 0.6547\n",
      "epoch: 0, loss: 0.8383, train acc: 0.6625\n",
      "epoch: 0, loss: 0.8244, train acc: 0.6650\n",
      "epoch: 0, loss: 0.8333, train acc: 0.6614\n",
      "epoch: 0, loss: 0.8175, train acc: 0.6677\n",
      "epoch: 0, loss: 0.8099, train acc: 0.6702\n",
      "epoch: 0, loss: 0.8802, train acc: 0.6589\n",
      "epoch: 0, loss: 0.8719, train acc: 0.6608\n",
      "epoch: 0, loss: 0.8816, train acc: 0.6547\n",
      "epoch: 0, loss: 0.8767, train acc: 0.6522\n",
      "epoch: 0, loss: 0.8810, train acc: 0.6514\n",
      "epoch: 0, loss: 0.8739, train acc: 0.6520\n",
      "epoch: 0, loss: 0.8695, train acc: 0.6512\n",
      "epoch: 0, loss: 0.8693, train acc: 0.6518\n",
      ">epoch: 0, val_acc: 0.7181, val_f1: 0.2786\n",
      ">> epoch: 0, test_acc: 0.7431, test_f1: 0.2842\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 1\n",
      "epoch: 1, loss: 0.8412, train acc: 0.6719\n",
      "epoch: 1, loss: 0.8362, train acc: 0.6667\n",
      "epoch: 1, loss: 0.8133, train acc: 0.6429\n",
      "epoch: 1, loss: 0.7849, train acc: 0.6447\n",
      "epoch: 1, loss: 0.7612, train acc: 0.6562\n",
      "epoch: 1, loss: 0.8029, train acc: 0.6530\n",
      "epoch: 1, loss: 0.8042, train acc: 0.6471\n",
      "epoch: 1, loss: 0.8008, train acc: 0.6506\n",
      "epoch: 1, loss: 0.8132, train acc: 0.6506\n",
      "epoch: 1, loss: 0.8129, train acc: 0.6505\n",
      "epoch: 1, loss: 0.8026, train acc: 0.6609\n",
      "epoch: 1, loss: 0.8051, train acc: 0.6653\n",
      "epoch: 1, loss: 0.8157, train acc: 0.6562\n",
      "epoch: 1, loss: 0.8032, train acc: 0.6612\n",
      "epoch: 1, loss: 0.8081, train acc: 0.6537\n",
      "epoch: 1, loss: 0.8047, train acc: 0.6582\n",
      "epoch: 1, loss: 0.7903, train acc: 0.6689\n",
      "epoch: 1, loss: 0.7920, train acc: 0.6721\n",
      "epoch: 1, loss: 0.7927, train acc: 0.6722\n",
      "epoch: 1, loss: 0.7959, train acc: 0.6679\n",
      "epoch: 1, loss: 0.8002, train acc: 0.6617\n",
      ">epoch: 1, val_acc: 0.7181, val_f1: 0.2786\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 2\n",
      "epoch: 2, loss: 0.8084, train acc: 0.6458\n",
      "epoch: 2, loss: 0.7024, train acc: 0.7188\n",
      "epoch: 2, loss: 0.7301, train acc: 0.7212\n",
      "epoch: 2, loss: 0.7492, train acc: 0.7153\n",
      "epoch: 2, loss: 0.7552, train acc: 0.7065\n",
      "epoch: 2, loss: 0.7273, train acc: 0.7143\n",
      "epoch: 2, loss: 0.7258, train acc: 0.7159\n",
      "epoch: 2, loss: 0.7424, train acc: 0.7122\n",
      "epoch: 2, loss: 0.7378, train acc: 0.7137\n",
      "epoch: 2, loss: 0.7311, train acc: 0.7201\n",
      "epoch: 2, loss: 0.7319, train acc: 0.7170\n",
      "epoch: 2, loss: 0.7351, train acc: 0.7155\n",
      "epoch: 2, loss: 0.7326, train acc: 0.7163\n",
      "epoch: 2, loss: 0.7334, train acc: 0.7142\n",
      "epoch: 2, loss: 0.7490, train acc: 0.7080\n",
      "epoch: 2, loss: 0.7558, train acc: 0.7035\n",
      "epoch: 2, loss: 0.7592, train acc: 0.7003\n",
      "epoch: 2, loss: 0.7639, train acc: 0.6967\n",
      "epoch: 2, loss: 0.7606, train acc: 0.6983\n",
      "epoch: 2, loss: 0.7612, train acc: 0.6977\n",
      "epoch: 2, loss: 0.7645, train acc: 0.6972\n",
      ">epoch: 2, val_acc: 0.7181, val_f1: 0.2786\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 3\n",
      "epoch: 3, loss: 0.6608, train acc: 0.6875\n",
      "epoch: 3, loss: 0.7148, train acc: 0.6607\n",
      "epoch: 3, loss: 0.7417, train acc: 0.6719\n",
      "epoch: 3, loss: 0.7230, train acc: 0.7059\n",
      "epoch: 3, loss: 0.7323, train acc: 0.6989\n",
      "epoch: 3, loss: 0.7602, train acc: 0.6852\n",
      "epoch: 3, loss: 0.7438, train acc: 0.6914\n",
      "epoch: 3, loss: 0.7555, train acc: 0.6824\n",
      "epoch: 3, loss: 0.7585, train acc: 0.6801\n",
      "epoch: 3, loss: 0.7463, train acc: 0.6862\n",
      "epoch: 3, loss: 0.7370, train acc: 0.6935\n",
      "epoch: 3, loss: 0.7502, train acc: 0.6908\n",
      "epoch: 3, loss: 0.7497, train acc: 0.6885\n",
      "epoch: 3, loss: 0.7410, train acc: 0.6931\n",
      "epoch: 3, loss: 0.7389, train acc: 0.6953\n",
      "epoch: 3, loss: 0.7416, train acc: 0.6875\n",
      "epoch: 3, loss: 0.7521, train acc: 0.6837\n",
      "epoch: 3, loss: 0.7528, train acc: 0.6810\n",
      "epoch: 3, loss: 0.7498, train acc: 0.6827\n",
      "epoch: 3, loss: 0.7436, train acc: 0.6849\n",
      "epoch: 3, loss: 0.7544, train acc: 0.6826\n",
      ">epoch: 3, val_acc: 0.7181, val_f1: 0.2786\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 4\n",
      "epoch: 4, loss: 0.5026, train acc: 0.7500\n",
      "epoch: 4, loss: 0.7300, train acc: 0.6562\n",
      "epoch: 4, loss: 0.7141, train acc: 0.6818\n",
      "epoch: 4, loss: 0.7243, train acc: 0.6719\n",
      "epoch: 4, loss: 0.7461, train acc: 0.6786\n",
      "epoch: 4, loss: 0.7518, train acc: 0.6659\n",
      "epoch: 4, loss: 0.7616, train acc: 0.6573\n",
      "epoch: 4, loss: 0.7602, train acc: 0.6597\n",
      "epoch: 4, loss: 0.7667, train acc: 0.6555\n",
      "epoch: 4, loss: 0.7565, train acc: 0.6630\n",
      "epoch: 4, loss: 0.7653, train acc: 0.6618\n",
      "epoch: 4, loss: 0.7664, train acc: 0.6663\n",
      "epoch: 4, loss: 0.7639, train acc: 0.6680\n",
      "epoch: 4, loss: 0.7648, train acc: 0.6648\n",
      "epoch: 4, loss: 0.7567, train acc: 0.6725\n",
      "epoch: 4, loss: 0.7581, train acc: 0.6752\n",
      "epoch: 4, loss: 0.7650, train acc: 0.6713\n",
      "epoch: 4, loss: 0.7570, train acc: 0.6759\n",
      "epoch: 4, loss: 0.7565, train acc: 0.6738\n",
      "epoch: 4, loss: 0.7622, train acc: 0.6712\n",
      "epoch: 4, loss: 0.7599, train acc: 0.6696\n",
      "epoch: 4, loss: 0.7672, train acc: 0.6678\n",
      ">epoch: 4, val_acc: 0.7181, val_f1: 0.2786\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 5\n",
      "epoch: 5, loss: 0.6819, train acc: 0.6750\n",
      "epoch: 5, loss: 0.6785, train acc: 0.6875\n",
      "epoch: 5, loss: 0.6805, train acc: 0.7042\n",
      "epoch: 5, loss: 0.7210, train acc: 0.6906\n",
      "epoch: 5, loss: 0.7566, train acc: 0.6675\n",
      "epoch: 5, loss: 0.7601, train acc: 0.6813\n",
      "epoch: 5, loss: 0.7761, train acc: 0.6839\n",
      "epoch: 5, loss: 0.7629, train acc: 0.6875\n",
      "epoch: 5, loss: 0.7672, train acc: 0.6847\n",
      "epoch: 5, loss: 0.7745, train acc: 0.6863\n",
      "epoch: 5, loss: 0.7617, train acc: 0.6932\n",
      "epoch: 5, loss: 0.7491, train acc: 0.7010\n",
      "epoch: 5, loss: 0.7539, train acc: 0.7000\n",
      "epoch: 5, loss: 0.7560, train acc: 0.6982\n",
      "epoch: 5, loss: 0.7573, train acc: 0.6942\n",
      "epoch: 5, loss: 0.7555, train acc: 0.6898\n",
      "epoch: 5, loss: 0.7546, train acc: 0.6846\n",
      "epoch: 5, loss: 0.7606, train acc: 0.6826\n",
      "epoch: 5, loss: 0.7599, train acc: 0.6842\n",
      "epoch: 5, loss: 0.7569, train acc: 0.6863\n",
      "epoch: 5, loss: 0.7558, train acc: 0.6863\n",
      ">epoch: 5, val_acc: 0.7181, val_f1: 0.2786\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 6\n",
      "epoch: 6, loss: 0.8391, train acc: 0.6875\n",
      "epoch: 6, loss: 0.8547, train acc: 0.6528\n",
      "epoch: 6, loss: 0.9026, train acc: 0.6161\n",
      "epoch: 6, loss: 0.8900, train acc: 0.6184\n",
      "epoch: 6, loss: 0.8799, train acc: 0.6198\n",
      "epoch: 6, loss: 0.8617, train acc: 0.6272\n",
      "epoch: 6, loss: 0.8308, train acc: 0.6489\n",
      "epoch: 6, loss: 0.7907, train acc: 0.6731\n",
      "epoch: 6, loss: 0.7722, train acc: 0.6861\n",
      "epoch: 6, loss: 0.7674, train acc: 0.6862\n",
      "epoch: 6, loss: 0.7671, train acc: 0.6840\n",
      "epoch: 6, loss: 0.7680, train acc: 0.6811\n",
      "epoch: 6, loss: 0.7647, train acc: 0.6777\n",
      "epoch: 6, loss: 0.7662, train acc: 0.6739\n",
      "epoch: 6, loss: 0.7690, train acc: 0.6774\n",
      "epoch: 6, loss: 0.7689, train acc: 0.6788\n",
      "epoch: 6, loss: 0.7672, train acc: 0.6793\n",
      "epoch: 6, loss: 0.7685, train acc: 0.6784\n",
      "epoch: 6, loss: 0.7614, train acc: 0.6828\n",
      "epoch: 6, loss: 0.7633, train acc: 0.6818\n",
      "epoch: 6, loss: 0.7655, train acc: 0.6797\n",
      ">epoch: 6, val_acc: 0.7181, val_f1: 0.2786\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 7\n",
      "epoch: 7, loss: 0.7113, train acc: 0.6875\n",
      "epoch: 7, loss: 0.7656, train acc: 0.6562\n",
      "epoch: 7, loss: 0.7815, train acc: 0.6635\n",
      "epoch: 7, loss: 0.8033, train acc: 0.6528\n",
      "epoch: 7, loss: 0.7832, train acc: 0.6549\n",
      "epoch: 7, loss: 0.7654, train acc: 0.6674\n",
      "epoch: 7, loss: 0.7613, train acc: 0.6686\n",
      "epoch: 7, loss: 0.7398, train acc: 0.6743\n",
      "epoch: 7, loss: 0.7307, train acc: 0.6802\n",
      "epoch: 7, loss: 0.7592, train acc: 0.6810\n",
      "epoch: 7, loss: 0.7599, train acc: 0.6804\n",
      "epoch: 7, loss: 0.7611, train acc: 0.6875\n",
      "epoch: 7, loss: 0.7634, train acc: 0.6875\n",
      "epoch: 7, loss: 0.7529, train acc: 0.6930\n",
      "epoch: 7, loss: 0.7480, train acc: 0.6969\n",
      "epoch: 7, loss: 0.7604, train acc: 0.6915\n",
      "epoch: 7, loss: 0.7593, train acc: 0.6890\n",
      "epoch: 7, loss: 0.7623, train acc: 0.6839\n",
      "epoch: 7, loss: 0.7634, train acc: 0.6835\n",
      "epoch: 7, loss: 0.7598, train acc: 0.6862\n",
      "epoch: 7, loss: 0.7561, train acc: 0.6881\n",
      ">epoch: 7, val_acc: 0.7181, val_f1: 0.2786\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 8\n",
      "epoch: 8, loss: 0.6330, train acc: 0.8125\n",
      "epoch: 8, loss: 0.7103, train acc: 0.7411\n",
      "epoch: 8, loss: 0.7591, train acc: 0.7240\n",
      "epoch: 8, loss: 0.8323, train acc: 0.6838\n",
      "epoch: 8, loss: 0.8206, train acc: 0.6733\n",
      "epoch: 8, loss: 0.7918, train acc: 0.6968\n",
      "epoch: 8, loss: 0.7856, train acc: 0.7012\n",
      "epoch: 8, loss: 0.7755, train acc: 0.7027\n",
      "epoch: 8, loss: 0.7552, train acc: 0.7113\n",
      "epoch: 8, loss: 0.7522, train acc: 0.7154\n",
      "epoch: 8, loss: 0.7489, train acc: 0.7127\n",
      "epoch: 8, loss: 0.7604, train acc: 0.7039\n",
      "epoch: 8, loss: 0.7601, train acc: 0.7036\n",
      "epoch: 8, loss: 0.7469, train acc: 0.7062\n",
      "epoch: 8, loss: 0.7433, train acc: 0.7031\n",
      "epoch: 8, loss: 0.7439, train acc: 0.7005\n",
      "epoch: 8, loss: 0.7435, train acc: 0.6982\n",
      "epoch: 8, loss: 0.7443, train acc: 0.6954\n",
      "epoch: 8, loss: 0.7553, train acc: 0.6916\n",
      "epoch: 8, loss: 0.7546, train acc: 0.6933\n",
      "epoch: 8, loss: 0.7532, train acc: 0.6912\n",
      ">epoch: 8, val_acc: 0.7181, val_f1: 0.2786\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 9\n",
      "epoch: 9, loss: 0.7509, train acc: 0.7500\n",
      "epoch: 9, loss: 0.8072, train acc: 0.7083\n",
      "epoch: 9, loss: 0.7675, train acc: 0.7045\n",
      "epoch: 9, loss: 0.7339, train acc: 0.7148\n",
      "epoch: 9, loss: 0.7442, train acc: 0.7054\n",
      "epoch: 9, loss: 0.7758, train acc: 0.6995\n",
      "epoch: 9, loss: 0.7804, train acc: 0.6996\n",
      "epoch: 9, loss: 0.7830, train acc: 0.6892\n",
      "epoch: 9, loss: 0.7731, train acc: 0.6875\n",
      "epoch: 9, loss: 0.7734, train acc: 0.6793\n",
      "epoch: 9, loss: 0.7724, train acc: 0.6801\n",
      "epoch: 9, loss: 0.7759, train acc: 0.6763\n",
      "epoch: 9, loss: 0.7586, train acc: 0.6865\n",
      "epoch: 9, loss: 0.7650, train acc: 0.6809\n",
      "epoch: 9, loss: 0.7562, train acc: 0.6831\n",
      "epoch: 9, loss: 0.7548, train acc: 0.6850\n",
      "epoch: 9, loss: 0.7445, train acc: 0.6906\n",
      "epoch: 9, loss: 0.7512, train acc: 0.6882\n",
      "epoch: 9, loss: 0.7479, train acc: 0.6902\n",
      "epoch: 9, loss: 0.7451, train acc: 0.6895\n",
      "epoch: 9, loss: 0.7467, train acc: 0.6900\n",
      "epoch: 9, loss: 0.7442, train acc: 0.6927\n",
      ">epoch: 9, val_acc: 0.7181, val_f1: 0.2786\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 10\n",
      "epoch: 10, loss: 0.6702, train acc: 0.7500\n",
      "epoch: 10, loss: 0.6180, train acc: 0.7812\n",
      "epoch: 10, loss: 0.6889, train acc: 0.7375\n",
      "epoch: 10, loss: 0.6510, train acc: 0.7562\n",
      "epoch: 10, loss: 0.6791, train acc: 0.7250\n",
      "epoch: 10, loss: 0.6543, train acc: 0.7354\n",
      "epoch: 10, loss: 0.6490, train acc: 0.7321\n",
      "epoch: 10, loss: 0.6523, train acc: 0.7359\n",
      "epoch: 10, loss: 0.6487, train acc: 0.7444\n",
      "epoch: 10, loss: 0.6577, train acc: 0.7350\n",
      "epoch: 10, loss: 0.6749, train acc: 0.7261\n",
      "epoch: 10, loss: 0.6924, train acc: 0.7229\n",
      "epoch: 10, loss: 0.7041, train acc: 0.7173\n",
      "epoch: 10, loss: 0.7104, train acc: 0.7089\n",
      "epoch: 10, loss: 0.7236, train acc: 0.7025\n",
      "epoch: 10, loss: 0.7285, train acc: 0.7008\n",
      "epoch: 10, loss: 0.7302, train acc: 0.6993\n",
      "epoch: 10, loss: 0.7307, train acc: 0.6958\n",
      "epoch: 10, loss: 0.7313, train acc: 0.6947\n",
      "epoch: 10, loss: 0.7400, train acc: 0.6919\n",
      "epoch: 10, loss: 0.7374, train acc: 0.6935\n",
      ">epoch: 10, val_acc: 0.7181, val_f1: 0.2786\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 11\n",
      "epoch: 11, loss: 0.8594, train acc: 0.6406\n",
      "epoch: 11, loss: 0.8907, train acc: 0.6181\n",
      "epoch: 11, loss: 0.8154, train acc: 0.6473\n",
      "epoch: 11, loss: 0.7918, train acc: 0.6743\n",
      "epoch: 11, loss: 0.7989, train acc: 0.6641\n",
      "epoch: 11, loss: 0.7923, train acc: 0.6724\n",
      "epoch: 11, loss: 0.7984, train acc: 0.6728\n",
      "epoch: 11, loss: 0.8135, train acc: 0.6635\n",
      "epoch: 11, loss: 0.7989, train acc: 0.6705\n",
      "epoch: 11, loss: 0.7862, train acc: 0.6709\n",
      "epoch: 11, loss: 0.7767, train acc: 0.6725\n",
      "epoch: 11, loss: 0.7678, train acc: 0.6780\n",
      "epoch: 11, loss: 0.7583, train acc: 0.6826\n",
      "epoch: 11, loss: 0.7594, train acc: 0.6812\n",
      "epoch: 11, loss: 0.7532, train acc: 0.6892\n",
      "epoch: 11, loss: 0.7597, train acc: 0.6843\n",
      "epoch: 11, loss: 0.7581, train acc: 0.6868\n",
      "epoch: 11, loss: 0.7570, train acc: 0.6861\n",
      "epoch: 11, loss: 0.7593, train acc: 0.6822\n",
      "epoch: 11, loss: 0.7597, train acc: 0.6824\n",
      "epoch: 11, loss: 0.7485, train acc: 0.6905\n",
      ">epoch: 11, val_acc: 0.7181, val_f1: 0.2786\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 12\n",
      "epoch: 12, loss: 0.6977, train acc: 0.7917\n",
      "epoch: 12, loss: 0.7872, train acc: 0.7109\n",
      "epoch: 12, loss: 0.7724, train acc: 0.7260\n",
      "epoch: 12, loss: 0.8107, train acc: 0.6875\n",
      "epoch: 12, loss: 0.8094, train acc: 0.6685\n",
      "epoch: 12, loss: 0.8034, train acc: 0.6719\n",
      "epoch: 12, loss: 0.7936, train acc: 0.6705\n",
      "epoch: 12, loss: 0.7883, train acc: 0.6694\n",
      "epoch: 12, loss: 0.7748, train acc: 0.6730\n",
      "epoch: 12, loss: 0.7706, train acc: 0.6758\n",
      "epoch: 12, loss: 0.7802, train acc: 0.6722\n",
      "epoch: 12, loss: 0.7642, train acc: 0.6778\n",
      "epoch: 12, loss: 0.7679, train acc: 0.6726\n",
      "epoch: 12, loss: 0.7614, train acc: 0.6756\n",
      "epoch: 12, loss: 0.7611, train acc: 0.6747\n",
      "epoch: 12, loss: 0.7536, train acc: 0.6763\n",
      "epoch: 12, loss: 0.7464, train acc: 0.6822\n",
      "epoch: 12, loss: 0.7432, train acc: 0.6832\n",
      "epoch: 12, loss: 0.7468, train acc: 0.6835\n",
      "epoch: 12, loss: 0.7429, train acc: 0.6862\n",
      "epoch: 12, loss: 0.7443, train acc: 0.6887\n",
      ">epoch: 12, val_acc: 0.7181, val_f1: 0.2786\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 13\n",
      "epoch: 13, loss: 0.9543, train acc: 0.6875\n",
      "epoch: 13, loss: 0.7321, train acc: 0.7411\n",
      "epoch: 13, loss: 0.7336, train acc: 0.7240\n",
      "epoch: 13, loss: 0.7801, train acc: 0.6912\n",
      "epoch: 13, loss: 0.7426, train acc: 0.7102\n",
      "epoch: 13, loss: 0.7269, train acc: 0.7199\n",
      "epoch: 13, loss: 0.7355, train acc: 0.7168\n",
      "epoch: 13, loss: 0.7612, train acc: 0.6976\n",
      "epoch: 13, loss: 0.7610, train acc: 0.6964\n",
      "epoch: 13, loss: 0.7626, train acc: 0.6955\n",
      "epoch: 13, loss: 0.7528, train acc: 0.7007\n",
      "epoch: 13, loss: 0.7568, train acc: 0.6952\n",
      "epoch: 13, loss: 0.7517, train acc: 0.6976\n",
      "epoch: 13, loss: 0.7436, train acc: 0.7006\n",
      "epoch: 13, loss: 0.7398, train acc: 0.7023\n",
      "epoch: 13, loss: 0.7514, train acc: 0.6972\n",
      "epoch: 13, loss: 0.7484, train acc: 0.6989\n",
      "epoch: 13, loss: 0.7489, train acc: 0.6976\n",
      "epoch: 13, loss: 0.7504, train acc: 0.6977\n",
      "epoch: 13, loss: 0.7468, train acc: 0.6978\n",
      "epoch: 13, loss: 0.7459, train acc: 0.6967\n",
      ">epoch: 13, val_acc: 0.7181, val_f1: 0.2786\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 14\n",
      "epoch: 14, loss: 0.7586, train acc: 0.6250\n",
      "epoch: 14, loss: 0.8044, train acc: 0.6042\n",
      "epoch: 14, loss: 0.7253, train acc: 0.6591\n",
      "epoch: 14, loss: 0.7641, train acc: 0.6445\n",
      "epoch: 14, loss: 0.7871, train acc: 0.6399\n",
      "epoch: 14, loss: 0.7801, train acc: 0.6538\n",
      "epoch: 14, loss: 0.7866, train acc: 0.6452\n",
      "epoch: 14, loss: 0.7930, train acc: 0.6424\n",
      "epoch: 14, loss: 0.7886, train acc: 0.6479\n",
      "epoch: 14, loss: 0.7786, train acc: 0.6630\n",
      "epoch: 14, loss: 0.7851, train acc: 0.6630\n",
      "epoch: 14, loss: 0.7803, train acc: 0.6652\n",
      "epoch: 14, loss: 0.7768, train acc: 0.6670\n",
      "epoch: 14, loss: 0.7713, train acc: 0.6686\n",
      "epoch: 14, loss: 0.7652, train acc: 0.6699\n",
      "epoch: 14, loss: 0.7625, train acc: 0.6735\n",
      "epoch: 14, loss: 0.7608, train acc: 0.6767\n",
      "epoch: 14, loss: 0.7566, train acc: 0.6810\n",
      "epoch: 14, loss: 0.7493, train acc: 0.6848\n",
      "epoch: 14, loss: 0.7529, train acc: 0.6849\n",
      "epoch: 14, loss: 0.7494, train acc: 0.6894\n",
      "epoch: 14, loss: 0.7451, train acc: 0.6921\n",
      ">epoch: 14, val_acc: 0.7181, val_f1: 0.2786\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 15\n",
      "epoch: 15, loss: 0.7374, train acc: 0.6875\n",
      "epoch: 15, loss: 0.7629, train acc: 0.7000\n",
      "epoch: 15, loss: 0.7751, train acc: 0.6917\n",
      "epoch: 15, loss: 0.7620, train acc: 0.6844\n",
      "epoch: 15, loss: 0.7393, train acc: 0.6950\n",
      "epoch: 15, loss: 0.7278, train acc: 0.7000\n",
      "epoch: 15, loss: 0.7295, train acc: 0.7018\n",
      "epoch: 15, loss: 0.7234, train acc: 0.7016\n",
      "epoch: 15, loss: 0.7203, train acc: 0.7042\n",
      "epoch: 15, loss: 0.7087, train acc: 0.7113\n",
      "epoch: 15, loss: 0.7065, train acc: 0.7102\n",
      "epoch: 15, loss: 0.7094, train acc: 0.7083\n",
      "epoch: 15, loss: 0.7116, train acc: 0.7096\n",
      "epoch: 15, loss: 0.7325, train acc: 0.6991\n",
      "epoch: 15, loss: 0.7422, train acc: 0.6933\n",
      "epoch: 15, loss: 0.7378, train acc: 0.6937\n",
      "epoch: 15, loss: 0.7377, train acc: 0.6941\n",
      "epoch: 15, loss: 0.7387, train acc: 0.6903\n",
      "epoch: 15, loss: 0.7388, train acc: 0.6908\n",
      "epoch: 15, loss: 0.7383, train acc: 0.6887\n",
      "epoch: 15, loss: 0.7362, train acc: 0.6911\n",
      ">epoch: 15, val_acc: 0.7181, val_f1: 0.2786\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 16\n",
      "epoch: 16, loss: 0.7658, train acc: 0.6875\n",
      "epoch: 16, loss: 0.7559, train acc: 0.7153\n",
      "epoch: 16, loss: 0.7947, train acc: 0.7009\n",
      "epoch: 16, loss: 0.7786, train acc: 0.6875\n",
      "epoch: 16, loss: 0.7483, train acc: 0.7109\n",
      "epoch: 16, loss: 0.7485, train acc: 0.6940\n",
      "epoch: 16, loss: 0.7576, train acc: 0.6875\n",
      "epoch: 16, loss: 0.7725, train acc: 0.6843\n",
      "epoch: 16, loss: 0.7719, train acc: 0.6889\n",
      "epoch: 16, loss: 0.7651, train acc: 0.6913\n",
      "epoch: 16, loss: 0.7654, train acc: 0.6875\n",
      "epoch: 16, loss: 0.7669, train acc: 0.6822\n",
      "epoch: 16, loss: 0.7657, train acc: 0.6816\n",
      "epoch: 16, loss: 0.7606, train acc: 0.6821\n",
      "epoch: 16, loss: 0.7636, train acc: 0.6807\n",
      "epoch: 16, loss: 0.7615, train acc: 0.6812\n",
      "epoch: 16, loss: 0.7594, train acc: 0.6830\n",
      "epoch: 16, loss: 0.7569, train acc: 0.6854\n",
      "epoch: 16, loss: 0.7575, train acc: 0.6855\n",
      "epoch: 16, loss: 0.7511, train acc: 0.6869\n",
      "epoch: 16, loss: 0.7436, train acc: 0.6923\n",
      ">epoch: 16, val_acc: 0.7181, val_f1: 0.2786\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 17\n",
      "epoch: 17, loss: 0.5847, train acc: 0.7500\n",
      "epoch: 17, loss: 0.7608, train acc: 0.6719\n",
      "epoch: 17, loss: 0.7900, train acc: 0.6635\n",
      "epoch: 17, loss: 0.7651, train acc: 0.6806\n",
      "epoch: 17, loss: 0.7438, train acc: 0.6957\n",
      "epoch: 17, loss: 0.7415, train acc: 0.6987\n",
      "epoch: 17, loss: 0.7276, train acc: 0.7064\n",
      "epoch: 17, loss: 0.7358, train acc: 0.6957\n",
      "epoch: 17, loss: 0.7429, train acc: 0.6962\n",
      "epoch: 17, loss: 0.7469, train acc: 0.6914\n",
      "epoch: 17, loss: 0.7429, train acc: 0.6922\n",
      "epoch: 17, loss: 0.7487, train acc: 0.6886\n",
      "epoch: 17, loss: 0.7450, train acc: 0.6875\n",
      "epoch: 17, loss: 0.7462, train acc: 0.6838\n",
      "epoch: 17, loss: 0.7405, train acc: 0.6832\n",
      "epoch: 17, loss: 0.7350, train acc: 0.6867\n",
      "epoch: 17, loss: 0.7308, train acc: 0.6898\n",
      "epoch: 17, loss: 0.7410, train acc: 0.6847\n",
      "epoch: 17, loss: 0.7419, train acc: 0.6875\n",
      "epoch: 17, loss: 0.7385, train acc: 0.6907\n",
      "epoch: 17, loss: 0.7342, train acc: 0.6936\n",
      ">epoch: 17, val_acc: 0.7181, val_f1: 0.2786\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 18\n",
      "epoch: 18, loss: 0.8156, train acc: 0.6250\n",
      "epoch: 18, loss: 0.7044, train acc: 0.7232\n",
      "epoch: 18, loss: 0.7037, train acc: 0.7292\n",
      "epoch: 18, loss: 0.7354, train acc: 0.7243\n",
      "epoch: 18, loss: 0.7198, train acc: 0.7330\n",
      "epoch: 18, loss: 0.7038, train acc: 0.7384\n",
      "epoch: 18, loss: 0.7081, train acc: 0.7305\n",
      "epoch: 18, loss: 0.7044, train acc: 0.7297\n",
      "epoch: 18, loss: 0.7147, train acc: 0.7188\n",
      "epoch: 18, loss: 0.7203, train acc: 0.7061\n",
      "epoch: 18, loss: 0.7119, train acc: 0.7115\n",
      "epoch: 18, loss: 0.7098, train acc: 0.7127\n",
      "epoch: 18, loss: 0.7163, train acc: 0.7077\n",
      "epoch: 18, loss: 0.7133, train acc: 0.7108\n",
      "epoch: 18, loss: 0.7268, train acc: 0.7023\n",
      "epoch: 18, loss: 0.7303, train acc: 0.7005\n",
      "epoch: 18, loss: 0.7303, train acc: 0.6966\n",
      "epoch: 18, loss: 0.7376, train acc: 0.6932\n",
      "epoch: 18, loss: 0.7421, train acc: 0.6909\n",
      "epoch: 18, loss: 0.7384, train acc: 0.6920\n",
      "epoch: 18, loss: 0.7376, train acc: 0.6930\n",
      ">epoch: 18, val_acc: 0.7181, val_f1: 0.2786\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 19\n",
      "epoch: 19, loss: 0.6219, train acc: 0.8750\n",
      "epoch: 19, loss: 0.7594, train acc: 0.6979\n",
      "epoch: 19, loss: 0.7001, train acc: 0.7102\n",
      "epoch: 19, loss: 0.6752, train acc: 0.7305\n",
      "epoch: 19, loss: 0.7041, train acc: 0.7083\n",
      "epoch: 19, loss: 0.7378, train acc: 0.6995\n",
      "epoch: 19, loss: 0.7452, train acc: 0.6935\n",
      "epoch: 19, loss: 0.7425, train acc: 0.6979\n",
      "epoch: 19, loss: 0.7548, train acc: 0.6875\n",
      "epoch: 19, loss: 0.7472, train acc: 0.6916\n",
      "epoch: 19, loss: 0.7451, train acc: 0.6912\n",
      "epoch: 19, loss: 0.7529, train acc: 0.6897\n",
      "epoch: 19, loss: 0.7540, train acc: 0.6885\n",
      "epoch: 19, loss: 0.7557, train acc: 0.6884\n",
      "epoch: 19, loss: 0.7561, train acc: 0.6875\n",
      "epoch: 19, loss: 0.7558, train acc: 0.6859\n",
      "epoch: 19, loss: 0.7468, train acc: 0.6906\n",
      "epoch: 19, loss: 0.7383, train acc: 0.6948\n",
      "epoch: 19, loss: 0.7399, train acc: 0.6957\n",
      "epoch: 19, loss: 0.7390, train acc: 0.6953\n",
      "epoch: 19, loss: 0.7378, train acc: 0.6943\n",
      "epoch: 19, loss: 0.7410, train acc: 0.6944\n",
      ">epoch: 19, val_acc: 0.7181, val_f1: 0.2786\n",
      "Namespace(model_type='tri_gcn', modules={'tgcn': True, 'semgcn': True, 'lexgcn': True}, num_layers={'tgcn': 2, 'semgcn': 2, 'lexgcn': 2}, year='2016', train_file='data/train2016restaurant.txt', test_file='data/test2016restaurant.txt', model_path='test_models/2016tri_gcn+concat_seed255_reg0.0005_drop0.4_cdrop0.4_lr2.5e-05_epochs20_adam', val_file='val.txt', log='log', bert_model='bert_large_uncased', cooc_path='cooc_matrix.csv', cooc=       100       1000   10000  10003     10005     10007     10009     1001   \\\n",
      "100      0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "1000     0.0  0.000000    0.0    0.0  0.000606  0.000303  0.000606  0.035120   \n",
      "10000    0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.333333   \n",
      "10003    0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "10005    0.0  0.333333    0.0    0.0  0.000000  0.166667  0.000000  0.166667   \n",
      "...      ...       ...    ...    ...       ...       ...       ...       ...   \n",
      "9992     0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "9993     0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "9996     0.0  0.500000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "9997     0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "9998     0.0  0.400000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "       10010  10015  ...     9986      9987      999    9990      9991   \\\n",
      "100      0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "1000     0.0    0.0  ...  0.002422  0.000303  0.062368    0.0  0.000606   \n",
      "10000    0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "10003    0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "10005    0.0    0.0  ...  0.000000  0.000000  0.333333    0.0  0.000000   \n",
      "...      ...    ...  ...       ...       ...       ...    ...       ...   \n",
      "9992     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "9993     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "9996     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "9997     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "9998     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "\n",
      "       9992   9993      9996   9997      9998   \n",
      "100      0.0    0.0  0.000000    0.0  0.000000  \n",
      "1000     0.0    0.0  0.000606    0.0  0.001211  \n",
      "10000    0.0    0.0  0.000000    0.0  0.000000  \n",
      "10003    0.0    0.0  0.000000    0.0  0.000000  \n",
      "10005    0.0    0.0  0.000000    0.0  0.000000  \n",
      "...      ...    ...       ...    ...       ...  \n",
      "9992     0.0    0.0  0.000000    0.0  0.000000  \n",
      "9993     0.0    0.0  0.000000    0.0  0.000000  \n",
      "9996     0.0    0.0  0.000000    0.0  0.000000  \n",
      "9997     0.0    0.0  0.000000    0.0  0.000000  \n",
      "9998     0.0    0.0  0.000000    0.0  0.000000  \n",
      "\n",
      "[13075 rows x 13075 columns], learning_rate=2.5e-05, dropout=0.4, concat_dropout=0.4, bert_dropout=0.2, l2reg=0.0005, num_epoch=20, batch_size=16, log_step=5, max_seq_len=100, polarities_dim=3, device=device(type='cuda'), seed=255, valset_ratio=0.1, do_train=True, do_eval=True, eval_epoch_num=0, fusion_type='concat', use_ensemble=True, save_models='none', print_sent=False, optim='adam', n_gpu=1)\n",
      "{'none': 0, 'acl': 1, 'acl:relcl': 2, 'advcl': 3, 'advcl:relcl': 4, 'advmod': 5, 'amod': 6, 'appos': 7, 'aux': 8, 'aux:pass': 9, 'case': 10, 'cc': 11, 'cc:preconj': 12, 'ccomp': 13, 'compound': 14, 'compound:prt': 15, 'conj': 16, 'cop': 17, 'csubj': 18, 'dep': 19, 'det': 20, 'det:predet': 21, 'discourse': 22, 'dislocated': 23, 'expl': 24, 'fixed': 25, 'flat': 26, 'iobj': 27, 'list': 28, 'mark': 29, 'nmod': 30, 'nmod:npmod': 31, 'nmod:poss': 32, 'nmod:tmod': 33, 'nsubj': 34, 'nsubj:outer': 35, 'nsubj:pass': 36, 'nummod': 37, 'obj': 38, 'obl': 39, 'obl:agent': 40, 'obl:npmod': 41, 'obl:tmod': 42, 'parataxis': 43, 'punct': 44, 'reparandum': 45, 'root': 46, 'vocative': 47, 'xcomp': 48}\n",
      "{'-1': 0, '0': 1, '1': 2}\n",
      "Model name 'bert_large_uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming 'bert_large_uncased' is a path or url to a directory containing tokenizer files.\n",
      "Didn't find file bert_large_uncased/added_tokens.json. We won't load it.\n",
      "Didn't find file bert_large_uncased/special_tokens_map.json. We won't load it.\n",
      "Didn't find file bert_large_uncased/tokenizer_config.json. We won't load it.\n",
      "loading file bert_large_uncased/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "{\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 3,\n",
      "  \"num_types\": 49,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file bert_large_uncased/pytorch_model.bin\n",
      "Weights of AsaTgcnSem not initialized from pretrained model: ['gate_weight', 'gate_bias', 'ensemble', 'TGCNLayers.0.weight', 'TGCNLayers.0.bias', 'TGCNLayers.0.dense.weight', 'TGCNLayers.1.weight', 'TGCNLayers.1.bias', 'TGCNLayers.1.dense.weight', 'SemGCNLayers.0.weight', 'SemGCNLayers.0.bias', 'SemGCNLayers.1.weight', 'SemGCNLayers.1.bias', 'LexGCNLayers.0.weight', 'LexGCNLayers.0.bias', 'LexGCNLayers.1.weight', 'LexGCNLayers.1.bias', 'fc_single.weight', 'fc_single.bias', 'ensemble_linear_tgcn.weight', 'ensemble_linear_tgcn.bias', 'ensemble_linear_semgcn.weight', 'ensemble_linear_semgcn.bias', 'ensemble_linear_lexgcn.weight', 'ensemble_linear_lexgcn.bias', 'dep_embedding.weight']\n",
      "Weights from pretrained model not used in AsaTgcnSem: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "[{'input_ids': tensor([  101, 13325,  2013,  3025,  8466,  2023,  2109,  2000,  2022,  1037,\n",
      "         2204,  2173,  1010,  2021,  2025,  2151,  2936,  1012,   102,  2173,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'valid_ids': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'segment_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'mem_valid_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'dep_adj_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'dep_value_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'polarity': 0, 'raw_text': 'judging from previous posts this used to be a good place , but not any longer .', 'aspect': 'place'}]\n",
      "[{'input_ids': tensor([  101, 13325,  2013,  3025,  8466,  2023,  2109,  2000,  2022,  1037,\n",
      "         2204,  2173,  1010,  2021,  2025,  2151,  2936,  1012,   102,  2173,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'valid_ids': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'segment_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'mem_valid_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'dep_adj_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'dep_value_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'polarity': 0, 'raw_text': 'judging from previous posts this used to be a good place , but not any longer .', 'aspect': 'place'}]\n",
      "[{'input_ids': tensor([  101,  4240,  2428,  2204, 10514,  6182,  1012,   102, 10514,  6182,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'valid_ids': tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'segment_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'mem_valid_ids': tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'dep_adj_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'dep_value_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 5,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'polarity': 2, 'raw_text': 'serves really good sushi .', 'aspect': 'sushi'}]\n",
      "cuda memory allocated: 1400893440\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 0\n",
      "epoch: 0, loss: 1.0484, train acc: 0.5625\n",
      "epoch: 0, loss: 1.0285, train acc: 0.6188\n",
      "epoch: 0, loss: 0.9894, train acc: 0.6167\n",
      "epoch: 0, loss: 0.9301, train acc: 0.6156\n",
      "epoch: 0, loss: 0.8608, train acc: 0.6500\n",
      "epoch: 0, loss: 0.9686, train acc: 0.6396\n",
      "epoch: 0, loss: 1.0816, train acc: 0.6071\n",
      "epoch: 0, loss: 1.0821, train acc: 0.6094\n",
      "epoch: 0, loss: 1.0669, train acc: 0.6236\n",
      "epoch: 0, loss: 1.0474, train acc: 0.6325\n",
      "epoch: 0, loss: 1.0515, train acc: 0.6273\n",
      "epoch: 0, loss: 1.0675, train acc: 0.6177\n",
      "epoch: 0, loss: 1.0664, train acc: 0.6087\n",
      "epoch: 0, loss: 1.0617, train acc: 0.6089\n",
      "epoch: 0, loss: 1.0648, train acc: 0.6075\n",
      "epoch: 0, loss: 1.0648, train acc: 0.6094\n",
      "epoch: 0, loss: 1.0521, train acc: 0.6096\n",
      "epoch: 0, loss: 1.0353, train acc: 0.6125\n",
      "epoch: 0, loss: 1.0338, train acc: 0.6105\n",
      "epoch: 0, loss: 1.0328, train acc: 0.6081\n",
      "epoch: 0, loss: 1.0240, train acc: 0.6048\n",
      ">epoch: 0, val_acc: 0.7181, val_f1: 0.2786\n",
      ">> epoch: 0, test_acc: 0.7431, test_f1: 0.2842\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 1\n",
      "epoch: 1, loss: 0.9440, train acc: 0.5781\n",
      "epoch: 1, loss: 1.1214, train acc: 0.5347\n",
      "epoch: 1, loss: 1.0577, train acc: 0.5357\n",
      "epoch: 1, loss: 1.0272, train acc: 0.5362\n",
      "epoch: 1, loss: 0.9502, train acc: 0.5755\n",
      "epoch: 1, loss: 0.9646, train acc: 0.5733\n",
      "epoch: 1, loss: 0.9437, train acc: 0.5735\n",
      "epoch: 1, loss: 0.9297, train acc: 0.5881\n",
      "epoch: 1, loss: 0.9479, train acc: 0.5895\n",
      "epoch: 1, loss: 0.9366, train acc: 0.5931\n",
      "epoch: 1, loss: 0.9236, train acc: 0.5938\n",
      "epoch: 1, loss: 0.9189, train acc: 0.5911\n",
      "epoch: 1, loss: 0.9209, train acc: 0.5918\n",
      "epoch: 1, loss: 0.9132, train acc: 0.5987\n",
      "epoch: 1, loss: 0.9121, train acc: 0.6030\n",
      "epoch: 1, loss: 0.9084, train acc: 0.6060\n",
      "epoch: 1, loss: 0.8970, train acc: 0.6131\n",
      "epoch: 1, loss: 0.8911, train acc: 0.6180\n",
      "epoch: 1, loss: 0.8827, train acc: 0.6230\n",
      "epoch: 1, loss: 0.8866, train acc: 0.6218\n",
      "epoch: 1, loss: 0.9010, train acc: 0.6166\n",
      ">epoch: 1, val_acc: 0.7181, val_f1: 0.2786\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 2\n",
      "epoch: 2, loss: 0.8467, train acc: 0.5625\n",
      "epoch: 2, loss: 0.7868, train acc: 0.6172\n",
      "epoch: 2, loss: 0.8343, train acc: 0.6346\n",
      "epoch: 2, loss: 0.8628, train acc: 0.6285\n",
      "epoch: 2, loss: 0.8649, train acc: 0.6440\n",
      "epoch: 2, loss: 0.8694, train acc: 0.6362\n",
      "epoch: 2, loss: 0.8462, train acc: 0.6477\n",
      "epoch: 2, loss: 0.8543, train acc: 0.6398\n",
      "epoch: 2, loss: 0.8488, train acc: 0.6497\n",
      "epoch: 2, loss: 0.8520, train acc: 0.6536\n",
      "epoch: 2, loss: 0.8524, train acc: 0.6580\n",
      "epoch: 2, loss: 0.8664, train acc: 0.6519\n",
      "epoch: 2, loss: 0.8600, train acc: 0.6498\n",
      "epoch: 2, loss: 0.8527, train acc: 0.6498\n",
      "epoch: 2, loss: 0.8423, train acc: 0.6541\n",
      "epoch: 2, loss: 0.8437, train acc: 0.6522\n",
      "epoch: 2, loss: 0.8388, train acc: 0.6521\n",
      "epoch: 2, loss: 0.8405, train acc: 0.6491\n",
      "epoch: 2, loss: 0.8446, train acc: 0.6465\n",
      "epoch: 2, loss: 0.8410, train acc: 0.6473\n",
      "epoch: 2, loss: 0.8513, train acc: 0.6432\n",
      ">epoch: 2, val_acc: 0.7181, val_f1: 0.2786\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 3\n",
      "epoch: 3, loss: 0.7606, train acc: 0.7188\n",
      "epoch: 3, loss: 0.7267, train acc: 0.6786\n",
      "epoch: 3, loss: 0.8304, train acc: 0.6510\n",
      "epoch: 3, loss: 0.8827, train acc: 0.6287\n",
      "epoch: 3, loss: 0.9205, train acc: 0.6023\n",
      "epoch: 3, loss: 0.9174, train acc: 0.6088\n",
      "epoch: 3, loss: 0.9211, train acc: 0.6035\n",
      "epoch: 3, loss: 0.8788, train acc: 0.6233\n",
      "epoch: 3, loss: 0.8600, train acc: 0.6369\n",
      "epoch: 3, loss: 0.8597, train acc: 0.6410\n",
      "epoch: 3, loss: 0.8364, train acc: 0.6514\n",
      "epoch: 3, loss: 0.8321, train acc: 0.6524\n",
      "epoch: 3, loss: 0.8283, train acc: 0.6492\n",
      "epoch: 3, loss: 0.8390, train acc: 0.6465\n",
      "epoch: 3, loss: 0.8504, train acc: 0.6389\n",
      "epoch: 3, loss: 0.8493, train acc: 0.6429\n",
      "epoch: 3, loss: 0.8467, train acc: 0.6486\n",
      "epoch: 3, loss: 0.8426, train acc: 0.6545\n",
      "epoch: 3, loss: 0.8342, train acc: 0.6583\n",
      "epoch: 3, loss: 0.8417, train acc: 0.6553\n",
      "epoch: 3, loss: 0.8423, train acc: 0.6495\n",
      ">epoch: 3, val_acc: 0.7181, val_f1: 0.2786\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 4\n",
      "epoch: 4, loss: 1.0078, train acc: 0.5000\n",
      "epoch: 4, loss: 0.8554, train acc: 0.5625\n",
      "epoch: 4, loss: 0.8664, train acc: 0.5909\n",
      "epoch: 4, loss: 0.7902, train acc: 0.6484\n",
      "epoch: 4, loss: 0.7973, train acc: 0.6577\n",
      "epoch: 4, loss: 0.8235, train acc: 0.6370\n",
      "epoch: 4, loss: 0.8218, train acc: 0.6371\n",
      "epoch: 4, loss: 0.8051, train acc: 0.6458\n",
      "epoch: 4, loss: 0.7989, train acc: 0.6479\n",
      "epoch: 4, loss: 0.7800, train acc: 0.6590\n",
      "epoch: 4, loss: 0.8061, train acc: 0.6569\n",
      "epoch: 4, loss: 0.7847, train acc: 0.6685\n",
      "epoch: 4, loss: 0.7825, train acc: 0.6711\n",
      "epoch: 4, loss: 0.7982, train acc: 0.6638\n",
      "epoch: 4, loss: 0.8021, train acc: 0.6585\n",
      "epoch: 4, loss: 0.8037, train acc: 0.6505\n",
      "epoch: 4, loss: 0.7973, train acc: 0.6535\n",
      "epoch: 4, loss: 0.7912, train acc: 0.6599\n",
      "epoch: 4, loss: 0.7928, train acc: 0.6614\n",
      "epoch: 4, loss: 0.8014, train acc: 0.6615\n",
      "epoch: 4, loss: 0.8036, train acc: 0.6590\n",
      "epoch: 4, loss: 0.7997, train acc: 0.6584\n",
      ">epoch: 4, val_acc: 0.7181, val_f1: 0.2786\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 5\n",
      "epoch: 5, loss: 0.8254, train acc: 0.6375\n",
      "epoch: 5, loss: 0.7968, train acc: 0.6687\n",
      "epoch: 5, loss: 0.7910, train acc: 0.6708\n",
      "epoch: 5, loss: 0.7412, train acc: 0.6906\n",
      "epoch: 5, loss: 0.7781, train acc: 0.6750\n",
      "epoch: 5, loss: 0.7778, train acc: 0.6750\n",
      "epoch: 5, loss: 0.7980, train acc: 0.6696\n",
      "epoch: 5, loss: 0.7829, train acc: 0.6719\n",
      "epoch: 5, loss: 0.7887, train acc: 0.6667\n",
      "epoch: 5, loss: 0.7845, train acc: 0.6625\n",
      "epoch: 5, loss: 0.7896, train acc: 0.6591\n",
      "epoch: 5, loss: 0.7896, train acc: 0.6583\n",
      "epoch: 5, loss: 0.7970, train acc: 0.6577\n",
      "epoch: 5, loss: 0.8044, train acc: 0.6607\n",
      "epoch: 5, loss: 0.7986, train acc: 0.6633\n",
      "epoch: 5, loss: 0.8027, train acc: 0.6609\n",
      "epoch: 5, loss: 0.8000, train acc: 0.6596\n",
      "epoch: 5, loss: 0.7854, train acc: 0.6660\n",
      "epoch: 5, loss: 0.7882, train acc: 0.6638\n",
      "epoch: 5, loss: 0.7939, train acc: 0.6613\n",
      "epoch: 5, loss: 0.7885, train acc: 0.6661\n",
      ">epoch: 5, val_acc: 0.7181, val_f1: 0.2786\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 6\n",
      "epoch: 6, loss: 0.8986, train acc: 0.6250\n",
      "epoch: 6, loss: 0.9174, train acc: 0.6389\n",
      "epoch: 6, loss: 0.8739, train acc: 0.6696\n",
      "epoch: 6, loss: 0.8473, train acc: 0.6645\n",
      "epoch: 6, loss: 0.8498, train acc: 0.6589\n",
      "epoch: 6, loss: 0.8288, train acc: 0.6530\n",
      "epoch: 6, loss: 0.8181, train acc: 0.6507\n",
      "epoch: 6, loss: 0.8111, train acc: 0.6571\n",
      "epoch: 6, loss: 0.8208, train acc: 0.6648\n",
      "epoch: 6, loss: 0.8169, train acc: 0.6696\n",
      "epoch: 6, loss: 0.8257, train acc: 0.6725\n",
      "epoch: 6, loss: 0.8199, train acc: 0.6780\n",
      "epoch: 6, loss: 0.8177, train acc: 0.6748\n",
      "epoch: 6, loss: 0.8191, train acc: 0.6730\n",
      "epoch: 6, loss: 0.8237, train acc: 0.6681\n",
      "epoch: 6, loss: 0.8162, train acc: 0.6709\n",
      "epoch: 6, loss: 0.8212, train acc: 0.6689\n",
      "epoch: 6, loss: 0.8316, train acc: 0.6601\n",
      "epoch: 6, loss: 0.8458, train acc: 0.6483\n",
      "epoch: 6, loss: 0.8362, train acc: 0.6521\n",
      "epoch: 6, loss: 0.8289, train acc: 0.6520\n",
      ">epoch: 6, val_acc: 0.7181, val_f1: 0.2786\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 7\n",
      "epoch: 7, loss: 0.9718, train acc: 0.6458\n",
      "epoch: 7, loss: 0.9353, train acc: 0.6562\n",
      "epoch: 7, loss: 0.9168, train acc: 0.6779\n",
      "epoch: 7, loss: 0.8762, train acc: 0.6806\n",
      "epoch: 7, loss: 0.8518, train acc: 0.6549\n",
      "epoch: 7, loss: 0.8509, train acc: 0.6496\n",
      "epoch: 7, loss: 0.8550, train acc: 0.6572\n",
      "epoch: 7, loss: 0.8689, train acc: 0.6513\n",
      "epoch: 7, loss: 0.8574, train acc: 0.6526\n",
      "epoch: 7, loss: 0.8479, train acc: 0.6549\n",
      "epoch: 7, loss: 0.8292, train acc: 0.6675\n",
      "epoch: 7, loss: 0.8076, train acc: 0.6756\n",
      "epoch: 7, loss: 0.8014, train acc: 0.6815\n",
      "epoch: 7, loss: 0.7956, train acc: 0.6857\n",
      "epoch: 7, loss: 0.8159, train acc: 0.6789\n",
      "epoch: 7, loss: 0.8113, train acc: 0.6779\n",
      "epoch: 7, loss: 0.8118, train acc: 0.6762\n",
      "epoch: 7, loss: 0.8149, train acc: 0.6662\n",
      "epoch: 7, loss: 0.8182, train acc: 0.6620\n",
      "epoch: 7, loss: 0.8147, train acc: 0.6633\n",
      "epoch: 7, loss: 0.8088, train acc: 0.6650\n",
      ">epoch: 7, val_acc: 0.7181, val_f1: 0.2786\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 8\n",
      "epoch: 8, loss: 0.8275, train acc: 0.6562\n",
      "epoch: 8, loss: 0.7215, train acc: 0.7143\n",
      "epoch: 8, loss: 0.7326, train acc: 0.6979\n",
      "epoch: 8, loss: 0.7477, train acc: 0.6912\n",
      "epoch: 8, loss: 0.7595, train acc: 0.6733\n",
      "epoch: 8, loss: 0.7825, train acc: 0.6551\n",
      "epoch: 8, loss: 0.7617, train acc: 0.6602\n",
      "epoch: 8, loss: 0.7527, train acc: 0.6672\n",
      "epoch: 8, loss: 0.7509, train acc: 0.6756\n",
      "epoch: 8, loss: 0.7498, train acc: 0.6782\n",
      "epoch: 8, loss: 0.7501, train acc: 0.6743\n",
      "epoch: 8, loss: 0.7632, train acc: 0.6689\n",
      "epoch: 8, loss: 0.7694, train acc: 0.6613\n",
      "epoch: 8, loss: 0.7690, train acc: 0.6688\n",
      "epoch: 8, loss: 0.7887, train acc: 0.6623\n",
      "epoch: 8, loss: 0.7844, train acc: 0.6664\n",
      "epoch: 8, loss: 0.7908, train acc: 0.6623\n",
      "epoch: 8, loss: 0.7885, train acc: 0.6624\n",
      "epoch: 8, loss: 0.7885, train acc: 0.6617\n",
      "epoch: 8, loss: 0.7812, train acc: 0.6669\n",
      "epoch: 8, loss: 0.7779, train acc: 0.6691\n",
      ">epoch: 8, val_acc: 0.7181, val_f1: 0.2786\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 9\n",
      "epoch: 9, loss: 0.9744, train acc: 0.6250\n",
      "epoch: 9, loss: 0.7092, train acc: 0.7292\n",
      "epoch: 9, loss: 0.7525, train acc: 0.6875\n",
      "epoch: 9, loss: 0.7416, train acc: 0.6836\n",
      "epoch: 9, loss: 0.7843, train acc: 0.6637\n",
      "epoch: 9, loss: 0.7843, train acc: 0.6803\n",
      "epoch: 9, loss: 0.8024, train acc: 0.6714\n",
      "epoch: 9, loss: 0.8038, train acc: 0.6771\n",
      "epoch: 9, loss: 0.8132, train acc: 0.6784\n",
      "epoch: 9, loss: 0.8129, train acc: 0.6753\n",
      "epoch: 9, loss: 0.8142, train acc: 0.6691\n",
      "epoch: 9, loss: 0.8089, train acc: 0.6696\n",
      "epoch: 9, loss: 0.8113, train acc: 0.6639\n",
      "epoch: 9, loss: 0.8038, train acc: 0.6686\n",
      "epoch: 9, loss: 0.7986, train acc: 0.6646\n",
      "epoch: 9, loss: 0.7998, train acc: 0.6628\n",
      "epoch: 9, loss: 0.7941, train acc: 0.6636\n",
      "epoch: 9, loss: 0.7935, train acc: 0.6686\n",
      "epoch: 9, loss: 0.8014, train acc: 0.6676\n",
      "epoch: 9, loss: 0.8034, train acc: 0.6647\n",
      "epoch: 9, loss: 0.8016, train acc: 0.6640\n",
      "epoch: 9, loss: 0.8019, train acc: 0.6637\n",
      ">epoch: 9, val_acc: 0.7181, val_f1: 0.2786\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 10\n",
      "epoch: 10, loss: 0.8120, train acc: 0.6500\n",
      "epoch: 10, loss: 0.8069, train acc: 0.6438\n",
      "epoch: 10, loss: 0.7924, train acc: 0.6708\n",
      "epoch: 10, loss: 0.7877, train acc: 0.6813\n",
      "epoch: 10, loss: 0.7655, train acc: 0.6925\n",
      "epoch: 10, loss: 0.7685, train acc: 0.6854\n",
      "epoch: 10, loss: 0.7751, train acc: 0.6786\n",
      "epoch: 10, loss: 0.7722, train acc: 0.6844\n",
      "epoch: 10, loss: 0.7631, train acc: 0.6875\n",
      "epoch: 10, loss: 0.7506, train acc: 0.6900\n",
      "epoch: 10, loss: 0.7460, train acc: 0.6920\n",
      "epoch: 10, loss: 0.7432, train acc: 0.6917\n",
      "epoch: 10, loss: 0.7450, train acc: 0.6923\n",
      "epoch: 10, loss: 0.7559, train acc: 0.6848\n",
      "epoch: 10, loss: 0.7621, train acc: 0.6783\n",
      "epoch: 10, loss: 0.7554, train acc: 0.6773\n",
      "epoch: 10, loss: 0.7593, train acc: 0.6743\n",
      "epoch: 10, loss: 0.7499, train acc: 0.6813\n",
      "epoch: 10, loss: 0.7516, train acc: 0.6849\n",
      "epoch: 10, loss: 0.7665, train acc: 0.6806\n",
      "epoch: 10, loss: 0.7664, train acc: 0.6815\n",
      ">epoch: 10, val_acc: 0.7181, val_f1: 0.2786\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 11\n",
      "epoch: 11, loss: 0.6122, train acc: 0.7500\n",
      "epoch: 11, loss: 0.7434, train acc: 0.7014\n",
      "epoch: 11, loss: 0.6923, train acc: 0.7143\n",
      "epoch: 11, loss: 0.7149, train acc: 0.7138\n",
      "epoch: 11, loss: 0.7051, train acc: 0.7188\n",
      "epoch: 11, loss: 0.7028, train acc: 0.7198\n",
      "epoch: 11, loss: 0.7094, train acc: 0.7188\n",
      "epoch: 11, loss: 0.7112, train acc: 0.7163\n",
      "epoch: 11, loss: 0.7026, train acc: 0.7216\n",
      "epoch: 11, loss: 0.7017, train acc: 0.7194\n",
      "epoch: 11, loss: 0.7119, train acc: 0.7118\n",
      "epoch: 11, loss: 0.7164, train acc: 0.7087\n",
      "epoch: 11, loss: 0.7183, train acc: 0.7061\n",
      "epoch: 11, loss: 0.7261, train acc: 0.6993\n",
      "epoch: 11, loss: 0.7494, train acc: 0.6909\n",
      "epoch: 11, loss: 0.7604, train acc: 0.6820\n",
      "epoch: 11, loss: 0.7606, train acc: 0.6793\n",
      "epoch: 11, loss: 0.7642, train acc: 0.6735\n",
      "epoch: 11, loss: 0.7729, train acc: 0.6709\n",
      "epoch: 11, loss: 0.7727, train acc: 0.6705\n",
      "epoch: 11, loss: 0.7818, train acc: 0.6683\n",
      ">epoch: 11, val_acc: 0.7181, val_f1: 0.2786\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 12\n",
      "epoch: 12, loss: 0.6526, train acc: 0.7292\n",
      "epoch: 12, loss: 0.7214, train acc: 0.6641\n",
      "epoch: 12, loss: 0.6423, train acc: 0.7308\n",
      "epoch: 12, loss: 0.6779, train acc: 0.7083\n",
      "epoch: 12, loss: 0.7122, train acc: 0.6929\n",
      "epoch: 12, loss: 0.7439, train acc: 0.6763\n",
      "epoch: 12, loss: 0.7534, train acc: 0.6761\n",
      "epoch: 12, loss: 0.7492, train acc: 0.6694\n",
      "epoch: 12, loss: 0.7504, train acc: 0.6701\n",
      "epoch: 12, loss: 0.7538, train acc: 0.6719\n",
      "epoch: 12, loss: 0.7447, train acc: 0.6733\n",
      "epoch: 12, loss: 0.7506, train acc: 0.6713\n",
      "epoch: 12, loss: 0.7664, train acc: 0.6677\n",
      "epoch: 12, loss: 0.7684, train acc: 0.6700\n",
      "epoch: 12, loss: 0.7807, train acc: 0.6627\n",
      "epoch: 12, loss: 0.7815, train acc: 0.6587\n",
      "epoch: 12, loss: 0.7807, train acc: 0.6559\n",
      "epoch: 12, loss: 0.7804, train acc: 0.6577\n",
      "epoch: 12, loss: 0.7889, train acc: 0.6566\n",
      "epoch: 12, loss: 0.7910, train acc: 0.6582\n",
      "epoch: 12, loss: 0.7841, train acc: 0.6632\n",
      ">epoch: 12, val_acc: 0.7181, val_f1: 0.2786\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 13\n",
      "epoch: 13, loss: 0.7275, train acc: 0.7500\n",
      "epoch: 13, loss: 0.7558, train acc: 0.7143\n",
      "epoch: 13, loss: 0.7380, train acc: 0.6979\n",
      "epoch: 13, loss: 0.7671, train acc: 0.6838\n",
      "epoch: 13, loss: 0.7672, train acc: 0.6818\n",
      "epoch: 13, loss: 0.7728, train acc: 0.6736\n",
      "epoch: 13, loss: 0.7591, train acc: 0.6680\n",
      "epoch: 13, loss: 0.7625, train acc: 0.6605\n",
      "epoch: 13, loss: 0.7486, train acc: 0.6667\n",
      "epoch: 13, loss: 0.7581, train acc: 0.6649\n",
      "epoch: 13, loss: 0.7399, train acc: 0.6743\n",
      "epoch: 13, loss: 0.7445, train acc: 0.6754\n",
      "epoch: 13, loss: 0.7378, train acc: 0.6754\n",
      "epoch: 13, loss: 0.7316, train acc: 0.6782\n",
      "epoch: 13, loss: 0.7247, train acc: 0.6823\n",
      "epoch: 13, loss: 0.7395, train acc: 0.6778\n",
      "epoch: 13, loss: 0.7362, train acc: 0.6776\n",
      "epoch: 13, loss: 0.7460, train acc: 0.6760\n",
      "epoch: 13, loss: 0.7456, train acc: 0.6773\n",
      "epoch: 13, loss: 0.7480, train acc: 0.6759\n",
      "epoch: 13, loss: 0.7430, train acc: 0.6801\n",
      ">epoch: 13, val_acc: 0.7181, val_f1: 0.2786\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 14\n",
      "epoch: 14, loss: 1.0053, train acc: 0.6875\n",
      "epoch: 14, loss: 0.8589, train acc: 0.6667\n",
      "epoch: 14, loss: 0.8338, train acc: 0.6818\n",
      "epoch: 14, loss: 0.7850, train acc: 0.6992\n",
      "epoch: 14, loss: 0.7777, train acc: 0.6964\n",
      "epoch: 14, loss: 0.7739, train acc: 0.6899\n",
      "epoch: 14, loss: 0.7814, train acc: 0.6895\n",
      "epoch: 14, loss: 0.7640, train acc: 0.6997\n",
      "epoch: 14, loss: 0.7509, train acc: 0.7088\n",
      "epoch: 14, loss: 0.7494, train acc: 0.7092\n",
      "epoch: 14, loss: 0.7590, train acc: 0.7047\n",
      "epoch: 14, loss: 0.7646, train acc: 0.6975\n",
      "epoch: 14, loss: 0.7618, train acc: 0.6967\n",
      "epoch: 14, loss: 0.7649, train acc: 0.6951\n",
      "epoch: 14, loss: 0.7618, train acc: 0.6945\n",
      "epoch: 14, loss: 0.7530, train acc: 0.6982\n",
      "epoch: 14, loss: 0.7526, train acc: 0.6991\n",
      "epoch: 14, loss: 0.7574, train acc: 0.6948\n",
      "epoch: 14, loss: 0.7592, train acc: 0.6916\n",
      "epoch: 14, loss: 0.7576, train acc: 0.6882\n",
      "epoch: 14, loss: 0.7570, train acc: 0.6856\n",
      "epoch: 14, loss: 0.7568, train acc: 0.6862\n",
      ">epoch: 14, val_acc: 0.6968, val_f1: 0.2738\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 15\n",
      "epoch: 15, loss: 0.6888, train acc: 0.7375\n",
      "epoch: 15, loss: 0.6958, train acc: 0.7312\n",
      "epoch: 15, loss: 0.7405, train acc: 0.7208\n",
      "epoch: 15, loss: 0.7176, train acc: 0.7344\n",
      "epoch: 15, loss: 0.7056, train acc: 0.7375\n",
      "epoch: 15, loss: 0.7193, train acc: 0.7250\n",
      "epoch: 15, loss: 0.7100, train acc: 0.7250\n",
      "epoch: 15, loss: 0.7162, train acc: 0.7188\n",
      "epoch: 15, loss: 0.7151, train acc: 0.7167\n",
      "epoch: 15, loss: 0.7305, train acc: 0.7212\n",
      "epoch: 15, loss: 0.7212, train acc: 0.7250\n",
      "epoch: 15, loss: 0.7294, train acc: 0.7208\n",
      "epoch: 15, loss: 0.7288, train acc: 0.7192\n",
      "epoch: 15, loss: 0.7307, train acc: 0.7161\n",
      "epoch: 15, loss: 0.7319, train acc: 0.7125\n",
      "epoch: 15, loss: 0.7367, train acc: 0.7055\n",
      "epoch: 15, loss: 0.7390, train acc: 0.6985\n",
      "epoch: 15, loss: 0.7390, train acc: 0.6979\n",
      "epoch: 15, loss: 0.7532, train acc: 0.6895\n",
      "epoch: 15, loss: 0.7544, train acc: 0.6881\n",
      "epoch: 15, loss: 0.7554, train acc: 0.6887\n",
      ">epoch: 15, val_acc: 0.7181, val_f1: 0.2786\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 16\n",
      "epoch: 16, loss: 0.8002, train acc: 0.6094\n",
      "epoch: 16, loss: 0.7526, train acc: 0.6250\n",
      "epoch: 16, loss: 0.7307, train acc: 0.6652\n",
      "epoch: 16, loss: 0.7689, train acc: 0.6579\n",
      "epoch: 16, loss: 0.7895, train acc: 0.6510\n",
      "epoch: 16, loss: 0.7889, train acc: 0.6401\n",
      "epoch: 16, loss: 0.8043, train acc: 0.6397\n",
      "epoch: 16, loss: 0.7917, train acc: 0.6426\n",
      "epoch: 16, loss: 0.7937, train acc: 0.6449\n",
      "epoch: 16, loss: 0.7783, train acc: 0.6454\n",
      "epoch: 16, loss: 0.7750, train acc: 0.6470\n",
      "epoch: 16, loss: 0.7774, train acc: 0.6504\n",
      "epoch: 16, loss: 0.7738, train acc: 0.6533\n",
      "epoch: 16, loss: 0.7754, train acc: 0.6603\n",
      "epoch: 16, loss: 0.7804, train acc: 0.6596\n",
      "epoch: 16, loss: 0.7723, train acc: 0.6653\n",
      "epoch: 16, loss: 0.7726, train acc: 0.6711\n",
      "epoch: 16, loss: 0.7728, train acc: 0.6713\n",
      "epoch: 16, loss: 0.7703, train acc: 0.6742\n",
      "epoch: 16, loss: 0.7658, train acc: 0.6742\n",
      "epoch: 16, loss: 0.7702, train acc: 0.6749\n",
      ">epoch: 16, val_acc: 0.7181, val_f1: 0.2786\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 17\n",
      "epoch: 17, loss: 0.5785, train acc: 0.7917\n",
      "epoch: 17, loss: 0.7533, train acc: 0.6875\n",
      "epoch: 17, loss: 0.7151, train acc: 0.7212\n",
      "epoch: 17, loss: 0.7164, train acc: 0.7083\n",
      "epoch: 17, loss: 0.7345, train acc: 0.7065\n",
      "epoch: 17, loss: 0.7238, train acc: 0.7210\n",
      "epoch: 17, loss: 0.7504, train acc: 0.7008\n",
      "epoch: 17, loss: 0.7500, train acc: 0.6990\n",
      "epoch: 17, loss: 0.7608, train acc: 0.6904\n",
      "epoch: 17, loss: 0.7556, train acc: 0.6901\n",
      "epoch: 17, loss: 0.7501, train acc: 0.6934\n",
      "epoch: 17, loss: 0.7509, train acc: 0.6918\n",
      "epoch: 17, loss: 0.7489, train acc: 0.6935\n",
      "epoch: 17, loss: 0.7477, train acc: 0.6930\n",
      "epoch: 17, loss: 0.7510, train acc: 0.6892\n",
      "epoch: 17, loss: 0.7520, train acc: 0.6819\n",
      "epoch: 17, loss: 0.7498, train acc: 0.6837\n",
      "epoch: 17, loss: 0.7581, train acc: 0.6825\n",
      "epoch: 17, loss: 0.7666, train acc: 0.6821\n",
      "epoch: 17, loss: 0.7625, train acc: 0.6856\n",
      "epoch: 17, loss: 0.7637, train acc: 0.6869\n",
      ">epoch: 17, val_acc: 0.7181, val_f1: 0.2786\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 18\n",
      "epoch: 18, loss: 0.6936, train acc: 0.7812\n",
      "epoch: 18, loss: 0.7389, train acc: 0.6964\n",
      "epoch: 18, loss: 0.7568, train acc: 0.6979\n",
      "epoch: 18, loss: 0.7162, train acc: 0.7169\n",
      "epoch: 18, loss: 0.7124, train acc: 0.7188\n",
      "epoch: 18, loss: 0.7526, train acc: 0.7037\n",
      "epoch: 18, loss: 0.7479, train acc: 0.7031\n",
      "epoch: 18, loss: 0.7479, train acc: 0.7027\n",
      "epoch: 18, loss: 0.7435, train acc: 0.7054\n",
      "epoch: 18, loss: 0.7490, train acc: 0.7101\n",
      "epoch: 18, loss: 0.7561, train acc: 0.7043\n",
      "epoch: 18, loss: 0.7674, train acc: 0.6941\n",
      "epoch: 18, loss: 0.7674, train acc: 0.6895\n",
      "epoch: 18, loss: 0.7741, train acc: 0.6819\n",
      "epoch: 18, loss: 0.7682, train acc: 0.6840\n",
      "epoch: 18, loss: 0.7676, train acc: 0.6843\n",
      "epoch: 18, loss: 0.7671, train acc: 0.6867\n",
      "epoch: 18, loss: 0.7704, train acc: 0.6846\n",
      "epoch: 18, loss: 0.7667, train acc: 0.6841\n",
      "epoch: 18, loss: 0.7663, train acc: 0.6849\n",
      "epoch: 18, loss: 0.7639, train acc: 0.6863\n",
      ">epoch: 18, val_acc: 0.7181, val_f1: 0.2786\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 19\n",
      "epoch: 19, loss: 0.3674, train acc: 0.9375\n",
      "epoch: 19, loss: 0.6048, train acc: 0.7396\n",
      "epoch: 19, loss: 0.7086, train acc: 0.7216\n",
      "epoch: 19, loss: 0.6876, train acc: 0.7383\n",
      "epoch: 19, loss: 0.7196, train acc: 0.7292\n",
      "epoch: 19, loss: 0.7110, train acc: 0.7260\n",
      "epoch: 19, loss: 0.7450, train acc: 0.7117\n",
      "epoch: 19, loss: 0.7544, train acc: 0.7066\n",
      "epoch: 19, loss: 0.7636, train acc: 0.6997\n",
      "epoch: 19, loss: 0.7571, train acc: 0.6970\n",
      "epoch: 19, loss: 0.7649, train acc: 0.6912\n",
      "epoch: 19, loss: 0.7628, train acc: 0.6908\n",
      "epoch: 19, loss: 0.7702, train acc: 0.6855\n",
      "epoch: 19, loss: 0.7679, train acc: 0.6828\n",
      "epoch: 19, loss: 0.7732, train acc: 0.6778\n",
      "epoch: 19, loss: 0.7706, train acc: 0.6809\n",
      "epoch: 19, loss: 0.7678, train acc: 0.6775\n",
      "epoch: 19, loss: 0.7680, train acc: 0.6759\n",
      "epoch: 19, loss: 0.7614, train acc: 0.6799\n",
      "epoch: 19, loss: 0.7553, train acc: 0.6816\n",
      "epoch: 19, loss: 0.7482, train acc: 0.6863\n",
      "epoch: 19, loss: 0.7462, train acc: 0.6874\n",
      ">epoch: 19, val_acc: 0.7181, val_f1: 0.2786\n",
      "Namespace(model_type='tri_gcn', modules={'tgcn': True, 'semgcn': True, 'lexgcn': True}, num_layers={'tgcn': 2, 'semgcn': 2, 'lexgcn': 2}, year='2016', train_file='data/train2016restaurant.txt', test_file='data/test2016restaurant.txt', model_path='test_models/2016tri_gcn+concat_seed819_reg0.0005_drop0.4_cdrop0.4_lr2.5e-05_epochs20_adam', val_file='val.txt', log='log', bert_model='bert_large_uncased', cooc_path='cooc_matrix.csv', cooc=       100       1000   10000  10003     10005     10007     10009     1001   \\\n",
      "100      0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "1000     0.0  0.000000    0.0    0.0  0.000606  0.000303  0.000606  0.035120   \n",
      "10000    0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.333333   \n",
      "10003    0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "10005    0.0  0.333333    0.0    0.0  0.000000  0.166667  0.000000  0.166667   \n",
      "...      ...       ...    ...    ...       ...       ...       ...       ...   \n",
      "9992     0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "9993     0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "9996     0.0  0.500000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "9997     0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "9998     0.0  0.400000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "       10010  10015  ...     9986      9987      999    9990      9991   \\\n",
      "100      0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "1000     0.0    0.0  ...  0.002422  0.000303  0.062368    0.0  0.000606   \n",
      "10000    0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "10003    0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "10005    0.0    0.0  ...  0.000000  0.000000  0.333333    0.0  0.000000   \n",
      "...      ...    ...  ...       ...       ...       ...    ...       ...   \n",
      "9992     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "9993     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "9996     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "9997     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "9998     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "\n",
      "       9992   9993      9996   9997      9998   \n",
      "100      0.0    0.0  0.000000    0.0  0.000000  \n",
      "1000     0.0    0.0  0.000606    0.0  0.001211  \n",
      "10000    0.0    0.0  0.000000    0.0  0.000000  \n",
      "10003    0.0    0.0  0.000000    0.0  0.000000  \n",
      "10005    0.0    0.0  0.000000    0.0  0.000000  \n",
      "...      ...    ...       ...    ...       ...  \n",
      "9992     0.0    0.0  0.000000    0.0  0.000000  \n",
      "9993     0.0    0.0  0.000000    0.0  0.000000  \n",
      "9996     0.0    0.0  0.000000    0.0  0.000000  \n",
      "9997     0.0    0.0  0.000000    0.0  0.000000  \n",
      "9998     0.0    0.0  0.000000    0.0  0.000000  \n",
      "\n",
      "[13075 rows x 13075 columns], learning_rate=2.5e-05, dropout=0.4, concat_dropout=0.4, bert_dropout=0.2, l2reg=0.0005, num_epoch=20, batch_size=16, log_step=5, max_seq_len=100, polarities_dim=3, device=device(type='cuda'), seed=819, valset_ratio=0.1, do_train=True, do_eval=True, eval_epoch_num=0, fusion_type='concat', use_ensemble=True, save_models='none', print_sent=False, optim='adam', n_gpu=1)\n",
      "{'none': 0, 'acl': 1, 'acl:relcl': 2, 'advcl': 3, 'advcl:relcl': 4, 'advmod': 5, 'amod': 6, 'appos': 7, 'aux': 8, 'aux:pass': 9, 'case': 10, 'cc': 11, 'cc:preconj': 12, 'ccomp': 13, 'compound': 14, 'compound:prt': 15, 'conj': 16, 'cop': 17, 'csubj': 18, 'dep': 19, 'det': 20, 'det:predet': 21, 'discourse': 22, 'dislocated': 23, 'expl': 24, 'fixed': 25, 'flat': 26, 'iobj': 27, 'list': 28, 'mark': 29, 'nmod': 30, 'nmod:npmod': 31, 'nmod:poss': 32, 'nmod:tmod': 33, 'nsubj': 34, 'nsubj:outer': 35, 'nsubj:pass': 36, 'nummod': 37, 'obj': 38, 'obl': 39, 'obl:agent': 40, 'obl:npmod': 41, 'obl:tmod': 42, 'parataxis': 43, 'punct': 44, 'reparandum': 45, 'root': 46, 'vocative': 47, 'xcomp': 48}\n",
      "{'-1': 0, '0': 1, '1': 2}\n",
      "Model name 'bert_large_uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming 'bert_large_uncased' is a path or url to a directory containing tokenizer files.\n",
      "Didn't find file bert_large_uncased/added_tokens.json. We won't load it.\n",
      "Didn't find file bert_large_uncased/special_tokens_map.json. We won't load it.\n",
      "Didn't find file bert_large_uncased/tokenizer_config.json. We won't load it.\n",
      "loading file bert_large_uncased/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "{\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 3,\n",
      "  \"num_types\": 49,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file bert_large_uncased/pytorch_model.bin\n",
      "Weights of AsaTgcnSem not initialized from pretrained model: ['gate_weight', 'gate_bias', 'ensemble', 'TGCNLayers.0.weight', 'TGCNLayers.0.bias', 'TGCNLayers.0.dense.weight', 'TGCNLayers.1.weight', 'TGCNLayers.1.bias', 'TGCNLayers.1.dense.weight', 'SemGCNLayers.0.weight', 'SemGCNLayers.0.bias', 'SemGCNLayers.1.weight', 'SemGCNLayers.1.bias', 'LexGCNLayers.0.weight', 'LexGCNLayers.0.bias', 'LexGCNLayers.1.weight', 'LexGCNLayers.1.bias', 'fc_single.weight', 'fc_single.bias', 'ensemble_linear_tgcn.weight', 'ensemble_linear_tgcn.bias', 'ensemble_linear_semgcn.weight', 'ensemble_linear_semgcn.bias', 'ensemble_linear_lexgcn.weight', 'ensemble_linear_lexgcn.bias', 'dep_embedding.weight']\n",
      "Weights from pretrained model not used in AsaTgcnSem: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "[{'input_ids': tensor([  101, 13325,  2013,  3025,  8466,  2023,  2109,  2000,  2022,  1037,\n",
      "         2204,  2173,  1010,  2021,  2025,  2151,  2936,  1012,   102,  2173,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'valid_ids': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'segment_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'mem_valid_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'dep_adj_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'dep_value_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'polarity': 0, 'raw_text': 'judging from previous posts this used to be a good place , but not any longer .', 'aspect': 'place'}]\n",
      "[{'input_ids': tensor([  101, 13325,  2013,  3025,  8466,  2023,  2109,  2000,  2022,  1037,\n",
      "         2204,  2173,  1010,  2021,  2025,  2151,  2936,  1012,   102,  2173,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'valid_ids': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'segment_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'mem_valid_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'dep_adj_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'dep_value_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'polarity': 0, 'raw_text': 'judging from previous posts this used to be a good place , but not any longer .', 'aspect': 'place'}]\n",
      "[{'input_ids': tensor([  101,  4240,  2428,  2204, 10514,  6182,  1012,   102, 10514,  6182,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'valid_ids': tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'segment_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'mem_valid_ids': tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'dep_adj_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'dep_value_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 5,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'polarity': 2, 'raw_text': 'serves really good sushi .', 'aspect': 'sushi'}]\n",
      "cuda memory allocated: 1400893440\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 0\n",
      "epoch: 0, loss: 0.7974, train acc: 0.6875\n",
      "epoch: 0, loss: 0.9262, train acc: 0.6687\n",
      "epoch: 0, loss: 0.8272, train acc: 0.7167\n",
      "epoch: 0, loss: 0.8633, train acc: 0.6906\n",
      "epoch: 0, loss: 0.8376, train acc: 0.6975\n",
      "epoch: 0, loss: 0.8350, train acc: 0.6875\n",
      "epoch: 0, loss: 0.8763, train acc: 0.6732\n",
      "epoch: 0, loss: 0.9128, train acc: 0.6609\n",
      "epoch: 0, loss: 0.9164, train acc: 0.6431\n",
      "epoch: 0, loss: 0.9262, train acc: 0.6438\n",
      "epoch: 0, loss: 0.9052, train acc: 0.6511\n",
      "epoch: 0, loss: 0.9221, train acc: 0.6500\n",
      "epoch: 0, loss: 0.9363, train acc: 0.6481\n",
      "epoch: 0, loss: 0.9366, train acc: 0.6473\n",
      "epoch: 0, loss: 0.9276, train acc: 0.6458\n",
      "epoch: 0, loss: 0.9226, train acc: 0.6469\n",
      "epoch: 0, loss: 0.9282, train acc: 0.6441\n",
      "epoch: 0, loss: 0.9286, train acc: 0.6451\n",
      "epoch: 0, loss: 0.9223, train acc: 0.6447\n",
      "epoch: 0, loss: 0.9237, train acc: 0.6419\n",
      "epoch: 0, loss: 0.9250, train acc: 0.6417\n",
      ">epoch: 0, val_acc: 0.6596, val_f1: 0.2758\n",
      ">> epoch: 0, test_acc: 0.7354, test_f1: 0.2871\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 1\n",
      "epoch: 1, loss: 0.9666, train acc: 0.5781\n",
      "epoch: 1, loss: 0.8326, train acc: 0.6319\n",
      "epoch: 1, loss: 0.8094, train acc: 0.6518\n",
      "epoch: 1, loss: 0.8227, train acc: 0.6645\n",
      "epoch: 1, loss: 0.7969, train acc: 0.6693\n",
      "epoch: 1, loss: 0.8126, train acc: 0.6509\n",
      "epoch: 1, loss: 0.8130, train acc: 0.6507\n",
      "epoch: 1, loss: 0.8273, train acc: 0.6506\n",
      "epoch: 1, loss: 0.8142, train acc: 0.6577\n",
      "epoch: 1, loss: 0.8340, train acc: 0.6543\n",
      "epoch: 1, loss: 0.8378, train acc: 0.6551\n",
      "epoch: 1, loss: 0.8523, train acc: 0.6483\n",
      "epoch: 1, loss: 0.8593, train acc: 0.6436\n",
      "epoch: 1, loss: 0.8561, train acc: 0.6431\n",
      "epoch: 1, loss: 0.8503, train acc: 0.6436\n",
      "epoch: 1, loss: 0.8587, train acc: 0.6377\n",
      "epoch: 1, loss: 0.8534, train acc: 0.6399\n",
      "epoch: 1, loss: 0.8532, train acc: 0.6412\n",
      "epoch: 1, loss: 0.8474, train acc: 0.6463\n",
      "epoch: 1, loss: 0.8416, train acc: 0.6490\n",
      "epoch: 1, loss: 0.8478, train acc: 0.6514\n",
      ">epoch: 1, val_acc: 0.6596, val_f1: 0.2758\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 2\n",
      "epoch: 2, loss: 1.0091, train acc: 0.5625\n",
      "epoch: 2, loss: 1.0240, train acc: 0.5625\n",
      "epoch: 2, loss: 0.9370, train acc: 0.5865\n",
      "epoch: 2, loss: 0.9098, train acc: 0.6111\n",
      "epoch: 2, loss: 0.9065, train acc: 0.6277\n",
      "epoch: 2, loss: 0.9012, train acc: 0.6339\n",
      "epoch: 2, loss: 0.8821, train acc: 0.6345\n",
      "epoch: 2, loss: 0.8803, train acc: 0.6266\n",
      "epoch: 2, loss: 0.8605, train acc: 0.6424\n",
      "epoch: 2, loss: 0.8679, train acc: 0.6393\n",
      "epoch: 2, loss: 0.8745, train acc: 0.6380\n",
      "epoch: 2, loss: 0.8797, train acc: 0.6390\n",
      "epoch: 2, loss: 0.8839, train acc: 0.6369\n",
      "epoch: 2, loss: 0.8725, train acc: 0.6406\n",
      "epoch: 2, loss: 0.8627, train acc: 0.6498\n",
      "epoch: 2, loss: 0.8675, train acc: 0.6506\n",
      "epoch: 2, loss: 0.8546, train acc: 0.6521\n",
      "epoch: 2, loss: 0.8497, train acc: 0.6534\n",
      "epoch: 2, loss: 0.8536, train acc: 0.6499\n",
      "epoch: 2, loss: 0.8447, train acc: 0.6518\n",
      "epoch: 2, loss: 0.8447, train acc: 0.6505\n",
      ">epoch: 2, val_acc: 0.6596, val_f1: 0.2758\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 3\n",
      "epoch: 3, loss: 0.9304, train acc: 0.6875\n",
      "epoch: 3, loss: 0.9611, train acc: 0.6696\n",
      "epoch: 3, loss: 0.8812, train acc: 0.6771\n",
      "epoch: 3, loss: 0.8202, train acc: 0.6949\n",
      "epoch: 3, loss: 0.8000, train acc: 0.7102\n",
      "epoch: 3, loss: 0.7908, train acc: 0.7106\n",
      "epoch: 3, loss: 0.8078, train acc: 0.7012\n",
      "epoch: 3, loss: 0.8153, train acc: 0.6959\n",
      "epoch: 3, loss: 0.8152, train acc: 0.6905\n",
      "epoch: 3, loss: 0.8238, train acc: 0.6835\n",
      "epoch: 3, loss: 0.8179, train acc: 0.6755\n",
      "epoch: 3, loss: 0.8157, train acc: 0.6754\n",
      "epoch: 3, loss: 0.8068, train acc: 0.6784\n",
      "epoch: 3, loss: 0.8009, train acc: 0.6782\n",
      "epoch: 3, loss: 0.8102, train acc: 0.6736\n",
      "epoch: 3, loss: 0.8115, train acc: 0.6713\n",
      "epoch: 3, loss: 0.8090, train acc: 0.6639\n",
      "epoch: 3, loss: 0.8109, train acc: 0.6588\n",
      "epoch: 3, loss: 0.8221, train acc: 0.6522\n",
      "epoch: 3, loss: 0.8226, train acc: 0.6488\n",
      "epoch: 3, loss: 0.8164, train acc: 0.6489\n",
      ">epoch: 3, val_acc: 0.6596, val_f1: 0.2758\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 4\n",
      "epoch: 4, loss: 0.8690, train acc: 0.7500\n",
      "epoch: 4, loss: 0.8586, train acc: 0.7083\n",
      "epoch: 4, loss: 0.8175, train acc: 0.6932\n",
      "epoch: 4, loss: 0.8346, train acc: 0.7031\n",
      "epoch: 4, loss: 0.8527, train acc: 0.6964\n",
      "epoch: 4, loss: 0.8478, train acc: 0.6875\n",
      "epoch: 4, loss: 0.8362, train acc: 0.6835\n",
      "epoch: 4, loss: 0.8169, train acc: 0.6875\n",
      "epoch: 4, loss: 0.7902, train acc: 0.6951\n",
      "epoch: 4, loss: 0.7897, train acc: 0.6943\n",
      "epoch: 4, loss: 0.7858, train acc: 0.6949\n",
      "epoch: 4, loss: 0.7946, train acc: 0.6875\n",
      "epoch: 4, loss: 0.7818, train acc: 0.6895\n",
      "epoch: 4, loss: 0.7821, train acc: 0.6884\n",
      "epoch: 4, loss: 0.7991, train acc: 0.6796\n",
      "epoch: 4, loss: 0.8028, train acc: 0.6768\n",
      "epoch: 4, loss: 0.8053, train acc: 0.6744\n",
      "epoch: 4, loss: 0.8051, train acc: 0.6751\n",
      "epoch: 4, loss: 0.8010, train acc: 0.6758\n",
      "epoch: 4, loss: 0.8026, train acc: 0.6719\n",
      "epoch: 4, loss: 0.7996, train acc: 0.6739\n",
      "epoch: 4, loss: 0.7985, train acc: 0.6749\n",
      ">epoch: 4, val_acc: 0.6596, val_f1: 0.2758\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 5\n",
      "epoch: 5, loss: 0.8997, train acc: 0.5875\n",
      "epoch: 5, loss: 0.7684, train acc: 0.6500\n",
      "epoch: 5, loss: 0.8341, train acc: 0.6250\n",
      "epoch: 5, loss: 0.8447, train acc: 0.6031\n",
      "epoch: 5, loss: 0.8473, train acc: 0.6000\n",
      "epoch: 5, loss: 0.8550, train acc: 0.5979\n",
      "epoch: 5, loss: 0.8597, train acc: 0.6000\n",
      "epoch: 5, loss: 0.8523, train acc: 0.6109\n",
      "epoch: 5, loss: 0.8464, train acc: 0.6194\n",
      "epoch: 5, loss: 0.8308, train acc: 0.6250\n",
      "epoch: 5, loss: 0.8145, train acc: 0.6330\n",
      "epoch: 5, loss: 0.8167, train acc: 0.6375\n",
      "epoch: 5, loss: 0.8172, train acc: 0.6346\n",
      "epoch: 5, loss: 0.8167, train acc: 0.6420\n",
      "epoch: 5, loss: 0.8057, train acc: 0.6492\n",
      "epoch: 5, loss: 0.7995, train acc: 0.6523\n",
      "epoch: 5, loss: 0.8051, train acc: 0.6566\n",
      "epoch: 5, loss: 0.7996, train acc: 0.6597\n",
      "epoch: 5, loss: 0.8045, train acc: 0.6586\n",
      "epoch: 5, loss: 0.8064, train acc: 0.6613\n",
      "epoch: 5, loss: 0.8011, train acc: 0.6649\n",
      ">epoch: 5, val_acc: 0.6596, val_f1: 0.2758\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 6\n",
      "epoch: 6, loss: 0.6753, train acc: 0.7969\n",
      "epoch: 6, loss: 0.7855, train acc: 0.7708\n",
      "epoch: 6, loss: 0.7666, train acc: 0.7634\n",
      "epoch: 6, loss: 0.8037, train acc: 0.7336\n",
      "epoch: 6, loss: 0.7854, train acc: 0.7318\n",
      "epoch: 6, loss: 0.7982, train acc: 0.7177\n",
      "epoch: 6, loss: 0.7931, train acc: 0.7077\n",
      "epoch: 6, loss: 0.7922, train acc: 0.7019\n",
      "epoch: 6, loss: 0.7876, train acc: 0.6960\n",
      "epoch: 6, loss: 0.7783, train acc: 0.7003\n",
      "epoch: 6, loss: 0.7854, train acc: 0.6944\n",
      "epoch: 6, loss: 0.8008, train acc: 0.6896\n",
      "epoch: 6, loss: 0.7978, train acc: 0.6904\n",
      "epoch: 6, loss: 0.7972, train acc: 0.6893\n",
      "epoch: 6, loss: 0.7881, train acc: 0.6926\n",
      "epoch: 6, loss: 0.7902, train acc: 0.6899\n",
      "epoch: 6, loss: 0.7870, train acc: 0.6912\n",
      "epoch: 6, loss: 0.7869, train acc: 0.6889\n",
      "epoch: 6, loss: 0.7917, train acc: 0.6848\n",
      "epoch: 6, loss: 0.7844, train acc: 0.6850\n",
      "epoch: 6, loss: 0.7829, train acc: 0.6851\n",
      ">epoch: 6, val_acc: 0.6596, val_f1: 0.2758\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 7\n",
      "epoch: 7, loss: 0.7708, train acc: 0.6875\n",
      "epoch: 7, loss: 0.8268, train acc: 0.6797\n",
      "epoch: 7, loss: 0.7963, train acc: 0.6490\n",
      "epoch: 7, loss: 0.8156, train acc: 0.6285\n",
      "epoch: 7, loss: 0.8037, train acc: 0.6386\n",
      "epoch: 7, loss: 0.8014, train acc: 0.6362\n",
      "epoch: 7, loss: 0.7882, train acc: 0.6402\n",
      "epoch: 7, loss: 0.7890, train acc: 0.6447\n",
      "epoch: 7, loss: 0.7776, train acc: 0.6497\n",
      "epoch: 7, loss: 0.7856, train acc: 0.6484\n",
      "epoch: 7, loss: 0.7764, train acc: 0.6545\n",
      "epoch: 7, loss: 0.7801, train acc: 0.6562\n",
      "epoch: 7, loss: 0.7840, train acc: 0.6597\n",
      "epoch: 7, loss: 0.7778, train acc: 0.6636\n",
      "epoch: 7, loss: 0.7841, train acc: 0.6635\n",
      "epoch: 7, loss: 0.7695, train acc: 0.6707\n",
      "epoch: 7, loss: 0.7678, train acc: 0.6732\n",
      "epoch: 7, loss: 0.7562, train acc: 0.6818\n",
      "epoch: 7, loss: 0.7592, train acc: 0.6828\n",
      "epoch: 7, loss: 0.7586, train acc: 0.6862\n",
      "epoch: 7, loss: 0.7618, train acc: 0.6845\n",
      ">epoch: 7, val_acc: 0.6596, val_f1: 0.2859\n",
      ">> epoch: 7, test_acc: 0.7262, test_f1: 0.2849\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 8\n",
      "epoch: 8, loss: 0.5319, train acc: 0.8125\n",
      "epoch: 8, loss: 0.7775, train acc: 0.7054\n",
      "epoch: 8, loss: 0.7769, train acc: 0.6719\n",
      "epoch: 8, loss: 0.7951, train acc: 0.6618\n",
      "epoch: 8, loss: 0.7796, train acc: 0.6676\n",
      "epoch: 8, loss: 0.7557, train acc: 0.6806\n",
      "epoch: 8, loss: 0.7750, train acc: 0.6680\n",
      "epoch: 8, loss: 0.7841, train acc: 0.6639\n",
      "epoch: 8, loss: 0.7945, train acc: 0.6533\n",
      "epoch: 8, loss: 0.8109, train acc: 0.6516\n",
      "epoch: 8, loss: 0.7984, train acc: 0.6562\n",
      "epoch: 8, loss: 0.7947, train acc: 0.6612\n",
      "epoch: 8, loss: 0.7967, train acc: 0.6633\n",
      "epoch: 8, loss: 0.7831, train acc: 0.6688\n",
      "epoch: 8, loss: 0.7774, train acc: 0.6753\n",
      "epoch: 8, loss: 0.7860, train acc: 0.6753\n",
      "epoch: 8, loss: 0.7868, train acc: 0.6723\n",
      "epoch: 8, loss: 0.7875, train acc: 0.6688\n",
      "epoch: 8, loss: 0.7797, train acc: 0.6698\n",
      "epoch: 8, loss: 0.7704, train acc: 0.6740\n",
      "epoch: 8, loss: 0.7612, train acc: 0.6808\n",
      ">epoch: 8, val_acc: 0.6596, val_f1: 0.2758\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 9\n",
      "epoch: 9, loss: 0.7932, train acc: 0.7500\n",
      "epoch: 9, loss: 0.6951, train acc: 0.7500\n",
      "epoch: 9, loss: 0.7279, train acc: 0.7159\n",
      "epoch: 9, loss: 0.7707, train acc: 0.6836\n",
      "epoch: 9, loss: 0.7533, train acc: 0.6756\n",
      "epoch: 9, loss: 0.7438, train acc: 0.6827\n",
      "epoch: 9, loss: 0.7461, train acc: 0.6956\n",
      "epoch: 9, loss: 0.7518, train acc: 0.6979\n",
      "epoch: 9, loss: 0.7584, train acc: 0.6997\n",
      "epoch: 9, loss: 0.7637, train acc: 0.7011\n",
      "epoch: 9, loss: 0.7701, train acc: 0.7010\n",
      "epoch: 9, loss: 0.7593, train acc: 0.7031\n",
      "epoch: 9, loss: 0.7629, train acc: 0.7008\n",
      "epoch: 9, loss: 0.7517, train acc: 0.7055\n",
      "epoch: 9, loss: 0.7506, train acc: 0.7060\n",
      "epoch: 9, loss: 0.7496, train acc: 0.7039\n",
      "epoch: 9, loss: 0.7582, train acc: 0.6991\n",
      "epoch: 9, loss: 0.7562, train acc: 0.6969\n",
      "epoch: 9, loss: 0.7536, train acc: 0.6985\n",
      "epoch: 9, loss: 0.7645, train acc: 0.6895\n",
      "epoch: 9, loss: 0.7662, train acc: 0.6894\n",
      "epoch: 9, loss: 0.7644, train acc: 0.6903\n",
      ">epoch: 9, val_acc: 0.6649, val_f1: 0.2972\n",
      ">> epoch: 9, test_acc: 0.7138, test_f1: 0.2819\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 10\n",
      "epoch: 10, loss: 0.7273, train acc: 0.6500\n",
      "epoch: 10, loss: 0.6477, train acc: 0.7250\n",
      "epoch: 10, loss: 0.6517, train acc: 0.7375\n",
      "epoch: 10, loss: 0.6554, train acc: 0.7344\n",
      "epoch: 10, loss: 0.6768, train acc: 0.7200\n",
      "epoch: 10, loss: 0.6784, train acc: 0.7167\n",
      "epoch: 10, loss: 0.6821, train acc: 0.7107\n",
      "epoch: 10, loss: 0.7028, train acc: 0.7063\n",
      "epoch: 10, loss: 0.7070, train acc: 0.7000\n",
      "epoch: 10, loss: 0.7189, train acc: 0.7000\n",
      "epoch: 10, loss: 0.7218, train acc: 0.6977\n",
      "epoch: 10, loss: 0.7346, train acc: 0.6948\n",
      "epoch: 10, loss: 0.7365, train acc: 0.6933\n",
      "epoch: 10, loss: 0.7392, train acc: 0.6884\n",
      "epoch: 10, loss: 0.7351, train acc: 0.6900\n",
      "epoch: 10, loss: 0.7385, train acc: 0.6914\n",
      "epoch: 10, loss: 0.7463, train acc: 0.6838\n",
      "epoch: 10, loss: 0.7471, train acc: 0.6813\n",
      "epoch: 10, loss: 0.7473, train acc: 0.6789\n",
      "epoch: 10, loss: 0.7501, train acc: 0.6775\n",
      "epoch: 10, loss: 0.7512, train acc: 0.6786\n",
      ">epoch: 10, val_acc: 0.6649, val_f1: 0.2972\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 11\n",
      "epoch: 11, loss: 0.7514, train acc: 0.6250\n",
      "epoch: 11, loss: 0.7463, train acc: 0.6528\n",
      "epoch: 11, loss: 0.6991, train acc: 0.6920\n",
      "epoch: 11, loss: 0.6959, train acc: 0.7039\n",
      "epoch: 11, loss: 0.7143, train acc: 0.6953\n",
      "epoch: 11, loss: 0.7169, train acc: 0.7047\n",
      "epoch: 11, loss: 0.7348, train acc: 0.6949\n",
      "epoch: 11, loss: 0.7412, train acc: 0.6923\n",
      "epoch: 11, loss: 0.7362, train acc: 0.6960\n",
      "epoch: 11, loss: 0.7335, train acc: 0.6964\n",
      "epoch: 11, loss: 0.7452, train acc: 0.6956\n",
      "epoch: 11, loss: 0.7380, train acc: 0.7002\n",
      "epoch: 11, loss: 0.7467, train acc: 0.6973\n",
      "epoch: 11, loss: 0.7456, train acc: 0.6975\n",
      "epoch: 11, loss: 0.7478, train acc: 0.6993\n",
      "epoch: 11, loss: 0.7451, train acc: 0.6986\n",
      "epoch: 11, loss: 0.7441, train acc: 0.6972\n",
      "epoch: 11, loss: 0.7479, train acc: 0.6980\n",
      "epoch: 11, loss: 0.7484, train acc: 0.6995\n",
      "epoch: 11, loss: 0.7463, train acc: 0.7020\n",
      "epoch: 11, loss: 0.7533, train acc: 0.6995\n",
      ">epoch: 11, val_acc: 0.6543, val_f1: 0.2743\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 12\n",
      "epoch: 12, loss: 0.6094, train acc: 0.7083\n",
      "epoch: 12, loss: 0.7537, train acc: 0.6719\n",
      "epoch: 12, loss: 0.7679, train acc: 0.6538\n",
      "epoch: 12, loss: 0.7605, train acc: 0.6806\n",
      "epoch: 12, loss: 0.7912, train acc: 0.6739\n",
      "epoch: 12, loss: 0.7737, train acc: 0.6719\n",
      "epoch: 12, loss: 0.7729, train acc: 0.6667\n",
      "epoch: 12, loss: 0.7672, train acc: 0.6694\n",
      "epoch: 12, loss: 0.7665, train acc: 0.6642\n",
      "epoch: 12, loss: 0.7672, train acc: 0.6615\n",
      "epoch: 12, loss: 0.7701, train acc: 0.6616\n",
      "epoch: 12, loss: 0.7743, train acc: 0.6638\n",
      "epoch: 12, loss: 0.7654, train acc: 0.6687\n",
      "epoch: 12, loss: 0.7541, train acc: 0.6746\n",
      "epoch: 12, loss: 0.7532, train acc: 0.6764\n",
      "epoch: 12, loss: 0.7488, train acc: 0.6755\n",
      "epoch: 12, loss: 0.7459, train acc: 0.6755\n",
      "epoch: 12, loss: 0.7464, train acc: 0.6783\n",
      "epoch: 12, loss: 0.7517, train acc: 0.6808\n",
      "epoch: 12, loss: 0.7505, train acc: 0.6824\n",
      "epoch: 12, loss: 0.7553, train acc: 0.6790\n",
      ">epoch: 12, val_acc: 0.6543, val_f1: 0.2743\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 13\n",
      "epoch: 13, loss: 0.7187, train acc: 0.7188\n",
      "epoch: 13, loss: 0.8087, train acc: 0.7143\n",
      "epoch: 13, loss: 0.7564, train acc: 0.7292\n",
      "epoch: 13, loss: 0.7741, train acc: 0.7243\n",
      "epoch: 13, loss: 0.7568, train acc: 0.7244\n",
      "epoch: 13, loss: 0.7569, train acc: 0.7083\n",
      "epoch: 13, loss: 0.7589, train acc: 0.6973\n",
      "epoch: 13, loss: 0.7542, train acc: 0.7010\n",
      "epoch: 13, loss: 0.7466, train acc: 0.7054\n",
      "epoch: 13, loss: 0.7572, train acc: 0.7008\n",
      "epoch: 13, loss: 0.7508, train acc: 0.7007\n",
      "epoch: 13, loss: 0.7563, train acc: 0.6974\n",
      "epoch: 13, loss: 0.7596, train acc: 0.6966\n",
      "epoch: 13, loss: 0.7504, train acc: 0.7006\n",
      "epoch: 13, loss: 0.7543, train acc: 0.6988\n",
      "epoch: 13, loss: 0.7596, train acc: 0.6916\n",
      "epoch: 13, loss: 0.7745, train acc: 0.6875\n",
      "epoch: 13, loss: 0.7716, train acc: 0.6904\n",
      "epoch: 13, loss: 0.7663, train acc: 0.6916\n",
      "epoch: 13, loss: 0.7695, train acc: 0.6881\n",
      "epoch: 13, loss: 0.7615, train acc: 0.6924\n",
      ">epoch: 13, val_acc: 0.6543, val_f1: 0.2743\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 14\n",
      "epoch: 14, loss: 0.9792, train acc: 0.5625\n",
      "epoch: 14, loss: 0.8284, train acc: 0.6250\n",
      "epoch: 14, loss: 0.8077, train acc: 0.6648\n",
      "epoch: 14, loss: 0.7794, train acc: 0.6836\n",
      "epoch: 14, loss: 0.7850, train acc: 0.6845\n",
      "epoch: 14, loss: 0.7584, train acc: 0.7043\n",
      "epoch: 14, loss: 0.7431, train acc: 0.7097\n",
      "epoch: 14, loss: 0.7383, train acc: 0.7101\n",
      "epoch: 14, loss: 0.7608, train acc: 0.6966\n",
      "epoch: 14, loss: 0.7493, train acc: 0.6957\n",
      "epoch: 14, loss: 0.7558, train acc: 0.6875\n",
      "epoch: 14, loss: 0.7515, train acc: 0.6886\n",
      "epoch: 14, loss: 0.7462, train acc: 0.6947\n",
      "epoch: 14, loss: 0.7426, train acc: 0.6998\n",
      "epoch: 14, loss: 0.7347, train acc: 0.7025\n",
      "epoch: 14, loss: 0.7288, train acc: 0.7048\n",
      "epoch: 14, loss: 0.7303, train acc: 0.7029\n",
      "epoch: 14, loss: 0.7283, train acc: 0.7049\n",
      "epoch: 14, loss: 0.7334, train acc: 0.7012\n",
      "epoch: 14, loss: 0.7313, train acc: 0.6992\n",
      "epoch: 14, loss: 0.7386, train acc: 0.6943\n",
      "epoch: 14, loss: 0.7437, train acc: 0.6939\n",
      ">epoch: 14, val_acc: 0.6649, val_f1: 0.2972\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 15\n",
      "epoch: 15, loss: 0.7806, train acc: 0.6750\n",
      "epoch: 15, loss: 0.8061, train acc: 0.6562\n",
      "epoch: 15, loss: 0.7782, train acc: 0.6833\n",
      "epoch: 15, loss: 0.7653, train acc: 0.6906\n",
      "epoch: 15, loss: 0.8256, train acc: 0.6675\n",
      "epoch: 15, loss: 0.8101, train acc: 0.6708\n",
      "epoch: 15, loss: 0.8092, train acc: 0.6732\n",
      "epoch: 15, loss: 0.7810, train acc: 0.6797\n",
      "epoch: 15, loss: 0.7855, train acc: 0.6764\n",
      "epoch: 15, loss: 0.7732, train acc: 0.6800\n",
      "epoch: 15, loss: 0.7648, train acc: 0.6852\n",
      "epoch: 15, loss: 0.7546, train acc: 0.6927\n",
      "epoch: 15, loss: 0.7537, train acc: 0.6962\n",
      "epoch: 15, loss: 0.7497, train acc: 0.6982\n",
      "epoch: 15, loss: 0.7417, train acc: 0.7000\n",
      "epoch: 15, loss: 0.7491, train acc: 0.6992\n",
      "epoch: 15, loss: 0.7522, train acc: 0.6993\n",
      "epoch: 15, loss: 0.7497, train acc: 0.7000\n",
      "epoch: 15, loss: 0.7507, train acc: 0.6954\n",
      "epoch: 15, loss: 0.7500, train acc: 0.6944\n",
      "epoch: 15, loss: 0.7466, train acc: 0.6970\n",
      ">epoch: 15, val_acc: 0.6596, val_f1: 0.2859\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 16\n",
      "epoch: 16, loss: 0.8100, train acc: 0.5625\n",
      "epoch: 16, loss: 0.7620, train acc: 0.6667\n",
      "epoch: 16, loss: 0.8229, train acc: 0.6339\n",
      "epoch: 16, loss: 0.8125, train acc: 0.6546\n",
      "epoch: 16, loss: 0.7795, train acc: 0.6589\n",
      "epoch: 16, loss: 0.7742, train acc: 0.6659\n",
      "epoch: 16, loss: 0.7555, train acc: 0.6746\n",
      "epoch: 16, loss: 0.7454, train acc: 0.6859\n",
      "epoch: 16, loss: 0.7306, train acc: 0.6946\n",
      "epoch: 16, loss: 0.7127, train acc: 0.7015\n",
      "epoch: 16, loss: 0.7249, train acc: 0.6956\n",
      "epoch: 16, loss: 0.7194, train acc: 0.7002\n",
      "epoch: 16, loss: 0.7135, train acc: 0.7031\n",
      "epoch: 16, loss: 0.7108, train acc: 0.7029\n",
      "epoch: 16, loss: 0.7408, train acc: 0.6951\n",
      "epoch: 16, loss: 0.7485, train acc: 0.6930\n",
      "epoch: 16, loss: 0.7573, train acc: 0.6890\n",
      "epoch: 16, loss: 0.7590, train acc: 0.6882\n",
      "epoch: 16, loss: 0.7602, train acc: 0.6855\n",
      "epoch: 16, loss: 0.7558, train acc: 0.6862\n",
      "epoch: 16, loss: 0.7507, train acc: 0.6893\n",
      ">epoch: 16, val_acc: 0.6543, val_f1: 0.2743\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 17\n",
      "epoch: 17, loss: 0.6046, train acc: 0.7708\n",
      "epoch: 17, loss: 0.6354, train acc: 0.7500\n",
      "epoch: 17, loss: 0.7096, train acc: 0.6923\n",
      "epoch: 17, loss: 0.7041, train acc: 0.7014\n",
      "epoch: 17, loss: 0.7207, train acc: 0.6848\n",
      "epoch: 17, loss: 0.7260, train acc: 0.6987\n",
      "epoch: 17, loss: 0.7356, train acc: 0.6856\n",
      "epoch: 17, loss: 0.7427, train acc: 0.6826\n",
      "epoch: 17, loss: 0.7482, train acc: 0.6773\n",
      "epoch: 17, loss: 0.7325, train acc: 0.6901\n",
      "epoch: 17, loss: 0.7274, train acc: 0.6946\n",
      "epoch: 17, loss: 0.7362, train acc: 0.6940\n",
      "epoch: 17, loss: 0.7292, train acc: 0.6944\n",
      "epoch: 17, loss: 0.7250, train acc: 0.6994\n",
      "epoch: 17, loss: 0.7300, train acc: 0.6986\n",
      "epoch: 17, loss: 0.7387, train acc: 0.6955\n",
      "epoch: 17, loss: 0.7398, train acc: 0.6950\n",
      "epoch: 17, loss: 0.7417, train acc: 0.6939\n",
      "epoch: 17, loss: 0.7444, train acc: 0.6909\n",
      "epoch: 17, loss: 0.7377, train acc: 0.6939\n",
      "epoch: 17, loss: 0.7344, train acc: 0.6936\n",
      ">epoch: 17, val_acc: 0.6596, val_f1: 0.2859\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 18\n",
      "epoch: 18, loss: 0.7182, train acc: 0.6562\n",
      "epoch: 18, loss: 0.7861, train acc: 0.6518\n",
      "epoch: 18, loss: 0.7242, train acc: 0.6667\n",
      "epoch: 18, loss: 0.7433, train acc: 0.6765\n",
      "epoch: 18, loss: 0.7453, train acc: 0.6790\n",
      "epoch: 18, loss: 0.7685, train acc: 0.6644\n",
      "epoch: 18, loss: 0.7730, train acc: 0.6719\n",
      "epoch: 18, loss: 0.7457, train acc: 0.6807\n",
      "epoch: 18, loss: 0.7504, train acc: 0.6875\n",
      "epoch: 18, loss: 0.7448, train acc: 0.6902\n",
      "epoch: 18, loss: 0.7433, train acc: 0.6899\n",
      "epoch: 18, loss: 0.7475, train acc: 0.6864\n",
      "epoch: 18, loss: 0.7494, train acc: 0.6875\n",
      "epoch: 18, loss: 0.7591, train acc: 0.6856\n",
      "epoch: 18, loss: 0.7537, train acc: 0.6858\n",
      "epoch: 18, loss: 0.7561, train acc: 0.6834\n",
      "epoch: 18, loss: 0.7451, train acc: 0.6913\n",
      "epoch: 18, loss: 0.7440, train acc: 0.6925\n",
      "epoch: 18, loss: 0.7428, train acc: 0.6950\n",
      "epoch: 18, loss: 0.7419, train acc: 0.6985\n",
      "epoch: 18, loss: 0.7458, train acc: 0.6973\n",
      ">epoch: 18, val_acc: 0.6649, val_f1: 0.2972\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 19\n",
      "epoch: 19, loss: 0.6943, train acc: 0.7500\n",
      "epoch: 19, loss: 0.7125, train acc: 0.7188\n",
      "epoch: 19, loss: 0.7422, train acc: 0.6875\n",
      "epoch: 19, loss: 0.7387, train acc: 0.6875\n",
      "epoch: 19, loss: 0.7426, train acc: 0.6786\n",
      "epoch: 19, loss: 0.7426, train acc: 0.6875\n",
      "epoch: 19, loss: 0.7469, train acc: 0.6815\n",
      "epoch: 19, loss: 0.7518, train acc: 0.6788\n",
      "epoch: 19, loss: 0.7460, train acc: 0.6875\n",
      "epoch: 19, loss: 0.7390, train acc: 0.6929\n",
      "epoch: 19, loss: 0.7463, train acc: 0.6912\n",
      "epoch: 19, loss: 0.7337, train acc: 0.7009\n",
      "epoch: 19, loss: 0.7300, train acc: 0.7029\n",
      "epoch: 19, loss: 0.7355, train acc: 0.7027\n",
      "epoch: 19, loss: 0.7376, train acc: 0.6998\n",
      "epoch: 19, loss: 0.7395, train acc: 0.6990\n",
      "epoch: 19, loss: 0.7365, train acc: 0.7006\n",
      "epoch: 19, loss: 0.7343, train acc: 0.7020\n",
      "epoch: 19, loss: 0.7386, train acc: 0.6985\n",
      "epoch: 19, loss: 0.7448, train acc: 0.6960\n",
      "epoch: 19, loss: 0.7420, train acc: 0.6955\n",
      "epoch: 19, loss: 0.7362, train acc: 0.6986\n",
      ">epoch: 19, val_acc: 0.6596, val_f1: 0.2859\n",
      "Namespace(model_type='tri_gcn', modules={'tgcn': True, 'semgcn': True, 'lexgcn': True}, num_layers={'tgcn': 2, 'semgcn': 2, 'lexgcn': 2}, year='2016', train_file='data/train2016restaurant.txt', test_file='data/test2016restaurant.txt', model_path='test_models/2016tri_gcn+concat_seed449_reg0.0005_drop0.4_cdrop0.4_lr2.5e-05_epochs20_adam', val_file='val.txt', log='log', bert_model='bert_large_uncased', cooc_path='cooc_matrix.csv', cooc=       100       1000   10000  10003     10005     10007     10009     1001   \\\n",
      "100      0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "1000     0.0  0.000000    0.0    0.0  0.000606  0.000303  0.000606  0.035120   \n",
      "10000    0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.333333   \n",
      "10003    0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "10005    0.0  0.333333    0.0    0.0  0.000000  0.166667  0.000000  0.166667   \n",
      "...      ...       ...    ...    ...       ...       ...       ...       ...   \n",
      "9992     0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "9993     0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "9996     0.0  0.500000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "9997     0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "9998     0.0  0.400000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "       10010  10015  ...     9986      9987      999    9990      9991   \\\n",
      "100      0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "1000     0.0    0.0  ...  0.002422  0.000303  0.062368    0.0  0.000606   \n",
      "10000    0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "10003    0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "10005    0.0    0.0  ...  0.000000  0.000000  0.333333    0.0  0.000000   \n",
      "...      ...    ...  ...       ...       ...       ...    ...       ...   \n",
      "9992     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "9993     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "9996     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "9997     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "9998     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "\n",
      "       9992   9993      9996   9997      9998   \n",
      "100      0.0    0.0  0.000000    0.0  0.000000  \n",
      "1000     0.0    0.0  0.000606    0.0  0.001211  \n",
      "10000    0.0    0.0  0.000000    0.0  0.000000  \n",
      "10003    0.0    0.0  0.000000    0.0  0.000000  \n",
      "10005    0.0    0.0  0.000000    0.0  0.000000  \n",
      "...      ...    ...       ...    ...       ...  \n",
      "9992     0.0    0.0  0.000000    0.0  0.000000  \n",
      "9993     0.0    0.0  0.000000    0.0  0.000000  \n",
      "9996     0.0    0.0  0.000000    0.0  0.000000  \n",
      "9997     0.0    0.0  0.000000    0.0  0.000000  \n",
      "9998     0.0    0.0  0.000000    0.0  0.000000  \n",
      "\n",
      "[13075 rows x 13075 columns], learning_rate=2.5e-05, dropout=0.4, concat_dropout=0.4, bert_dropout=0.2, l2reg=0.0005, num_epoch=20, batch_size=16, log_step=5, max_seq_len=100, polarities_dim=3, device=device(type='cuda'), seed=449, valset_ratio=0.1, do_train=True, do_eval=True, eval_epoch_num=0, fusion_type='concat', use_ensemble=True, save_models='none', print_sent=False, optim='adam', n_gpu=1)\n",
      "{'none': 0, 'acl': 1, 'acl:relcl': 2, 'advcl': 3, 'advcl:relcl': 4, 'advmod': 5, 'amod': 6, 'appos': 7, 'aux': 8, 'aux:pass': 9, 'case': 10, 'cc': 11, 'cc:preconj': 12, 'ccomp': 13, 'compound': 14, 'compound:prt': 15, 'conj': 16, 'cop': 17, 'csubj': 18, 'dep': 19, 'det': 20, 'det:predet': 21, 'discourse': 22, 'dislocated': 23, 'expl': 24, 'fixed': 25, 'flat': 26, 'iobj': 27, 'list': 28, 'mark': 29, 'nmod': 30, 'nmod:npmod': 31, 'nmod:poss': 32, 'nmod:tmod': 33, 'nsubj': 34, 'nsubj:outer': 35, 'nsubj:pass': 36, 'nummod': 37, 'obj': 38, 'obl': 39, 'obl:agent': 40, 'obl:npmod': 41, 'obl:tmod': 42, 'parataxis': 43, 'punct': 44, 'reparandum': 45, 'root': 46, 'vocative': 47, 'xcomp': 48}\n",
      "{'-1': 0, '0': 1, '1': 2}\n",
      "Model name 'bert_large_uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming 'bert_large_uncased' is a path or url to a directory containing tokenizer files.\n",
      "Didn't find file bert_large_uncased/added_tokens.json. We won't load it.\n",
      "Didn't find file bert_large_uncased/special_tokens_map.json. We won't load it.\n",
      "Didn't find file bert_large_uncased/tokenizer_config.json. We won't load it.\n",
      "loading file bert_large_uncased/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "{\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 3,\n",
      "  \"num_types\": 49,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file bert_large_uncased/pytorch_model.bin\n",
      "Weights of AsaTgcnSem not initialized from pretrained model: ['gate_weight', 'gate_bias', 'ensemble', 'TGCNLayers.0.weight', 'TGCNLayers.0.bias', 'TGCNLayers.0.dense.weight', 'TGCNLayers.1.weight', 'TGCNLayers.1.bias', 'TGCNLayers.1.dense.weight', 'SemGCNLayers.0.weight', 'SemGCNLayers.0.bias', 'SemGCNLayers.1.weight', 'SemGCNLayers.1.bias', 'LexGCNLayers.0.weight', 'LexGCNLayers.0.bias', 'LexGCNLayers.1.weight', 'LexGCNLayers.1.bias', 'fc_single.weight', 'fc_single.bias', 'ensemble_linear_tgcn.weight', 'ensemble_linear_tgcn.bias', 'ensemble_linear_semgcn.weight', 'ensemble_linear_semgcn.bias', 'ensemble_linear_lexgcn.weight', 'ensemble_linear_lexgcn.bias', 'dep_embedding.weight']\n",
      "Weights from pretrained model not used in AsaTgcnSem: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "[{'input_ids': tensor([  101, 13325,  2013,  3025,  8466,  2023,  2109,  2000,  2022,  1037,\n",
      "         2204,  2173,  1010,  2021,  2025,  2151,  2936,  1012,   102,  2173,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'valid_ids': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'segment_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'mem_valid_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'dep_adj_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'dep_value_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'polarity': 0, 'raw_text': 'judging from previous posts this used to be a good place , but not any longer .', 'aspect': 'place'}]\n",
      "[{'input_ids': tensor([  101, 13325,  2013,  3025,  8466,  2023,  2109,  2000,  2022,  1037,\n",
      "         2204,  2173,  1010,  2021,  2025,  2151,  2936,  1012,   102,  2173,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'valid_ids': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'segment_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'mem_valid_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'dep_adj_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'dep_value_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'polarity': 0, 'raw_text': 'judging from previous posts this used to be a good place , but not any longer .', 'aspect': 'place'}]\n",
      "[{'input_ids': tensor([  101,  4240,  2428,  2204, 10514,  6182,  1012,   102, 10514,  6182,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'valid_ids': tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'segment_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'mem_valid_ids': tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'dep_adj_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'dep_value_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 5,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'polarity': 2, 'raw_text': 'serves really good sushi .', 'aspect': 'sushi'}]\n",
      "cuda memory allocated: 1400893440\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 0\n",
      "epoch: 0, loss: 0.7874, train acc: 0.6875\n",
      "epoch: 0, loss: 0.7212, train acc: 0.7188\n",
      "epoch: 0, loss: 0.7748, train acc: 0.7167\n",
      "epoch: 0, loss: 0.7780, train acc: 0.6969\n",
      "epoch: 0, loss: 0.7903, train acc: 0.6875\n",
      "epoch: 0, loss: 0.7991, train acc: 0.6792\n",
      "epoch: 0, loss: 0.8030, train acc: 0.6786\n",
      "epoch: 0, loss: 0.8206, train acc: 0.6703\n",
      "epoch: 0, loss: 0.7917, train acc: 0.6875\n",
      "epoch: 0, loss: 0.7935, train acc: 0.6900\n",
      "epoch: 0, loss: 0.8003, train acc: 0.6886\n",
      "epoch: 0, loss: 0.7983, train acc: 0.6813\n",
      "epoch: 0, loss: 0.7959, train acc: 0.6837\n",
      "epoch: 0, loss: 0.7939, train acc: 0.6839\n",
      "epoch: 0, loss: 0.8023, train acc: 0.6792\n",
      "epoch: 0, loss: 0.7918, train acc: 0.6844\n",
      "epoch: 0, loss: 0.7858, train acc: 0.6875\n",
      "epoch: 0, loss: 0.7898, train acc: 0.6833\n",
      "epoch: 0, loss: 0.7944, train acc: 0.6783\n",
      "epoch: 0, loss: 0.7906, train acc: 0.6725\n",
      "epoch: 0, loss: 0.7902, train acc: 0.6702\n",
      ">epoch: 0, val_acc: 0.7234, val_f1: 0.2798\n",
      ">> epoch: 0, test_acc: 0.7431, test_f1: 0.2842\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 1\n",
      "epoch: 1, loss: 0.6860, train acc: 0.7031\n",
      "epoch: 1, loss: 0.6763, train acc: 0.7292\n",
      "epoch: 1, loss: 0.6836, train acc: 0.7321\n",
      "epoch: 1, loss: 0.7024, train acc: 0.7270\n",
      "epoch: 1, loss: 0.7312, train acc: 0.7188\n",
      "epoch: 1, loss: 0.7447, train acc: 0.7069\n",
      "epoch: 1, loss: 0.7550, train acc: 0.6985\n",
      "epoch: 1, loss: 0.7697, train acc: 0.6923\n",
      "epoch: 1, loss: 0.7685, train acc: 0.6989\n",
      "epoch: 1, loss: 0.7683, train acc: 0.7015\n",
      "epoch: 1, loss: 0.7636, train acc: 0.7049\n",
      "epoch: 1, loss: 0.7669, train acc: 0.7023\n",
      "epoch: 1, loss: 0.7666, train acc: 0.7031\n",
      "epoch: 1, loss: 0.7773, train acc: 0.6902\n",
      "epoch: 1, loss: 0.7817, train acc: 0.6833\n",
      "epoch: 1, loss: 0.7785, train acc: 0.6851\n",
      "epoch: 1, loss: 0.7833, train acc: 0.6815\n",
      "epoch: 1, loss: 0.7825, train acc: 0.6805\n",
      "epoch: 1, loss: 0.7825, train acc: 0.6782\n",
      "epoch: 1, loss: 0.7781, train acc: 0.6787\n",
      "epoch: 1, loss: 0.7775, train acc: 0.6761\n",
      ">epoch: 1, val_acc: 0.7234, val_f1: 0.2798\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 2\n",
      "epoch: 2, loss: 0.8149, train acc: 0.5625\n",
      "epoch: 2, loss: 0.7694, train acc: 0.6250\n",
      "epoch: 2, loss: 0.7494, train acc: 0.6587\n",
      "epoch: 2, loss: 0.7262, train acc: 0.6875\n",
      "epoch: 2, loss: 0.7928, train acc: 0.6739\n",
      "epoch: 2, loss: 0.7916, train acc: 0.6696\n",
      "epoch: 2, loss: 0.8170, train acc: 0.6420\n",
      "epoch: 2, loss: 0.8036, train acc: 0.6464\n",
      "epoch: 2, loss: 0.8065, train acc: 0.6570\n",
      "epoch: 2, loss: 0.7949, train acc: 0.6667\n",
      "epoch: 2, loss: 0.7805, train acc: 0.6757\n",
      "epoch: 2, loss: 0.7753, train acc: 0.6789\n",
      "epoch: 2, loss: 0.7790, train acc: 0.6776\n",
      "epoch: 2, loss: 0.7705, train acc: 0.6838\n",
      "epoch: 2, loss: 0.7728, train acc: 0.6841\n",
      "epoch: 2, loss: 0.7761, train acc: 0.6827\n",
      "epoch: 2, loss: 0.7752, train acc: 0.6852\n",
      "epoch: 2, loss: 0.7785, train acc: 0.6832\n",
      "epoch: 2, loss: 0.7762, train acc: 0.6815\n",
      "epoch: 2, loss: 0.7739, train acc: 0.6811\n",
      "epoch: 2, loss: 0.7775, train acc: 0.6778\n",
      ">epoch: 2, val_acc: 0.7234, val_f1: 0.2798\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 3\n",
      "epoch: 3, loss: 0.6430, train acc: 0.7188\n",
      "epoch: 3, loss: 0.7078, train acc: 0.6964\n",
      "epoch: 3, loss: 0.7499, train acc: 0.6667\n",
      "epoch: 3, loss: 0.7176, train acc: 0.6985\n",
      "epoch: 3, loss: 0.7074, train acc: 0.7102\n",
      "epoch: 3, loss: 0.7193, train acc: 0.7106\n",
      "epoch: 3, loss: 0.7148, train acc: 0.7129\n",
      "epoch: 3, loss: 0.7134, train acc: 0.7128\n",
      "epoch: 3, loss: 0.7210, train acc: 0.7024\n",
      "epoch: 3, loss: 0.7203, train acc: 0.7008\n",
      "epoch: 3, loss: 0.7272, train acc: 0.6983\n",
      "epoch: 3, loss: 0.7461, train acc: 0.6853\n",
      "epoch: 3, loss: 0.7619, train acc: 0.6794\n",
      "epoch: 3, loss: 0.7565, train acc: 0.6856\n",
      "epoch: 3, loss: 0.7541, train acc: 0.6892\n",
      "epoch: 3, loss: 0.7608, train acc: 0.6875\n",
      "epoch: 3, loss: 0.7533, train acc: 0.6928\n",
      "epoch: 3, loss: 0.7533, train acc: 0.6925\n",
      "epoch: 3, loss: 0.7508, train acc: 0.6929\n",
      "epoch: 3, loss: 0.7534, train acc: 0.6907\n",
      "epoch: 3, loss: 0.7546, train acc: 0.6869\n",
      ">epoch: 3, val_acc: 0.7234, val_f1: 0.2798\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 4\n",
      "epoch: 4, loss: 0.8298, train acc: 0.7500\n",
      "epoch: 4, loss: 0.6957, train acc: 0.6979\n",
      "epoch: 4, loss: 0.6625, train acc: 0.7386\n",
      "epoch: 4, loss: 0.7614, train acc: 0.6992\n",
      "epoch: 4, loss: 0.8028, train acc: 0.6696\n",
      "epoch: 4, loss: 0.7989, train acc: 0.6490\n",
      "epoch: 4, loss: 0.7986, train acc: 0.6411\n",
      "epoch: 4, loss: 0.7806, train acc: 0.6510\n",
      "epoch: 4, loss: 0.7852, train acc: 0.6524\n",
      "epoch: 4, loss: 0.7624, train acc: 0.6685\n",
      "epoch: 4, loss: 0.7547, train acc: 0.6740\n",
      "epoch: 4, loss: 0.7486, train acc: 0.6752\n",
      "epoch: 4, loss: 0.7499, train acc: 0.6732\n",
      "epoch: 4, loss: 0.7570, train acc: 0.6695\n",
      "epoch: 4, loss: 0.7554, train acc: 0.6690\n",
      "epoch: 4, loss: 0.7474, train acc: 0.6735\n",
      "epoch: 4, loss: 0.7407, train acc: 0.6790\n",
      "epoch: 4, loss: 0.7380, train acc: 0.6860\n",
      "epoch: 4, loss: 0.7482, train acc: 0.6834\n",
      "epoch: 4, loss: 0.7468, train acc: 0.6829\n",
      "epoch: 4, loss: 0.7459, train acc: 0.6832\n",
      "epoch: 4, loss: 0.7496, train acc: 0.6838\n",
      ">epoch: 4, val_acc: 0.7234, val_f1: 0.2798\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 5\n",
      "epoch: 5, loss: 0.7548, train acc: 0.5875\n",
      "epoch: 5, loss: 0.7370, train acc: 0.6062\n",
      "epoch: 5, loss: 0.7173, train acc: 0.6458\n",
      "epoch: 5, loss: 0.7296, train acc: 0.6594\n",
      "epoch: 5, loss: 0.7458, train acc: 0.6650\n",
      "epoch: 5, loss: 0.7695, train acc: 0.6562\n",
      "epoch: 5, loss: 0.7594, train acc: 0.6661\n",
      "epoch: 5, loss: 0.7392, train acc: 0.6781\n",
      "epoch: 5, loss: 0.7390, train acc: 0.6819\n",
      "epoch: 5, loss: 0.7369, train acc: 0.6913\n",
      "epoch: 5, loss: 0.7377, train acc: 0.6932\n",
      "epoch: 5, loss: 0.7374, train acc: 0.6937\n",
      "epoch: 5, loss: 0.7490, train acc: 0.6894\n",
      "epoch: 5, loss: 0.7439, train acc: 0.6911\n",
      "epoch: 5, loss: 0.7463, train acc: 0.6900\n",
      "epoch: 5, loss: 0.7453, train acc: 0.6867\n",
      "epoch: 5, loss: 0.7525, train acc: 0.6860\n",
      "epoch: 5, loss: 0.7583, train acc: 0.6847\n",
      "epoch: 5, loss: 0.7590, train acc: 0.6803\n",
      "epoch: 5, loss: 0.7565, train acc: 0.6781\n",
      "epoch: 5, loss: 0.7537, train acc: 0.6821\n",
      ">epoch: 5, val_acc: 0.7234, val_f1: 0.2798\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 6\n",
      "epoch: 6, loss: 0.6997, train acc: 0.7500\n",
      "epoch: 6, loss: 0.7060, train acc: 0.7431\n",
      "epoch: 6, loss: 0.7179, train acc: 0.7232\n",
      "epoch: 6, loss: 0.7039, train acc: 0.7303\n",
      "epoch: 6, loss: 0.7140, train acc: 0.7240\n",
      "epoch: 6, loss: 0.6968, train acc: 0.7306\n",
      "epoch: 6, loss: 0.7136, train acc: 0.7114\n",
      "epoch: 6, loss: 0.7262, train acc: 0.7019\n",
      "epoch: 6, loss: 0.7311, train acc: 0.6946\n",
      "epoch: 6, loss: 0.7200, train acc: 0.7003\n",
      "epoch: 6, loss: 0.7248, train acc: 0.7014\n",
      "epoch: 6, loss: 0.7277, train acc: 0.7013\n",
      "epoch: 6, loss: 0.7314, train acc: 0.7012\n",
      "epoch: 6, loss: 0.7268, train acc: 0.7065\n",
      "epoch: 6, loss: 0.7181, train acc: 0.7095\n",
      "epoch: 6, loss: 0.7264, train acc: 0.7081\n",
      "epoch: 6, loss: 0.7347, train acc: 0.7024\n",
      "epoch: 6, loss: 0.7361, train acc: 0.7015\n",
      "epoch: 6, loss: 0.7406, train acc: 0.6988\n",
      "epoch: 6, loss: 0.7441, train acc: 0.6944\n",
      "epoch: 6, loss: 0.7479, train acc: 0.6935\n",
      ">epoch: 6, val_acc: 0.7234, val_f1: 0.2798\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 7\n",
      "epoch: 7, loss: 0.7200, train acc: 0.7083\n",
      "epoch: 7, loss: 0.6390, train acc: 0.7500\n",
      "epoch: 7, loss: 0.6780, train acc: 0.7212\n",
      "epoch: 7, loss: 0.6918, train acc: 0.6979\n",
      "epoch: 7, loss: 0.7099, train acc: 0.6875\n",
      "epoch: 7, loss: 0.6991, train acc: 0.6853\n",
      "epoch: 7, loss: 0.7226, train acc: 0.6705\n",
      "epoch: 7, loss: 0.6944, train acc: 0.6908\n",
      "epoch: 7, loss: 0.6924, train acc: 0.6962\n",
      "epoch: 7, loss: 0.6899, train acc: 0.7018\n",
      "epoch: 7, loss: 0.7068, train acc: 0.6993\n",
      "epoch: 7, loss: 0.7181, train acc: 0.6950\n",
      "epoch: 7, loss: 0.7280, train acc: 0.6875\n",
      "epoch: 7, loss: 0.7306, train acc: 0.6866\n",
      "epoch: 7, loss: 0.7410, train acc: 0.6798\n",
      "epoch: 7, loss: 0.7336, train acc: 0.6891\n",
      "epoch: 7, loss: 0.7353, train acc: 0.6875\n",
      "epoch: 7, loss: 0.7407, train acc: 0.6875\n",
      "epoch: 7, loss: 0.7390, train acc: 0.6902\n",
      "epoch: 7, loss: 0.7439, train acc: 0.6894\n",
      "epoch: 7, loss: 0.7431, train acc: 0.6875\n",
      ">epoch: 7, val_acc: 0.7181, val_f1: 0.2786\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 8\n",
      "epoch: 8, loss: 0.8945, train acc: 0.5000\n",
      "epoch: 8, loss: 0.9170, train acc: 0.5625\n",
      "epoch: 8, loss: 0.8358, train acc: 0.6042\n",
      "epoch: 8, loss: 0.7944, train acc: 0.6360\n",
      "epoch: 8, loss: 0.7723, train acc: 0.6562\n",
      "epoch: 8, loss: 0.7875, train acc: 0.6528\n",
      "epoch: 8, loss: 0.7889, train acc: 0.6504\n",
      "epoch: 8, loss: 0.7705, train acc: 0.6655\n",
      "epoch: 8, loss: 0.7640, train acc: 0.6696\n",
      "epoch: 8, loss: 0.7600, train acc: 0.6742\n",
      "epoch: 8, loss: 0.7643, train acc: 0.6755\n",
      "epoch: 8, loss: 0.7448, train acc: 0.6842\n",
      "epoch: 8, loss: 0.7576, train acc: 0.6835\n",
      "epoch: 8, loss: 0.7486, train acc: 0.6912\n",
      "epoch: 8, loss: 0.7507, train acc: 0.6884\n",
      "epoch: 8, loss: 0.7538, train acc: 0.6867\n",
      "epoch: 8, loss: 0.7509, train acc: 0.6883\n",
      "epoch: 8, loss: 0.7495, train acc: 0.6853\n",
      "epoch: 8, loss: 0.7476, train acc: 0.6889\n",
      "epoch: 8, loss: 0.7500, train acc: 0.6862\n",
      "epoch: 8, loss: 0.7443, train acc: 0.6893\n",
      ">epoch: 8, val_acc: 0.7234, val_f1: 0.2798\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 9\n",
      "epoch: 9, loss: 0.8464, train acc: 0.6250\n",
      "epoch: 9, loss: 0.9072, train acc: 0.5729\n",
      "epoch: 9, loss: 0.8312, train acc: 0.6364\n",
      "epoch: 9, loss: 0.8204, train acc: 0.6367\n",
      "epoch: 9, loss: 0.7738, train acc: 0.6667\n",
      "epoch: 9, loss: 0.7525, train acc: 0.6731\n",
      "epoch: 9, loss: 0.7708, train acc: 0.6673\n",
      "epoch: 9, loss: 0.7714, train acc: 0.6701\n",
      "epoch: 9, loss: 0.7479, train acc: 0.6799\n",
      "epoch: 9, loss: 0.7437, train acc: 0.6807\n",
      "epoch: 9, loss: 0.7521, train acc: 0.6814\n",
      "epoch: 9, loss: 0.7556, train acc: 0.6786\n",
      "epoch: 9, loss: 0.7587, train acc: 0.6793\n",
      "epoch: 9, loss: 0.7586, train acc: 0.6761\n",
      "epoch: 9, loss: 0.7543, train acc: 0.6796\n",
      "epoch: 9, loss: 0.7588, train acc: 0.6743\n",
      "epoch: 9, loss: 0.7587, train acc: 0.6775\n",
      "epoch: 9, loss: 0.7517, train acc: 0.6810\n",
      "epoch: 9, loss: 0.7480, train acc: 0.6841\n",
      "epoch: 9, loss: 0.7513, train acc: 0.6823\n",
      "epoch: 9, loss: 0.7440, train acc: 0.6900\n",
      "epoch: 9, loss: 0.7407, train acc: 0.6933\n",
      ">epoch: 9, val_acc: 0.7234, val_f1: 0.2798\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 10\n",
      "epoch: 10, loss: 0.7796, train acc: 0.6750\n",
      "epoch: 10, loss: 0.7141, train acc: 0.7063\n",
      "epoch: 10, loss: 0.7189, train acc: 0.7042\n",
      "epoch: 10, loss: 0.7438, train acc: 0.7031\n",
      "epoch: 10, loss: 0.7683, train acc: 0.6925\n",
      "epoch: 10, loss: 0.7571, train acc: 0.6896\n",
      "epoch: 10, loss: 0.7609, train acc: 0.6857\n",
      "epoch: 10, loss: 0.7501, train acc: 0.6922\n",
      "epoch: 10, loss: 0.7469, train acc: 0.6986\n",
      "epoch: 10, loss: 0.7584, train acc: 0.6887\n",
      "epoch: 10, loss: 0.7543, train acc: 0.6932\n",
      "epoch: 10, loss: 0.7482, train acc: 0.6979\n",
      "epoch: 10, loss: 0.7434, train acc: 0.7010\n",
      "epoch: 10, loss: 0.7397, train acc: 0.7054\n",
      "epoch: 10, loss: 0.7429, train acc: 0.7017\n",
      "epoch: 10, loss: 0.7388, train acc: 0.7023\n",
      "epoch: 10, loss: 0.7334, train acc: 0.7059\n",
      "epoch: 10, loss: 0.7305, train acc: 0.7049\n",
      "epoch: 10, loss: 0.7389, train acc: 0.6993\n",
      "epoch: 10, loss: 0.7358, train acc: 0.7006\n",
      "epoch: 10, loss: 0.7347, train acc: 0.7012\n",
      ">epoch: 10, val_acc: 0.7234, val_f1: 0.2798\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 11\n",
      "epoch: 11, loss: 0.7075, train acc: 0.7031\n",
      "epoch: 11, loss: 0.7550, train acc: 0.6875\n",
      "epoch: 11, loss: 0.7876, train acc: 0.6964\n",
      "epoch: 11, loss: 0.7765, train acc: 0.7007\n",
      "epoch: 11, loss: 0.7821, train acc: 0.6953\n",
      "epoch: 11, loss: 0.7622, train acc: 0.7026\n",
      "epoch: 11, loss: 0.7527, train acc: 0.7077\n",
      "epoch: 11, loss: 0.7559, train acc: 0.7019\n",
      "epoch: 11, loss: 0.7536, train acc: 0.7017\n",
      "epoch: 11, loss: 0.7417, train acc: 0.7041\n",
      "epoch: 11, loss: 0.7426, train acc: 0.7025\n",
      "epoch: 11, loss: 0.7417, train acc: 0.7013\n",
      "epoch: 11, loss: 0.7422, train acc: 0.6973\n",
      "epoch: 11, loss: 0.7350, train acc: 0.7011\n",
      "epoch: 11, loss: 0.7334, train acc: 0.6968\n",
      "epoch: 11, loss: 0.7326, train acc: 0.6954\n",
      "epoch: 11, loss: 0.7296, train acc: 0.6972\n",
      "epoch: 11, loss: 0.7277, train acc: 0.7008\n",
      "epoch: 11, loss: 0.7327, train acc: 0.6988\n",
      "epoch: 11, loss: 0.7323, train acc: 0.7001\n",
      "epoch: 11, loss: 0.7343, train acc: 0.6977\n",
      ">epoch: 11, val_acc: 0.7234, val_f1: 0.2798\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 12\n",
      "epoch: 12, loss: 0.6109, train acc: 0.7292\n",
      "epoch: 12, loss: 0.6650, train acc: 0.6875\n",
      "epoch: 12, loss: 0.6512, train acc: 0.7308\n",
      "epoch: 12, loss: 0.6495, train acc: 0.7222\n",
      "epoch: 12, loss: 0.7173, train acc: 0.7011\n",
      "epoch: 12, loss: 0.7387, train acc: 0.7009\n",
      "epoch: 12, loss: 0.7332, train acc: 0.6970\n",
      "epoch: 12, loss: 0.7283, train acc: 0.7023\n",
      "epoch: 12, loss: 0.7227, train acc: 0.6991\n",
      "epoch: 12, loss: 0.7306, train acc: 0.6914\n",
      "epoch: 12, loss: 0.7282, train acc: 0.6887\n",
      "epoch: 12, loss: 0.7303, train acc: 0.6897\n",
      "epoch: 12, loss: 0.7476, train acc: 0.6815\n",
      "epoch: 12, loss: 0.7446, train acc: 0.6875\n",
      "epoch: 12, loss: 0.7475, train acc: 0.6858\n",
      "epoch: 12, loss: 0.7425, train acc: 0.6867\n",
      "epoch: 12, loss: 0.7367, train acc: 0.6867\n",
      "epoch: 12, loss: 0.7398, train acc: 0.6868\n",
      "epoch: 12, loss: 0.7421, train acc: 0.6848\n",
      "epoch: 12, loss: 0.7374, train acc: 0.6888\n",
      "epoch: 12, loss: 0.7328, train acc: 0.6917\n",
      ">epoch: 12, val_acc: 0.7234, val_f1: 0.2798\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 13\n",
      "epoch: 13, loss: 0.6474, train acc: 0.7500\n",
      "epoch: 13, loss: 0.7521, train acc: 0.7054\n",
      "epoch: 13, loss: 0.7502, train acc: 0.7135\n",
      "epoch: 13, loss: 0.7177, train acc: 0.7206\n",
      "epoch: 13, loss: 0.7421, train acc: 0.7045\n",
      "epoch: 13, loss: 0.7075, train acc: 0.7222\n",
      "epoch: 13, loss: 0.7257, train acc: 0.7207\n",
      "epoch: 13, loss: 0.7330, train acc: 0.7145\n",
      "epoch: 13, loss: 0.7370, train acc: 0.7054\n",
      "epoch: 13, loss: 0.7364, train acc: 0.7061\n",
      "epoch: 13, loss: 0.7394, train acc: 0.7079\n",
      "epoch: 13, loss: 0.7535, train acc: 0.6996\n",
      "epoch: 13, loss: 0.7568, train acc: 0.6935\n",
      "epoch: 13, loss: 0.7583, train acc: 0.6894\n",
      "epoch: 13, loss: 0.7539, train acc: 0.6944\n",
      "epoch: 13, loss: 0.7465, train acc: 0.6997\n",
      "epoch: 13, loss: 0.7425, train acc: 0.7012\n",
      "epoch: 13, loss: 0.7444, train acc: 0.7004\n",
      "epoch: 13, loss: 0.7469, train acc: 0.6990\n",
      "epoch: 13, loss: 0.7434, train acc: 0.7004\n",
      "epoch: 13, loss: 0.7439, train acc: 0.6998\n",
      ">epoch: 13, val_acc: 0.7128, val_f1: 0.2774\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 14\n",
      "epoch: 14, loss: 0.4993, train acc: 0.8125\n",
      "epoch: 14, loss: 0.6614, train acc: 0.7500\n",
      "epoch: 14, loss: 0.7509, train acc: 0.6875\n",
      "epoch: 14, loss: 0.7367, train acc: 0.6875\n",
      "epoch: 14, loss: 0.7171, train acc: 0.6964\n",
      "epoch: 14, loss: 0.7054, train acc: 0.7019\n",
      "epoch: 14, loss: 0.7051, train acc: 0.6935\n",
      "epoch: 14, loss: 0.7162, train acc: 0.6892\n",
      "epoch: 14, loss: 0.7175, train acc: 0.6829\n",
      "epoch: 14, loss: 0.7138, train acc: 0.6861\n",
      "epoch: 14, loss: 0.7288, train acc: 0.6777\n",
      "epoch: 14, loss: 0.7261, train acc: 0.6797\n",
      "epoch: 14, loss: 0.7155, train acc: 0.6844\n",
      "epoch: 14, loss: 0.7195, train acc: 0.6875\n",
      "epoch: 14, loss: 0.7225, train acc: 0.6866\n",
      "epoch: 14, loss: 0.7256, train acc: 0.6875\n",
      "epoch: 14, loss: 0.7265, train acc: 0.6890\n",
      "epoch: 14, loss: 0.7299, train acc: 0.6868\n",
      "epoch: 14, loss: 0.7343, train acc: 0.6868\n",
      "epoch: 14, loss: 0.7373, train acc: 0.6882\n",
      "epoch: 14, loss: 0.7372, train acc: 0.6887\n",
      "epoch: 14, loss: 0.7353, train acc: 0.6909\n",
      ">epoch: 14, val_acc: 0.7234, val_f1: 0.2798\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 15\n",
      "epoch: 15, loss: 0.7530, train acc: 0.7000\n",
      "epoch: 15, loss: 0.7531, train acc: 0.6937\n",
      "epoch: 15, loss: 0.7528, train acc: 0.6917\n",
      "epoch: 15, loss: 0.7585, train acc: 0.6875\n",
      "epoch: 15, loss: 0.7527, train acc: 0.6875\n",
      "epoch: 15, loss: 0.7480, train acc: 0.6917\n",
      "epoch: 15, loss: 0.7439, train acc: 0.6982\n",
      "epoch: 15, loss: 0.7444, train acc: 0.7000\n",
      "epoch: 15, loss: 0.7571, train acc: 0.6958\n",
      "epoch: 15, loss: 0.7634, train acc: 0.6913\n",
      "epoch: 15, loss: 0.7519, train acc: 0.6989\n",
      "epoch: 15, loss: 0.7539, train acc: 0.6979\n",
      "epoch: 15, loss: 0.7485, train acc: 0.6990\n",
      "epoch: 15, loss: 0.7510, train acc: 0.6991\n",
      "epoch: 15, loss: 0.7480, train acc: 0.6992\n",
      "epoch: 15, loss: 0.7477, train acc: 0.6984\n",
      "epoch: 15, loss: 0.7410, train acc: 0.6985\n",
      "epoch: 15, loss: 0.7373, train acc: 0.7000\n",
      "epoch: 15, loss: 0.7372, train acc: 0.6980\n",
      "epoch: 15, loss: 0.7375, train acc: 0.6975\n",
      "epoch: 15, loss: 0.7375, train acc: 0.6940\n",
      ">epoch: 15, val_acc: 0.7181, val_f1: 0.2919\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 16\n",
      "epoch: 16, loss: 0.7565, train acc: 0.6719\n",
      "epoch: 16, loss: 0.8353, train acc: 0.6181\n",
      "epoch: 16, loss: 0.8602, train acc: 0.5938\n",
      "epoch: 16, loss: 0.8262, train acc: 0.6316\n",
      "epoch: 16, loss: 0.8140, train acc: 0.6406\n",
      "epoch: 16, loss: 0.7901, train acc: 0.6552\n",
      "epoch: 16, loss: 0.7744, train acc: 0.6654\n",
      "epoch: 16, loss: 0.7640, train acc: 0.6731\n",
      "epoch: 16, loss: 0.7438, train acc: 0.6875\n",
      "epoch: 16, loss: 0.7332, train acc: 0.6939\n",
      "epoch: 16, loss: 0.7461, train acc: 0.6898\n",
      "epoch: 16, loss: 0.7463, train acc: 0.6886\n",
      "epoch: 16, loss: 0.7519, train acc: 0.6875\n",
      "epoch: 16, loss: 0.7506, train acc: 0.6857\n",
      "epoch: 16, loss: 0.7552, train acc: 0.6824\n",
      "epoch: 16, loss: 0.7556, train acc: 0.6788\n",
      "epoch: 16, loss: 0.7598, train acc: 0.6786\n",
      "epoch: 16, loss: 0.7561, train acc: 0.6791\n",
      "epoch: 16, loss: 0.7532, train acc: 0.6802\n",
      "epoch: 16, loss: 0.7502, train acc: 0.6824\n",
      "epoch: 16, loss: 0.7487, train acc: 0.6821\n",
      ">epoch: 16, val_acc: 0.7234, val_f1: 0.2798\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 17\n",
      "epoch: 17, loss: 0.6863, train acc: 0.7500\n",
      "epoch: 17, loss: 0.6960, train acc: 0.7500\n",
      "epoch: 17, loss: 0.6906, train acc: 0.7548\n",
      "epoch: 17, loss: 0.7236, train acc: 0.7361\n",
      "epoch: 17, loss: 0.7386, train acc: 0.7255\n",
      "epoch: 17, loss: 0.7459, train acc: 0.7076\n",
      "epoch: 17, loss: 0.7593, train acc: 0.6932\n",
      "epoch: 17, loss: 0.7468, train acc: 0.6908\n",
      "epoch: 17, loss: 0.7370, train acc: 0.6962\n",
      "epoch: 17, loss: 0.7400, train acc: 0.6992\n",
      "epoch: 17, loss: 0.7384, train acc: 0.7005\n",
      "epoch: 17, loss: 0.7403, train acc: 0.6983\n",
      "epoch: 17, loss: 0.7369, train acc: 0.6964\n",
      "epoch: 17, loss: 0.7436, train acc: 0.6958\n",
      "epoch: 17, loss: 0.7366, train acc: 0.6961\n",
      "epoch: 17, loss: 0.7295, train acc: 0.7003\n",
      "epoch: 17, loss: 0.7357, train acc: 0.6950\n",
      "epoch: 17, loss: 0.7373, train acc: 0.6960\n",
      "epoch: 17, loss: 0.7327, train acc: 0.6983\n",
      "epoch: 17, loss: 0.7382, train acc: 0.6964\n",
      "epoch: 17, loss: 0.7346, train acc: 0.6984\n",
      ">epoch: 17, val_acc: 0.7234, val_f1: 0.2798\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 18\n",
      "epoch: 18, loss: 0.9055, train acc: 0.6562\n",
      "epoch: 18, loss: 0.7368, train acc: 0.6964\n",
      "epoch: 18, loss: 0.7596, train acc: 0.6771\n",
      "epoch: 18, loss: 0.7324, train acc: 0.6875\n",
      "epoch: 18, loss: 0.6981, train acc: 0.6960\n",
      "epoch: 18, loss: 0.7057, train acc: 0.6968\n",
      "epoch: 18, loss: 0.6978, train acc: 0.6992\n",
      "epoch: 18, loss: 0.6967, train acc: 0.6993\n",
      "epoch: 18, loss: 0.6892, train acc: 0.7113\n",
      "epoch: 18, loss: 0.6979, train acc: 0.7061\n",
      "epoch: 18, loss: 0.7076, train acc: 0.7067\n",
      "epoch: 18, loss: 0.7262, train acc: 0.6985\n",
      "epoch: 18, loss: 0.7449, train acc: 0.6915\n",
      "epoch: 18, loss: 0.7382, train acc: 0.6968\n",
      "epoch: 18, loss: 0.7372, train acc: 0.6944\n",
      "epoch: 18, loss: 0.7327, train acc: 0.6989\n",
      "epoch: 18, loss: 0.7339, train acc: 0.6974\n",
      "epoch: 18, loss: 0.7354, train acc: 0.6983\n",
      "epoch: 18, loss: 0.7319, train acc: 0.7018\n",
      "epoch: 18, loss: 0.7371, train acc: 0.6972\n",
      "epoch: 18, loss: 0.7375, train acc: 0.6955\n",
      ">epoch: 18, val_acc: 0.7181, val_f1: 0.2919\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 19\n",
      "epoch: 19, loss: 0.7464, train acc: 0.6250\n",
      "epoch: 19, loss: 0.8458, train acc: 0.5729\n",
      "epoch: 19, loss: 0.7505, train acc: 0.6648\n",
      "epoch: 19, loss: 0.7467, train acc: 0.6758\n",
      "epoch: 19, loss: 0.7483, train acc: 0.6786\n",
      "epoch: 19, loss: 0.7618, train acc: 0.6803\n",
      "epoch: 19, loss: 0.7670, train acc: 0.6734\n",
      "epoch: 19, loss: 0.7575, train acc: 0.6788\n",
      "epoch: 19, loss: 0.7550, train acc: 0.6784\n",
      "epoch: 19, loss: 0.7546, train acc: 0.6834\n",
      "epoch: 19, loss: 0.7528, train acc: 0.6838\n",
      "epoch: 19, loss: 0.7494, train acc: 0.6864\n",
      "epoch: 19, loss: 0.7498, train acc: 0.6865\n",
      "epoch: 19, loss: 0.7488, train acc: 0.6866\n",
      "epoch: 19, loss: 0.7526, train acc: 0.6831\n",
      "epoch: 19, loss: 0.7445, train acc: 0.6875\n",
      "epoch: 19, loss: 0.7425, train acc: 0.6914\n",
      "epoch: 19, loss: 0.7377, train acc: 0.6904\n",
      "epoch: 19, loss: 0.7373, train acc: 0.6923\n",
      "epoch: 19, loss: 0.7369, train acc: 0.6908\n",
      "epoch: 19, loss: 0.7331, train acc: 0.6925\n",
      "epoch: 19, loss: 0.7273, train acc: 0.6956\n",
      ">epoch: 19, val_acc: 0.7128, val_f1: 0.2774\n",
      "Namespace(model_type='tri_gcn', modules={'tgcn': True, 'semgcn': True, 'lexgcn': True}, num_layers={'tgcn': 2, 'semgcn': 2, 'lexgcn': 2}, year='2016', train_file='data/train2016restaurant.txt', test_file='data/test2016restaurant.txt', model_path='test_models/2016tri_gcn+concat_seed699_reg0.0005_drop0.4_cdrop0.4_lr2.5e-05_epochs20_adam', val_file='val.txt', log='log', bert_model='bert_large_uncased', cooc_path='cooc_matrix.csv', cooc=       100       1000   10000  10003     10005     10007     10009     1001   \\\n",
      "100      0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "1000     0.0  0.000000    0.0    0.0  0.000606  0.000303  0.000606  0.035120   \n",
      "10000    0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.333333   \n",
      "10003    0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "10005    0.0  0.333333    0.0    0.0  0.000000  0.166667  0.000000  0.166667   \n",
      "...      ...       ...    ...    ...       ...       ...       ...       ...   \n",
      "9992     0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "9993     0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "9996     0.0  0.500000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "9997     0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "9998     0.0  0.400000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "       10010  10015  ...     9986      9987      999    9990      9991   \\\n",
      "100      0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "1000     0.0    0.0  ...  0.002422  0.000303  0.062368    0.0  0.000606   \n",
      "10000    0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "10003    0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "10005    0.0    0.0  ...  0.000000  0.000000  0.333333    0.0  0.000000   \n",
      "...      ...    ...  ...       ...       ...       ...    ...       ...   \n",
      "9992     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "9993     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "9996     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "9997     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "9998     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "\n",
      "       9992   9993      9996   9997      9998   \n",
      "100      0.0    0.0  0.000000    0.0  0.000000  \n",
      "1000     0.0    0.0  0.000606    0.0  0.001211  \n",
      "10000    0.0    0.0  0.000000    0.0  0.000000  \n",
      "10003    0.0    0.0  0.000000    0.0  0.000000  \n",
      "10005    0.0    0.0  0.000000    0.0  0.000000  \n",
      "...      ...    ...       ...    ...       ...  \n",
      "9992     0.0    0.0  0.000000    0.0  0.000000  \n",
      "9993     0.0    0.0  0.000000    0.0  0.000000  \n",
      "9996     0.0    0.0  0.000000    0.0  0.000000  \n",
      "9997     0.0    0.0  0.000000    0.0  0.000000  \n",
      "9998     0.0    0.0  0.000000    0.0  0.000000  \n",
      "\n",
      "[13075 rows x 13075 columns], learning_rate=2.5e-05, dropout=0.4, concat_dropout=0.4, bert_dropout=0.2, l2reg=0.0005, num_epoch=20, batch_size=16, log_step=5, max_seq_len=100, polarities_dim=3, device=device(type='cuda'), seed=699, valset_ratio=0.1, do_train=True, do_eval=True, eval_epoch_num=0, fusion_type='concat', use_ensemble=True, save_models='none', print_sent=False, optim='adam', n_gpu=1)\n",
      "{'none': 0, 'acl': 1, 'acl:relcl': 2, 'advcl': 3, 'advcl:relcl': 4, 'advmod': 5, 'amod': 6, 'appos': 7, 'aux': 8, 'aux:pass': 9, 'case': 10, 'cc': 11, 'cc:preconj': 12, 'ccomp': 13, 'compound': 14, 'compound:prt': 15, 'conj': 16, 'cop': 17, 'csubj': 18, 'dep': 19, 'det': 20, 'det:predet': 21, 'discourse': 22, 'dislocated': 23, 'expl': 24, 'fixed': 25, 'flat': 26, 'iobj': 27, 'list': 28, 'mark': 29, 'nmod': 30, 'nmod:npmod': 31, 'nmod:poss': 32, 'nmod:tmod': 33, 'nsubj': 34, 'nsubj:outer': 35, 'nsubj:pass': 36, 'nummod': 37, 'obj': 38, 'obl': 39, 'obl:agent': 40, 'obl:npmod': 41, 'obl:tmod': 42, 'parataxis': 43, 'punct': 44, 'reparandum': 45, 'root': 46, 'vocative': 47, 'xcomp': 48}\n",
      "{'-1': 0, '0': 1, '1': 2}\n",
      "Model name 'bert_large_uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming 'bert_large_uncased' is a path or url to a directory containing tokenizer files.\n",
      "Didn't find file bert_large_uncased/added_tokens.json. We won't load it.\n",
      "Didn't find file bert_large_uncased/special_tokens_map.json. We won't load it.\n",
      "Didn't find file bert_large_uncased/tokenizer_config.json. We won't load it.\n",
      "loading file bert_large_uncased/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "{\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 3,\n",
      "  \"num_types\": 49,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file bert_large_uncased/pytorch_model.bin\n",
      "Weights of AsaTgcnSem not initialized from pretrained model: ['gate_weight', 'gate_bias', 'ensemble', 'TGCNLayers.0.weight', 'TGCNLayers.0.bias', 'TGCNLayers.0.dense.weight', 'TGCNLayers.1.weight', 'TGCNLayers.1.bias', 'TGCNLayers.1.dense.weight', 'SemGCNLayers.0.weight', 'SemGCNLayers.0.bias', 'SemGCNLayers.1.weight', 'SemGCNLayers.1.bias', 'LexGCNLayers.0.weight', 'LexGCNLayers.0.bias', 'LexGCNLayers.1.weight', 'LexGCNLayers.1.bias', 'fc_single.weight', 'fc_single.bias', 'ensemble_linear_tgcn.weight', 'ensemble_linear_tgcn.bias', 'ensemble_linear_semgcn.weight', 'ensemble_linear_semgcn.bias', 'ensemble_linear_lexgcn.weight', 'ensemble_linear_lexgcn.bias', 'dep_embedding.weight']\n",
      "Weights from pretrained model not used in AsaTgcnSem: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "[{'input_ids': tensor([  101, 13325,  2013,  3025,  8466,  2023,  2109,  2000,  2022,  1037,\n",
      "         2204,  2173,  1010,  2021,  2025,  2151,  2936,  1012,   102,  2173,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'valid_ids': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'segment_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'mem_valid_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'dep_adj_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'dep_value_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'polarity': 0, 'raw_text': 'judging from previous posts this used to be a good place , but not any longer .', 'aspect': 'place'}]\n",
      "[{'input_ids': tensor([  101, 13325,  2013,  3025,  8466,  2023,  2109,  2000,  2022,  1037,\n",
      "         2204,  2173,  1010,  2021,  2025,  2151,  2936,  1012,   102,  2173,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'valid_ids': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'segment_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'mem_valid_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'dep_adj_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'dep_value_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'polarity': 0, 'raw_text': 'judging from previous posts this used to be a good place , but not any longer .', 'aspect': 'place'}]\n",
      "[{'input_ids': tensor([  101,  4240,  2428,  2204, 10514,  6182,  1012,   102, 10514,  6182,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'valid_ids': tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'segment_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'mem_valid_ids': tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'dep_adj_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'dep_value_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 5,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'polarity': 2, 'raw_text': 'serves really good sushi .', 'aspect': 'sushi'}]\n",
      "cuda memory allocated: 1400893440\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 0\n",
      "epoch: 0, loss: 0.9373, train acc: 0.5375\n",
      "epoch: 0, loss: 0.8974, train acc: 0.5875\n",
      "epoch: 0, loss: 0.8679, train acc: 0.6083\n",
      "epoch: 0, loss: 0.8891, train acc: 0.6031\n",
      "epoch: 0, loss: 0.8784, train acc: 0.6000\n",
      "epoch: 0, loss: 0.8472, train acc: 0.6208\n",
      "epoch: 0, loss: 0.8432, train acc: 0.6286\n",
      "epoch: 0, loss: 0.8365, train acc: 0.6422\n",
      "epoch: 0, loss: 0.8281, train acc: 0.6431\n",
      "epoch: 0, loss: 0.8431, train acc: 0.6388\n",
      "epoch: 0, loss: 0.8326, train acc: 0.6443\n",
      "epoch: 0, loss: 0.8225, train acc: 0.6510\n",
      "epoch: 0, loss: 0.8287, train acc: 0.6519\n",
      "epoch: 0, loss: 0.8264, train acc: 0.6491\n",
      "epoch: 0, loss: 0.8200, train acc: 0.6558\n",
      "epoch: 0, loss: 0.8084, train acc: 0.6570\n",
      "epoch: 0, loss: 0.8038, train acc: 0.6581\n",
      "epoch: 0, loss: 0.8045, train acc: 0.6604\n",
      "epoch: 0, loss: 0.8094, train acc: 0.6605\n",
      "epoch: 0, loss: 0.8002, train acc: 0.6656\n",
      "epoch: 0, loss: 0.7955, train acc: 0.6667\n",
      ">epoch: 0, val_acc: 0.6755, val_f1: 0.2688\n",
      ">> epoch: 0, test_acc: 0.7508, test_f1: 0.3093\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 1\n",
      "epoch: 1, loss: 0.5985, train acc: 0.7344\n",
      "epoch: 1, loss: 0.7280, train acc: 0.6806\n",
      "epoch: 1, loss: 0.6915, train acc: 0.7188\n",
      "epoch: 1, loss: 0.6767, train acc: 0.7171\n",
      "epoch: 1, loss: 0.6902, train acc: 0.7083\n",
      "epoch: 1, loss: 0.6839, train acc: 0.7112\n",
      "epoch: 1, loss: 0.6783, train acc: 0.7096\n",
      "epoch: 1, loss: 0.6768, train acc: 0.7051\n",
      "epoch: 1, loss: 0.6753, train acc: 0.6989\n",
      "epoch: 1, loss: 0.6709, train acc: 0.7079\n",
      "epoch: 1, loss: 0.6618, train acc: 0.7188\n",
      "epoch: 1, loss: 0.6529, train acc: 0.7256\n",
      "epoch: 1, loss: 0.6392, train acc: 0.7354\n",
      "epoch: 1, loss: 0.6268, train acc: 0.7418\n",
      "epoch: 1, loss: 0.6233, train acc: 0.7432\n",
      "epoch: 1, loss: 0.6177, train acc: 0.7460\n",
      "epoch: 1, loss: 0.5979, train acc: 0.7560\n",
      "epoch: 1, loss: 0.5836, train acc: 0.7647\n",
      "epoch: 1, loss: 0.5808, train acc: 0.7673\n",
      "epoch: 1, loss: 0.5778, train acc: 0.7696\n",
      "epoch: 1, loss: 0.5691, train acc: 0.7746\n",
      ">epoch: 1, val_acc: 0.8564, val_f1: 0.5676\n",
      ">> epoch: 1, test_acc: 0.8800, test_f1: 0.5710\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 2\n",
      "epoch: 2, loss: 0.3415, train acc: 0.9167\n",
      "epoch: 2, loss: 0.3916, train acc: 0.8594\n",
      "epoch: 2, loss: 0.3916, train acc: 0.8510\n",
      "epoch: 2, loss: 0.3546, train acc: 0.8646\n",
      "epoch: 2, loss: 0.4024, train acc: 0.8505\n",
      "epoch: 2, loss: 0.4047, train acc: 0.8549\n",
      "epoch: 2, loss: 0.4062, train acc: 0.8485\n",
      "epoch: 2, loss: 0.3985, train acc: 0.8503\n",
      "epoch: 2, loss: 0.3878, train acc: 0.8590\n",
      "epoch: 2, loss: 0.3946, train acc: 0.8568\n",
      "epoch: 2, loss: 0.3834, train acc: 0.8608\n",
      "epoch: 2, loss: 0.3762, train acc: 0.8653\n",
      "epoch: 2, loss: 0.3613, train acc: 0.8700\n",
      "epoch: 2, loss: 0.3629, train acc: 0.8695\n",
      "epoch: 2, loss: 0.3652, train acc: 0.8682\n",
      "epoch: 2, loss: 0.3605, train acc: 0.8710\n",
      "epoch: 2, loss: 0.3578, train acc: 0.8712\n",
      "epoch: 2, loss: 0.3514, train acc: 0.8736\n",
      "epoch: 2, loss: 0.3531, train acc: 0.8730\n",
      "epoch: 2, loss: 0.3575, train acc: 0.8705\n",
      "epoch: 2, loss: 0.3586, train acc: 0.8689\n",
      ">epoch: 2, val_acc: 0.8670, val_f1: 0.5764\n",
      ">> epoch: 2, test_acc: 0.8892, test_f1: 0.6171\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 3\n",
      "epoch: 3, loss: 0.3447, train acc: 0.9062\n",
      "epoch: 3, loss: 0.2365, train acc: 0.9464\n",
      "epoch: 3, loss: 0.2029, train acc: 0.9479\n",
      "epoch: 3, loss: 0.1752, train acc: 0.9522\n",
      "epoch: 3, loss: 0.1497, train acc: 0.9631\n",
      "epoch: 3, loss: 0.1833, train acc: 0.9560\n",
      "epoch: 3, loss: 0.1784, train acc: 0.9531\n",
      "epoch: 3, loss: 0.2001, train acc: 0.9426\n",
      "epoch: 3, loss: 0.2003, train acc: 0.9420\n",
      "epoch: 3, loss: 0.1988, train acc: 0.9415\n",
      "epoch: 3, loss: 0.1893, train acc: 0.9447\n",
      "epoch: 3, loss: 0.2066, train acc: 0.9364\n",
      "epoch: 3, loss: 0.2084, train acc: 0.9355\n",
      "epoch: 3, loss: 0.2146, train acc: 0.9310\n",
      "epoch: 3, loss: 0.2230, train acc: 0.9280\n",
      "epoch: 3, loss: 0.2217, train acc: 0.9286\n",
      "epoch: 3, loss: 0.2212, train acc: 0.9268\n",
      "epoch: 3, loss: 0.2265, train acc: 0.9239\n",
      "epoch: 3, loss: 0.2318, train acc: 0.9219\n",
      "epoch: 3, loss: 0.2280, train acc: 0.9233\n",
      "epoch: 3, loss: 0.2290, train acc: 0.9234\n",
      ">epoch: 3, val_acc: 0.8830, val_f1: 0.7462\n",
      ">> epoch: 3, test_acc: 0.9062, test_f1: 0.6802\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 4\n",
      "epoch: 4, loss: 0.0771, train acc: 1.0000\n",
      "epoch: 4, loss: 0.2120, train acc: 0.9167\n",
      "epoch: 4, loss: 0.1779, train acc: 0.9318\n",
      "epoch: 4, loss: 0.1538, train acc: 0.9414\n",
      "epoch: 4, loss: 0.1368, train acc: 0.9494\n",
      "epoch: 4, loss: 0.1253, train acc: 0.9567\n",
      "epoch: 4, loss: 0.1582, train acc: 0.9496\n",
      "epoch: 4, loss: 0.1628, train acc: 0.9497\n",
      "epoch: 4, loss: 0.1578, train acc: 0.9512\n",
      "epoch: 4, loss: 0.1540, train acc: 0.9511\n",
      "epoch: 4, loss: 0.1656, train acc: 0.9461\n",
      "epoch: 4, loss: 0.1558, train acc: 0.9498\n",
      "epoch: 4, loss: 0.1546, train acc: 0.9508\n",
      "epoch: 4, loss: 0.1508, train acc: 0.9527\n",
      "epoch: 4, loss: 0.1509, train acc: 0.9507\n",
      "epoch: 4, loss: 0.1615, train acc: 0.9482\n",
      "epoch: 4, loss: 0.1651, train acc: 0.9460\n",
      "epoch: 4, loss: 0.1688, train acc: 0.9448\n",
      "epoch: 4, loss: 0.1743, train acc: 0.9430\n",
      "epoch: 4, loss: 0.1743, train acc: 0.9427\n",
      "epoch: 4, loss: 0.1759, train acc: 0.9418\n",
      "epoch: 4, loss: 0.1758, train acc: 0.9409\n",
      ">epoch: 4, val_acc: 0.8670, val_f1: 0.6505\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 5\n",
      "epoch: 5, loss: 0.1061, train acc: 0.9750\n",
      "epoch: 5, loss: 0.1242, train acc: 0.9563\n",
      "epoch: 5, loss: 0.1310, train acc: 0.9458\n",
      "epoch: 5, loss: 0.1262, train acc: 0.9469\n",
      "epoch: 5, loss: 0.1246, train acc: 0.9450\n",
      "epoch: 5, loss: 0.1321, train acc: 0.9437\n",
      "epoch: 5, loss: 0.1336, train acc: 0.9464\n",
      "epoch: 5, loss: 0.1396, train acc: 0.9437\n",
      "epoch: 5, loss: 0.1501, train acc: 0.9431\n",
      "epoch: 5, loss: 0.1449, train acc: 0.9463\n",
      "epoch: 5, loss: 0.1394, train acc: 0.9489\n",
      "epoch: 5, loss: 0.1362, train acc: 0.9510\n",
      "epoch: 5, loss: 0.1333, train acc: 0.9519\n",
      "epoch: 5, loss: 0.1351, train acc: 0.9509\n",
      "epoch: 5, loss: 0.1351, train acc: 0.9517\n",
      "epoch: 5, loss: 0.1340, train acc: 0.9531\n",
      "epoch: 5, loss: 0.1303, train acc: 0.9544\n",
      "epoch: 5, loss: 0.1370, train acc: 0.9514\n",
      "epoch: 5, loss: 0.1411, train acc: 0.9500\n",
      "epoch: 5, loss: 0.1464, train acc: 0.9469\n",
      "epoch: 5, loss: 0.1481, train acc: 0.9452\n",
      ">epoch: 5, val_acc: 0.8670, val_f1: 0.7585\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 6\n",
      "epoch: 6, loss: 0.0563, train acc: 0.9844\n",
      "epoch: 6, loss: 0.1001, train acc: 0.9722\n",
      "epoch: 6, loss: 0.0896, train acc: 0.9732\n",
      "epoch: 6, loss: 0.0906, train acc: 0.9704\n",
      "epoch: 6, loss: 0.0898, train acc: 0.9714\n",
      "epoch: 6, loss: 0.0861, train acc: 0.9720\n",
      "epoch: 6, loss: 0.0773, train acc: 0.9743\n",
      "epoch: 6, loss: 0.0840, train acc: 0.9728\n",
      "epoch: 6, loss: 0.0818, train acc: 0.9730\n",
      "epoch: 6, loss: 0.0856, train acc: 0.9719\n",
      "epoch: 6, loss: 0.0817, train acc: 0.9734\n",
      "epoch: 6, loss: 0.0947, train acc: 0.9693\n",
      "epoch: 6, loss: 0.0955, train acc: 0.9688\n",
      "epoch: 6, loss: 0.0985, train acc: 0.9683\n",
      "epoch: 6, loss: 0.0966, train acc: 0.9679\n",
      "epoch: 6, loss: 0.0975, train acc: 0.9676\n",
      "epoch: 6, loss: 0.0986, train acc: 0.9680\n",
      "epoch: 6, loss: 0.0976, train acc: 0.9670\n",
      "epoch: 6, loss: 0.0971, train acc: 0.9674\n",
      "epoch: 6, loss: 0.0946, train acc: 0.9678\n",
      "epoch: 6, loss: 0.0931, train acc: 0.9675\n",
      ">epoch: 6, val_acc: 0.8883, val_f1: 0.8209\n",
      ">> epoch: 6, test_acc: 0.9015, test_f1: 0.6724\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 7\n",
      "epoch: 7, loss: 0.0559, train acc: 0.9792\n",
      "epoch: 7, loss: 0.0517, train acc: 0.9766\n",
      "epoch: 7, loss: 0.0391, train acc: 0.9856\n",
      "epoch: 7, loss: 0.0407, train acc: 0.9861\n",
      "epoch: 7, loss: 0.0427, train acc: 0.9864\n",
      "epoch: 7, loss: 0.0405, train acc: 0.9866\n",
      "epoch: 7, loss: 0.0489, train acc: 0.9830\n",
      "epoch: 7, loss: 0.0517, train acc: 0.9803\n",
      "epoch: 7, loss: 0.0521, train acc: 0.9811\n",
      "epoch: 7, loss: 0.0535, train acc: 0.9818\n",
      "epoch: 7, loss: 0.0607, train acc: 0.9788\n",
      "epoch: 7, loss: 0.0596, train acc: 0.9784\n",
      "epoch: 7, loss: 0.0614, train acc: 0.9762\n",
      "epoch: 7, loss: 0.0637, train acc: 0.9752\n",
      "epoch: 7, loss: 0.0633, train acc: 0.9760\n",
      "epoch: 7, loss: 0.0669, train acc: 0.9744\n",
      "epoch: 7, loss: 0.0718, train acc: 0.9736\n",
      "epoch: 7, loss: 0.0688, train acc: 0.9751\n",
      "epoch: 7, loss: 0.0691, train acc: 0.9751\n",
      "epoch: 7, loss: 0.0688, train acc: 0.9751\n",
      "epoch: 7, loss: 0.0677, train acc: 0.9751\n",
      ">epoch: 7, val_acc: 0.8936, val_f1: 0.8123\n",
      ">> epoch: 7, test_acc: 0.9062, test_f1: 0.7426\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 8\n",
      "epoch: 8, loss: 0.0351, train acc: 0.9688\n",
      "epoch: 8, loss: 0.0244, train acc: 0.9821\n",
      "epoch: 8, loss: 0.0267, train acc: 0.9844\n",
      "epoch: 8, loss: 0.0280, train acc: 0.9890\n",
      "epoch: 8, loss: 0.0383, train acc: 0.9858\n",
      "epoch: 8, loss: 0.0480, train acc: 0.9815\n",
      "epoch: 8, loss: 0.0525, train acc: 0.9785\n",
      "epoch: 8, loss: 0.0524, train acc: 0.9780\n",
      "epoch: 8, loss: 0.0573, train acc: 0.9747\n",
      "epoch: 8, loss: 0.0528, train acc: 0.9774\n",
      "epoch: 8, loss: 0.0520, train acc: 0.9772\n",
      "epoch: 8, loss: 0.0643, train acc: 0.9759\n",
      "epoch: 8, loss: 0.0672, train acc: 0.9748\n",
      "epoch: 8, loss: 0.0703, train acc: 0.9729\n",
      "epoch: 8, loss: 0.0705, train acc: 0.9731\n",
      "epoch: 8, loss: 0.0740, train acc: 0.9724\n",
      "epoch: 8, loss: 0.0757, train acc: 0.9726\n",
      "epoch: 8, loss: 0.0763, train acc: 0.9720\n",
      "epoch: 8, loss: 0.0749, train acc: 0.9721\n",
      "epoch: 8, loss: 0.0726, train acc: 0.9736\n",
      "epoch: 8, loss: 0.0726, train acc: 0.9724\n",
      ">epoch: 8, val_acc: 0.8777, val_f1: 0.8094\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 9\n",
      "epoch: 9, loss: 0.0039, train acc: 1.0000\n",
      "epoch: 9, loss: 0.0051, train acc: 1.0000\n",
      "epoch: 9, loss: 0.0069, train acc: 1.0000\n",
      "epoch: 9, loss: 0.0187, train acc: 0.9922\n",
      "epoch: 9, loss: 0.0192, train acc: 0.9911\n",
      "epoch: 9, loss: 0.0198, train acc: 0.9904\n",
      "epoch: 9, loss: 0.0180, train acc: 0.9919\n",
      "epoch: 9, loss: 0.0219, train acc: 0.9896\n",
      "epoch: 9, loss: 0.0372, train acc: 0.9863\n",
      "epoch: 9, loss: 0.0526, train acc: 0.9810\n",
      "epoch: 9, loss: 0.0562, train acc: 0.9779\n",
      "epoch: 9, loss: 0.0564, train acc: 0.9777\n",
      "epoch: 9, loss: 0.0568, train acc: 0.9785\n",
      "epoch: 9, loss: 0.0539, train acc: 0.9801\n",
      "epoch: 9, loss: 0.0591, train acc: 0.9789\n",
      "epoch: 9, loss: 0.0598, train acc: 0.9778\n",
      "epoch: 9, loss: 0.0642, train acc: 0.9761\n",
      "epoch: 9, loss: 0.0697, train acc: 0.9746\n",
      "epoch: 9, loss: 0.0747, train acc: 0.9746\n",
      "epoch: 9, loss: 0.0769, train acc: 0.9733\n",
      "epoch: 9, loss: 0.0765, train acc: 0.9734\n",
      "epoch: 9, loss: 0.0780, train acc: 0.9722\n",
      ">epoch: 9, val_acc: 0.8723, val_f1: 0.7730\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 10\n",
      "epoch: 10, loss: 0.0608, train acc: 0.9750\n",
      "epoch: 10, loss: 0.0631, train acc: 0.9750\n",
      "epoch: 10, loss: 0.0566, train acc: 0.9750\n",
      "epoch: 10, loss: 0.0515, train acc: 0.9812\n",
      "epoch: 10, loss: 0.0544, train acc: 0.9775\n",
      "epoch: 10, loss: 0.0476, train acc: 0.9812\n",
      "epoch: 10, loss: 0.0458, train acc: 0.9804\n",
      "epoch: 10, loss: 0.0587, train acc: 0.9781\n",
      "epoch: 10, loss: 0.0562, train acc: 0.9778\n",
      "epoch: 10, loss: 0.0528, train acc: 0.9788\n",
      "epoch: 10, loss: 0.0540, train acc: 0.9784\n",
      "epoch: 10, loss: 0.0527, train acc: 0.9792\n",
      "epoch: 10, loss: 0.0596, train acc: 0.9769\n",
      "epoch: 10, loss: 0.0660, train acc: 0.9759\n",
      "epoch: 10, loss: 0.0668, train acc: 0.9767\n",
      "epoch: 10, loss: 0.0642, train acc: 0.9781\n",
      "epoch: 10, loss: 0.0631, train acc: 0.9779\n",
      "epoch: 10, loss: 0.0644, train acc: 0.9778\n",
      "epoch: 10, loss: 0.0637, train acc: 0.9776\n",
      "epoch: 10, loss: 0.0656, train acc: 0.9762\n",
      "epoch: 10, loss: 0.0652, train acc: 0.9768\n",
      ">epoch: 10, val_acc: 0.8723, val_f1: 0.7864\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 11\n",
      "epoch: 11, loss: 0.0151, train acc: 1.0000\n",
      "epoch: 11, loss: 0.0513, train acc: 0.9722\n",
      "epoch: 11, loss: 0.0356, train acc: 0.9821\n",
      "epoch: 11, loss: 0.0359, train acc: 0.9836\n",
      "epoch: 11, loss: 0.0326, train acc: 0.9870\n",
      "epoch: 11, loss: 0.0301, train acc: 0.9871\n",
      "epoch: 11, loss: 0.0284, train acc: 0.9871\n",
      "epoch: 11, loss: 0.0370, train acc: 0.9856\n",
      "epoch: 11, loss: 0.0373, train acc: 0.9858\n",
      "epoch: 11, loss: 0.0404, train acc: 0.9847\n",
      "epoch: 11, loss: 0.0401, train acc: 0.9838\n",
      "epoch: 11, loss: 0.0382, train acc: 0.9841\n",
      "epoch: 11, loss: 0.0389, train acc: 0.9844\n",
      "epoch: 11, loss: 0.0431, train acc: 0.9837\n",
      "epoch: 11, loss: 0.0456, train acc: 0.9831\n",
      "epoch: 11, loss: 0.0473, train acc: 0.9818\n",
      "epoch: 11, loss: 0.0479, train acc: 0.9821\n",
      "epoch: 11, loss: 0.0476, train acc: 0.9824\n",
      "epoch: 11, loss: 0.0489, train acc: 0.9820\n",
      "epoch: 11, loss: 0.0482, train acc: 0.9823\n",
      "epoch: 11, loss: 0.0490, train acc: 0.9820\n",
      ">epoch: 11, val_acc: 0.8457, val_f1: 0.7433\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 12\n",
      "epoch: 12, loss: 0.0469, train acc: 0.9792\n",
      "epoch: 12, loss: 0.0401, train acc: 0.9844\n",
      "epoch: 12, loss: 0.0301, train acc: 0.9904\n",
      "epoch: 12, loss: 0.0581, train acc: 0.9861\n",
      "epoch: 12, loss: 0.0524, train acc: 0.9864\n",
      "epoch: 12, loss: 0.0479, train acc: 0.9866\n",
      "epoch: 12, loss: 0.0542, train acc: 0.9830\n",
      "epoch: 12, loss: 0.0524, train acc: 0.9819\n",
      "epoch: 12, loss: 0.0565, train acc: 0.9811\n",
      "epoch: 12, loss: 0.0595, train acc: 0.9792\n",
      "epoch: 12, loss: 0.0582, train acc: 0.9800\n",
      "epoch: 12, loss: 0.0556, train acc: 0.9806\n",
      "epoch: 12, loss: 0.0548, train acc: 0.9812\n",
      "epoch: 12, loss: 0.0531, train acc: 0.9816\n",
      "epoch: 12, loss: 0.0531, train acc: 0.9812\n",
      "epoch: 12, loss: 0.0548, train acc: 0.9800\n",
      "epoch: 12, loss: 0.0550, train acc: 0.9797\n",
      "epoch: 12, loss: 0.0540, train acc: 0.9801\n",
      "epoch: 12, loss: 0.0525, train acc: 0.9805\n",
      "epoch: 12, loss: 0.0519, train acc: 0.9809\n",
      "epoch: 12, loss: 0.0534, train acc: 0.9800\n",
      ">epoch: 12, val_acc: 0.8617, val_f1: 0.7568\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 13\n",
      "epoch: 13, loss: 0.0008, train acc: 1.0000\n",
      "epoch: 13, loss: 0.0400, train acc: 0.9911\n",
      "epoch: 13, loss: 0.0371, train acc: 0.9844\n",
      "epoch: 13, loss: 0.0342, train acc: 0.9853\n",
      "epoch: 13, loss: 0.0329, train acc: 0.9830\n",
      "epoch: 13, loss: 0.0316, train acc: 0.9861\n",
      "epoch: 13, loss: 0.0365, train acc: 0.9824\n",
      "epoch: 13, loss: 0.0383, train acc: 0.9797\n",
      "epoch: 13, loss: 0.0358, train acc: 0.9807\n",
      "epoch: 13, loss: 0.0334, train acc: 0.9814\n",
      "epoch: 13, loss: 0.0305, train acc: 0.9832\n",
      "epoch: 13, loss: 0.0321, train acc: 0.9825\n",
      "epoch: 13, loss: 0.0333, train acc: 0.9829\n",
      "epoch: 13, loss: 0.0318, train acc: 0.9832\n",
      "epoch: 13, loss: 0.0360, train acc: 0.9809\n",
      "epoch: 13, loss: 0.0389, train acc: 0.9797\n",
      "epoch: 13, loss: 0.0399, train acc: 0.9794\n",
      "epoch: 13, loss: 0.0407, train acc: 0.9792\n",
      "epoch: 13, loss: 0.0413, train acc: 0.9783\n",
      "epoch: 13, loss: 0.0402, train acc: 0.9794\n",
      "epoch: 13, loss: 0.0391, train acc: 0.9804\n",
      ">epoch: 13, val_acc: 0.8617, val_f1: 0.7665\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 14\n",
      "epoch: 14, loss: 0.0003, train acc: 1.0000\n",
      "epoch: 14, loss: 0.0181, train acc: 0.9896\n",
      "epoch: 14, loss: 0.0457, train acc: 0.9773\n",
      "epoch: 14, loss: 0.0412, train acc: 0.9766\n",
      "epoch: 14, loss: 0.0345, train acc: 0.9821\n",
      "epoch: 14, loss: 0.0375, train acc: 0.9808\n",
      "epoch: 14, loss: 0.0325, train acc: 0.9839\n",
      "epoch: 14, loss: 0.0352, train acc: 0.9809\n",
      "epoch: 14, loss: 0.0452, train acc: 0.9771\n",
      "epoch: 14, loss: 0.0457, train acc: 0.9783\n",
      "epoch: 14, loss: 0.0446, train acc: 0.9779\n",
      "epoch: 14, loss: 0.0450, train acc: 0.9777\n",
      "epoch: 14, loss: 0.0426, train acc: 0.9795\n",
      "epoch: 14, loss: 0.0437, train acc: 0.9792\n",
      "epoch: 14, loss: 0.0433, train acc: 0.9798\n",
      "epoch: 14, loss: 0.0454, train acc: 0.9778\n",
      "epoch: 14, loss: 0.0442, train acc: 0.9784\n",
      "epoch: 14, loss: 0.0502, train acc: 0.9782\n",
      "epoch: 14, loss: 0.0491, train acc: 0.9787\n",
      "epoch: 14, loss: 0.0476, train acc: 0.9798\n",
      "epoch: 14, loss: 0.0476, train acc: 0.9802\n",
      "epoch: 14, loss: 0.0471, train acc: 0.9805\n",
      ">epoch: 14, val_acc: 0.8777, val_f1: 0.8249\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 15\n",
      "epoch: 15, loss: 0.0279, train acc: 0.9750\n",
      "epoch: 15, loss: 0.0170, train acc: 0.9875\n",
      "epoch: 15, loss: 0.0269, train acc: 0.9833\n",
      "epoch: 15, loss: 0.0268, train acc: 0.9844\n",
      "epoch: 15, loss: 0.0289, train acc: 0.9850\n",
      "epoch: 15, loss: 0.0293, train acc: 0.9854\n",
      "epoch: 15, loss: 0.0347, train acc: 0.9821\n",
      "epoch: 15, loss: 0.0356, train acc: 0.9812\n",
      "epoch: 15, loss: 0.0343, train acc: 0.9819\n",
      "epoch: 15, loss: 0.0334, train acc: 0.9825\n",
      "epoch: 15, loss: 0.0407, train acc: 0.9807\n",
      "epoch: 15, loss: 0.0422, train acc: 0.9802\n",
      "epoch: 15, loss: 0.0485, train acc: 0.9788\n",
      "epoch: 15, loss: 0.0469, train acc: 0.9795\n",
      "epoch: 15, loss: 0.0444, train acc: 0.9808\n",
      "epoch: 15, loss: 0.0468, train acc: 0.9797\n",
      "epoch: 15, loss: 0.0448, train acc: 0.9809\n",
      "epoch: 15, loss: 0.0443, train acc: 0.9812\n",
      "epoch: 15, loss: 0.0464, train acc: 0.9796\n",
      "epoch: 15, loss: 0.0457, train acc: 0.9800\n",
      "epoch: 15, loss: 0.0467, train acc: 0.9798\n",
      ">epoch: 15, val_acc: 0.8511, val_f1: 0.7739\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 16\n",
      "epoch: 16, loss: 0.0438, train acc: 0.9844\n",
      "epoch: 16, loss: 0.0437, train acc: 0.9722\n",
      "epoch: 16, loss: 0.0363, train acc: 0.9777\n",
      "epoch: 16, loss: 0.0398, train acc: 0.9770\n",
      "epoch: 16, loss: 0.0327, train acc: 0.9818\n",
      "epoch: 16, loss: 0.0527, train acc: 0.9784\n",
      "epoch: 16, loss: 0.0466, train acc: 0.9816\n",
      "epoch: 16, loss: 0.0452, train acc: 0.9808\n",
      "epoch: 16, loss: 0.0426, train acc: 0.9815\n",
      "epoch: 16, loss: 0.0419, train acc: 0.9821\n",
      "epoch: 16, loss: 0.0393, train acc: 0.9826\n",
      "epoch: 16, loss: 0.0393, train acc: 0.9831\n",
      "epoch: 16, loss: 0.0388, train acc: 0.9834\n",
      "epoch: 16, loss: 0.0404, train acc: 0.9819\n",
      "epoch: 16, loss: 0.0387, train acc: 0.9823\n",
      "epoch: 16, loss: 0.0446, train acc: 0.9802\n",
      "epoch: 16, loss: 0.0422, train acc: 0.9814\n",
      "epoch: 16, loss: 0.0431, train acc: 0.9817\n",
      "epoch: 16, loss: 0.0419, train acc: 0.9820\n",
      "epoch: 16, loss: 0.0419, train acc: 0.9823\n",
      "epoch: 16, loss: 0.0423, train acc: 0.9826\n",
      ">epoch: 16, val_acc: 0.8777, val_f1: 0.7762\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 17\n",
      "epoch: 17, loss: 0.0371, train acc: 0.9792\n",
      "epoch: 17, loss: 0.0379, train acc: 0.9766\n",
      "epoch: 17, loss: 0.0340, train acc: 0.9808\n",
      "epoch: 17, loss: 0.0311, train acc: 0.9826\n",
      "epoch: 17, loss: 0.0432, train acc: 0.9783\n",
      "epoch: 17, loss: 0.0449, train acc: 0.9754\n",
      "epoch: 17, loss: 0.0444, train acc: 0.9754\n",
      "epoch: 17, loss: 0.0456, train acc: 0.9737\n",
      "epoch: 17, loss: 0.0483, train acc: 0.9738\n",
      "epoch: 17, loss: 0.0450, train acc: 0.9766\n",
      "epoch: 17, loss: 0.0421, train acc: 0.9776\n",
      "epoch: 17, loss: 0.0413, train acc: 0.9784\n",
      "epoch: 17, loss: 0.0401, train acc: 0.9792\n",
      "epoch: 17, loss: 0.0401, train acc: 0.9798\n",
      "epoch: 17, loss: 0.0375, train acc: 0.9812\n",
      "epoch: 17, loss: 0.0389, train acc: 0.9816\n",
      "epoch: 17, loss: 0.0377, train acc: 0.9819\n",
      "epoch: 17, loss: 0.0392, train acc: 0.9808\n",
      "epoch: 17, loss: 0.0376, train acc: 0.9819\n",
      "epoch: 17, loss: 0.0375, train acc: 0.9815\n",
      "epoch: 17, loss: 0.0392, train acc: 0.9806\n",
      ">epoch: 17, val_acc: 0.8564, val_f1: 0.7853\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 18\n",
      "epoch: 18, loss: 0.0154, train acc: 1.0000\n",
      "epoch: 18, loss: 0.0311, train acc: 0.9821\n",
      "epoch: 18, loss: 0.0336, train acc: 0.9792\n",
      "epoch: 18, loss: 0.0251, train acc: 0.9853\n",
      "epoch: 18, loss: 0.0213, train acc: 0.9886\n",
      "epoch: 18, loss: 0.0210, train acc: 0.9884\n",
      "epoch: 18, loss: 0.0268, train acc: 0.9863\n",
      "epoch: 18, loss: 0.0232, train acc: 0.9882\n",
      "epoch: 18, loss: 0.0559, train acc: 0.9792\n",
      "epoch: 18, loss: 0.0568, train acc: 0.9761\n",
      "epoch: 18, loss: 0.0623, train acc: 0.9760\n",
      "epoch: 18, loss: 0.0635, train acc: 0.9748\n",
      "epoch: 18, loss: 0.0673, train acc: 0.9738\n",
      "epoch: 18, loss: 0.0666, train acc: 0.9748\n",
      "epoch: 18, loss: 0.0743, train acc: 0.9722\n",
      "epoch: 18, loss: 0.0769, train acc: 0.9716\n",
      "epoch: 18, loss: 0.0815, train acc: 0.9688\n",
      "epoch: 18, loss: 0.0790, train acc: 0.9698\n",
      "epoch: 18, loss: 0.0791, train acc: 0.9701\n",
      "epoch: 18, loss: 0.0830, train acc: 0.9684\n",
      "epoch: 18, loss: 0.0858, train acc: 0.9669\n",
      ">epoch: 18, val_acc: 0.8564, val_f1: 0.7277\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 19\n",
      "epoch: 19, loss: 0.0009, train acc: 1.0000\n",
      "epoch: 19, loss: 0.0186, train acc: 1.0000\n",
      "epoch: 19, loss: 0.0682, train acc: 0.9716\n",
      "epoch: 19, loss: 0.0772, train acc: 0.9688\n",
      "epoch: 19, loss: 0.0915, train acc: 0.9702\n",
      "epoch: 19, loss: 0.1027, train acc: 0.9615\n",
      "epoch: 19, loss: 0.0983, train acc: 0.9637\n",
      "epoch: 19, loss: 0.0909, train acc: 0.9670\n",
      "epoch: 19, loss: 0.0867, train acc: 0.9665\n",
      "epoch: 19, loss: 0.0840, train acc: 0.9674\n",
      "epoch: 19, loss: 0.0916, train acc: 0.9657\n",
      "epoch: 19, loss: 0.0872, train acc: 0.9654\n",
      "epoch: 19, loss: 0.0958, train acc: 0.9621\n",
      "epoch: 19, loss: 0.0922, train acc: 0.9640\n",
      "epoch: 19, loss: 0.0939, train acc: 0.9648\n",
      "epoch: 19, loss: 0.0974, train acc: 0.9646\n",
      "epoch: 19, loss: 0.0968, train acc: 0.9645\n",
      "epoch: 19, loss: 0.0952, train acc: 0.9644\n",
      "epoch: 19, loss: 0.0913, train acc: 0.9663\n",
      "epoch: 19, loss: 0.0888, train acc: 0.9674\n",
      "epoch: 19, loss: 0.0857, train acc: 0.9684\n",
      "epoch: 19, loss: 0.0836, train acc: 0.9693\n",
      ">epoch: 19, val_acc: 0.8830, val_f1: 0.8172\n",
      "Namespace(model_type='tri_gcn', modules={'tgcn': True, 'semgcn': True, 'lexgcn': True}, num_layers={'tgcn': 2, 'semgcn': 2, 'lexgcn': 2}, year='2016', train_file='data/train2016restaurant.txt', test_file='data/test2016restaurant.txt', model_path='test_models/2016tri_gcn+concat_seed217_reg0.0005_drop0.4_cdrop0.4_lr2.5e-05_epochs20_adam', val_file='val.txt', log='log', bert_model='bert_large_uncased', cooc_path='cooc_matrix.csv', cooc=       100       1000   10000  10003     10005     10007     10009     1001   \\\n",
      "100      0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "1000     0.0  0.000000    0.0    0.0  0.000606  0.000303  0.000606  0.035120   \n",
      "10000    0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.333333   \n",
      "10003    0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "10005    0.0  0.333333    0.0    0.0  0.000000  0.166667  0.000000  0.166667   \n",
      "...      ...       ...    ...    ...       ...       ...       ...       ...   \n",
      "9992     0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "9993     0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "9996     0.0  0.500000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "9997     0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "9998     0.0  0.400000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "       10010  10015  ...     9986      9987      999    9990      9991   \\\n",
      "100      0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "1000     0.0    0.0  ...  0.002422  0.000303  0.062368    0.0  0.000606   \n",
      "10000    0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "10003    0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "10005    0.0    0.0  ...  0.000000  0.000000  0.333333    0.0  0.000000   \n",
      "...      ...    ...  ...       ...       ...       ...    ...       ...   \n",
      "9992     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "9993     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "9996     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "9997     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "9998     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "\n",
      "       9992   9993      9996   9997      9998   \n",
      "100      0.0    0.0  0.000000    0.0  0.000000  \n",
      "1000     0.0    0.0  0.000606    0.0  0.001211  \n",
      "10000    0.0    0.0  0.000000    0.0  0.000000  \n",
      "10003    0.0    0.0  0.000000    0.0  0.000000  \n",
      "10005    0.0    0.0  0.000000    0.0  0.000000  \n",
      "...      ...    ...       ...    ...       ...  \n",
      "9992     0.0    0.0  0.000000    0.0  0.000000  \n",
      "9993     0.0    0.0  0.000000    0.0  0.000000  \n",
      "9996     0.0    0.0  0.000000    0.0  0.000000  \n",
      "9997     0.0    0.0  0.000000    0.0  0.000000  \n",
      "9998     0.0    0.0  0.000000    0.0  0.000000  \n",
      "\n",
      "[13075 rows x 13075 columns], learning_rate=2.5e-05, dropout=0.4, concat_dropout=0.4, bert_dropout=0.2, l2reg=0.0005, num_epoch=20, batch_size=16, log_step=5, max_seq_len=100, polarities_dim=3, device=device(type='cuda'), seed=217, valset_ratio=0.1, do_train=True, do_eval=True, eval_epoch_num=0, fusion_type='concat', use_ensemble=True, save_models='none', print_sent=False, optim='adam', n_gpu=1)\n",
      "{'none': 0, 'acl': 1, 'acl:relcl': 2, 'advcl': 3, 'advcl:relcl': 4, 'advmod': 5, 'amod': 6, 'appos': 7, 'aux': 8, 'aux:pass': 9, 'case': 10, 'cc': 11, 'cc:preconj': 12, 'ccomp': 13, 'compound': 14, 'compound:prt': 15, 'conj': 16, 'cop': 17, 'csubj': 18, 'dep': 19, 'det': 20, 'det:predet': 21, 'discourse': 22, 'dislocated': 23, 'expl': 24, 'fixed': 25, 'flat': 26, 'iobj': 27, 'list': 28, 'mark': 29, 'nmod': 30, 'nmod:npmod': 31, 'nmod:poss': 32, 'nmod:tmod': 33, 'nsubj': 34, 'nsubj:outer': 35, 'nsubj:pass': 36, 'nummod': 37, 'obj': 38, 'obl': 39, 'obl:agent': 40, 'obl:npmod': 41, 'obl:tmod': 42, 'parataxis': 43, 'punct': 44, 'reparandum': 45, 'root': 46, 'vocative': 47, 'xcomp': 48}\n",
      "{'-1': 0, '0': 1, '1': 2}\n",
      "Model name 'bert_large_uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming 'bert_large_uncased' is a path or url to a directory containing tokenizer files.\n",
      "Didn't find file bert_large_uncased/added_tokens.json. We won't load it.\n",
      "Didn't find file bert_large_uncased/special_tokens_map.json. We won't load it.\n",
      "Didn't find file bert_large_uncased/tokenizer_config.json. We won't load it.\n",
      "loading file bert_large_uncased/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "{\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 3,\n",
      "  \"num_types\": 49,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file bert_large_uncased/pytorch_model.bin\n",
      "Weights of AsaTgcnSem not initialized from pretrained model: ['gate_weight', 'gate_bias', 'ensemble', 'TGCNLayers.0.weight', 'TGCNLayers.0.bias', 'TGCNLayers.0.dense.weight', 'TGCNLayers.1.weight', 'TGCNLayers.1.bias', 'TGCNLayers.1.dense.weight', 'SemGCNLayers.0.weight', 'SemGCNLayers.0.bias', 'SemGCNLayers.1.weight', 'SemGCNLayers.1.bias', 'LexGCNLayers.0.weight', 'LexGCNLayers.0.bias', 'LexGCNLayers.1.weight', 'LexGCNLayers.1.bias', 'fc_single.weight', 'fc_single.bias', 'ensemble_linear_tgcn.weight', 'ensemble_linear_tgcn.bias', 'ensemble_linear_semgcn.weight', 'ensemble_linear_semgcn.bias', 'ensemble_linear_lexgcn.weight', 'ensemble_linear_lexgcn.bias', 'dep_embedding.weight']\n",
      "Weights from pretrained model not used in AsaTgcnSem: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "[{'input_ids': tensor([  101, 13325,  2013,  3025,  8466,  2023,  2109,  2000,  2022,  1037,\n",
      "         2204,  2173,  1010,  2021,  2025,  2151,  2936,  1012,   102,  2173,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'valid_ids': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'segment_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'mem_valid_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'dep_adj_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'dep_value_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'polarity': 0, 'raw_text': 'judging from previous posts this used to be a good place , but not any longer .', 'aspect': 'place'}]\n",
      "[{'input_ids': tensor([  101, 13325,  2013,  3025,  8466,  2023,  2109,  2000,  2022,  1037,\n",
      "         2204,  2173,  1010,  2021,  2025,  2151,  2936,  1012,   102,  2173,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'valid_ids': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'segment_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'mem_valid_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'dep_adj_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'dep_value_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'polarity': 0, 'raw_text': 'judging from previous posts this used to be a good place , but not any longer .', 'aspect': 'place'}]\n",
      "[{'input_ids': tensor([  101,  4240,  2428,  2204, 10514,  6182,  1012,   102, 10514,  6182,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'valid_ids': tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'segment_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'mem_valid_ids': tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'dep_adj_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'dep_value_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 5,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'polarity': 2, 'raw_text': 'serves really good sushi .', 'aspect': 'sushi'}]\n",
      "cuda memory allocated: 1400893440\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 0\n",
      "epoch: 0, loss: 0.9474, train acc: 0.5625\n",
      "epoch: 0, loss: 0.8249, train acc: 0.6375\n",
      "epoch: 0, loss: 0.8673, train acc: 0.6375\n",
      "epoch: 0, loss: 0.8622, train acc: 0.6531\n",
      "epoch: 0, loss: 0.8559, train acc: 0.6450\n",
      "epoch: 0, loss: 0.8374, train acc: 0.6521\n",
      "epoch: 0, loss: 0.8180, train acc: 0.6589\n",
      "epoch: 0, loss: 0.8372, train acc: 0.6531\n",
      "epoch: 0, loss: 0.8413, train acc: 0.6514\n",
      "epoch: 0, loss: 0.8426, train acc: 0.6500\n",
      "epoch: 0, loss: 0.8386, train acc: 0.6568\n",
      "epoch: 0, loss: 0.8224, train acc: 0.6698\n",
      "epoch: 0, loss: 0.8352, train acc: 0.6596\n",
      "epoch: 0, loss: 0.8390, train acc: 0.6562\n",
      "epoch: 0, loss: 0.8416, train acc: 0.6500\n",
      "epoch: 0, loss: 0.8583, train acc: 0.6438\n",
      "epoch: 0, loss: 0.8543, train acc: 0.6478\n",
      "epoch: 0, loss: 0.8457, train acc: 0.6500\n",
      "epoch: 0, loss: 0.8486, train acc: 0.6474\n",
      "epoch: 0, loss: 0.8498, train acc: 0.6462\n",
      "epoch: 0, loss: 0.8460, train acc: 0.6494\n",
      ">epoch: 0, val_acc: 0.7234, val_f1: 0.2798\n",
      ">> epoch: 0, test_acc: 0.7431, test_f1: 0.2842\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 1\n",
      "epoch: 1, loss: 0.8799, train acc: 0.6562\n",
      "epoch: 1, loss: 0.8192, train acc: 0.6597\n",
      "epoch: 1, loss: 0.8605, train acc: 0.6518\n",
      "epoch: 1, loss: 0.8555, train acc: 0.6480\n",
      "epoch: 1, loss: 0.8576, train acc: 0.6510\n",
      "epoch: 1, loss: 0.8529, train acc: 0.6530\n",
      "epoch: 1, loss: 0.8530, train acc: 0.6471\n",
      "epoch: 1, loss: 0.8417, train acc: 0.6538\n",
      "epoch: 1, loss: 0.8469, train acc: 0.6562\n",
      "epoch: 1, loss: 0.8564, train acc: 0.6492\n",
      "epoch: 1, loss: 0.8719, train acc: 0.6412\n",
      "epoch: 1, loss: 0.8729, train acc: 0.6388\n",
      "epoch: 1, loss: 0.8724, train acc: 0.6348\n",
      "epoch: 1, loss: 0.8634, train acc: 0.6386\n",
      "epoch: 1, loss: 0.8713, train acc: 0.6377\n",
      "epoch: 1, loss: 0.8604, train acc: 0.6440\n",
      "epoch: 1, loss: 0.8524, train acc: 0.6481\n",
      "epoch: 1, loss: 0.8463, train acc: 0.6510\n",
      "epoch: 1, loss: 0.8405, train acc: 0.6523\n",
      "epoch: 1, loss: 0.8362, train acc: 0.6572\n",
      "epoch: 1, loss: 0.8304, train acc: 0.6575\n",
      ">epoch: 1, val_acc: 0.7234, val_f1: 0.2798\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 2\n",
      "epoch: 2, loss: 0.8705, train acc: 0.6458\n",
      "epoch: 2, loss: 0.8504, train acc: 0.6641\n",
      "epoch: 2, loss: 0.8185, train acc: 0.6683\n",
      "epoch: 2, loss: 0.8393, train acc: 0.6632\n",
      "epoch: 2, loss: 0.8188, train acc: 0.6766\n",
      "epoch: 2, loss: 0.8136, train acc: 0.6853\n",
      "epoch: 2, loss: 0.8248, train acc: 0.6875\n",
      "epoch: 2, loss: 0.8244, train acc: 0.6875\n",
      "epoch: 2, loss: 0.8048, train acc: 0.6860\n",
      "epoch: 2, loss: 0.7968, train acc: 0.6836\n",
      "epoch: 2, loss: 0.8033, train acc: 0.6781\n",
      "epoch: 2, loss: 0.8151, train acc: 0.6681\n",
      "epoch: 2, loss: 0.8101, train acc: 0.6736\n",
      "epoch: 2, loss: 0.8005, train acc: 0.6746\n",
      "epoch: 2, loss: 0.7961, train acc: 0.6738\n",
      "epoch: 2, loss: 0.7917, train acc: 0.6755\n",
      "epoch: 2, loss: 0.7892, train acc: 0.6807\n",
      "epoch: 2, loss: 0.7846, train acc: 0.6832\n",
      "epoch: 2, loss: 0.7851, train acc: 0.6855\n",
      "epoch: 2, loss: 0.7816, train acc: 0.6849\n",
      "epoch: 2, loss: 0.7829, train acc: 0.6820\n",
      ">epoch: 2, val_acc: 0.7234, val_f1: 0.2798\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 3\n",
      "epoch: 3, loss: 0.9749, train acc: 0.4375\n",
      "epoch: 3, loss: 0.9668, train acc: 0.5179\n",
      "epoch: 3, loss: 0.8741, train acc: 0.5677\n",
      "epoch: 3, loss: 0.8114, train acc: 0.6140\n",
      "epoch: 3, loss: 0.8057, train acc: 0.6136\n",
      "epoch: 3, loss: 0.8072, train acc: 0.6273\n",
      "epoch: 3, loss: 0.7871, train acc: 0.6387\n",
      "epoch: 3, loss: 0.7909, train acc: 0.6419\n",
      "epoch: 3, loss: 0.7724, train acc: 0.6607\n",
      "epoch: 3, loss: 0.7565, train acc: 0.6702\n",
      "epoch: 3, loss: 0.7771, train acc: 0.6695\n",
      "epoch: 3, loss: 0.7670, train acc: 0.6711\n",
      "epoch: 3, loss: 0.7707, train acc: 0.6613\n",
      "epoch: 3, loss: 0.7800, train acc: 0.6549\n",
      "epoch: 3, loss: 0.7742, train acc: 0.6606\n",
      "epoch: 3, loss: 0.7806, train acc: 0.6615\n",
      "epoch: 3, loss: 0.7768, train acc: 0.6631\n",
      "epoch: 3, loss: 0.7821, train acc: 0.6631\n",
      "epoch: 3, loss: 0.7852, train acc: 0.6630\n",
      "epoch: 3, loss: 0.7787, train acc: 0.6662\n",
      "epoch: 3, loss: 0.7908, train acc: 0.6648\n",
      ">epoch: 3, val_acc: 0.7234, val_f1: 0.2798\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 4\n",
      "epoch: 4, loss: 0.6777, train acc: 0.8125\n",
      "epoch: 4, loss: 0.6652, train acc: 0.7188\n",
      "epoch: 4, loss: 0.8174, train acc: 0.6534\n",
      "epoch: 4, loss: 0.8058, train acc: 0.6523\n",
      "epoch: 4, loss: 0.7999, train acc: 0.6548\n",
      "epoch: 4, loss: 0.8205, train acc: 0.6466\n",
      "epoch: 4, loss: 0.8125, train acc: 0.6512\n",
      "epoch: 4, loss: 0.8177, train acc: 0.6476\n",
      "epoch: 4, loss: 0.8344, train acc: 0.6448\n",
      "epoch: 4, loss: 0.8291, train acc: 0.6454\n",
      "epoch: 4, loss: 0.8184, train acc: 0.6544\n",
      "epoch: 4, loss: 0.8064, train acc: 0.6596\n",
      "epoch: 4, loss: 0.8027, train acc: 0.6639\n",
      "epoch: 4, loss: 0.7956, train acc: 0.6676\n",
      "epoch: 4, loss: 0.7925, train acc: 0.6752\n",
      "epoch: 4, loss: 0.7952, train acc: 0.6752\n",
      "epoch: 4, loss: 0.7993, train acc: 0.6744\n",
      "epoch: 4, loss: 0.7936, train acc: 0.6751\n",
      "epoch: 4, loss: 0.7907, train acc: 0.6772\n",
      "epoch: 4, loss: 0.7933, train acc: 0.6751\n",
      "epoch: 4, loss: 0.7867, train acc: 0.6770\n",
      "epoch: 4, loss: 0.7791, train acc: 0.6809\n",
      ">epoch: 4, val_acc: 0.7287, val_f1: 0.3087\n",
      ">> epoch: 4, test_acc: 0.7415, test_f1: 0.2886\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 5\n",
      "epoch: 5, loss: 0.7274, train acc: 0.7500\n",
      "epoch: 5, loss: 0.7046, train acc: 0.7125\n",
      "epoch: 5, loss: 0.7008, train acc: 0.7083\n",
      "epoch: 5, loss: 0.7176, train acc: 0.7000\n",
      "epoch: 5, loss: 0.7516, train acc: 0.6875\n",
      "epoch: 5, loss: 0.7811, train acc: 0.6813\n",
      "epoch: 5, loss: 0.8094, train acc: 0.6643\n",
      "epoch: 5, loss: 0.7987, train acc: 0.6672\n",
      "epoch: 5, loss: 0.7857, train acc: 0.6653\n",
      "epoch: 5, loss: 0.7822, train acc: 0.6663\n",
      "epoch: 5, loss: 0.7855, train acc: 0.6693\n",
      "epoch: 5, loss: 0.7799, train acc: 0.6740\n",
      "epoch: 5, loss: 0.7685, train acc: 0.6808\n",
      "epoch: 5, loss: 0.7757, train acc: 0.6786\n",
      "epoch: 5, loss: 0.7700, train acc: 0.6783\n",
      "epoch: 5, loss: 0.7647, train acc: 0.6813\n",
      "epoch: 5, loss: 0.7744, train acc: 0.6757\n",
      "epoch: 5, loss: 0.7744, train acc: 0.6750\n",
      "epoch: 5, loss: 0.7657, train acc: 0.6809\n",
      "epoch: 5, loss: 0.7748, train acc: 0.6794\n",
      "epoch: 5, loss: 0.7809, train acc: 0.6762\n",
      ">epoch: 5, val_acc: 0.7234, val_f1: 0.2798\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 6\n",
      "epoch: 6, loss: 0.8191, train acc: 0.6562\n",
      "epoch: 6, loss: 0.8428, train acc: 0.6111\n",
      "epoch: 6, loss: 0.8456, train acc: 0.6250\n",
      "epoch: 6, loss: 0.8699, train acc: 0.6086\n",
      "epoch: 6, loss: 0.8251, train acc: 0.6302\n",
      "epoch: 6, loss: 0.8020, train acc: 0.6358\n",
      "epoch: 6, loss: 0.8199, train acc: 0.6379\n",
      "epoch: 6, loss: 0.8183, train acc: 0.6442\n",
      "epoch: 6, loss: 0.8110, train acc: 0.6506\n",
      "epoch: 6, loss: 0.7997, train acc: 0.6505\n",
      "epoch: 6, loss: 0.7906, train acc: 0.6586\n",
      "epoch: 6, loss: 0.7839, train acc: 0.6621\n",
      "epoch: 6, loss: 0.7923, train acc: 0.6602\n",
      "epoch: 6, loss: 0.7922, train acc: 0.6630\n",
      "epoch: 6, loss: 0.7921, train acc: 0.6622\n",
      "epoch: 6, loss: 0.7940, train acc: 0.6598\n",
      "epoch: 6, loss: 0.7911, train acc: 0.6629\n",
      "epoch: 6, loss: 0.7873, train acc: 0.6664\n",
      "epoch: 6, loss: 0.7793, train acc: 0.6695\n",
      "epoch: 6, loss: 0.7864, train acc: 0.6698\n",
      "epoch: 6, loss: 0.7880, train acc: 0.6701\n",
      ">epoch: 6, val_acc: 0.7287, val_f1: 0.2955\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 7\n",
      "epoch: 7, loss: 0.7875, train acc: 0.6250\n",
      "epoch: 7, loss: 0.7834, train acc: 0.6406\n",
      "epoch: 7, loss: 0.7555, train acc: 0.6635\n",
      "epoch: 7, loss: 0.7544, train acc: 0.6701\n",
      "epoch: 7, loss: 0.7556, train acc: 0.6793\n",
      "epoch: 7, loss: 0.7582, train acc: 0.6853\n",
      "epoch: 7, loss: 0.7504, train acc: 0.6856\n",
      "epoch: 7, loss: 0.7423, train acc: 0.6924\n",
      "epoch: 7, loss: 0.7309, train acc: 0.7049\n",
      "epoch: 7, loss: 0.7387, train acc: 0.6966\n",
      "epoch: 7, loss: 0.7330, train acc: 0.6981\n",
      "epoch: 7, loss: 0.7353, train acc: 0.6961\n",
      "epoch: 7, loss: 0.7396, train acc: 0.6915\n",
      "epoch: 7, loss: 0.7466, train acc: 0.6811\n",
      "epoch: 7, loss: 0.7545, train acc: 0.6738\n",
      "epoch: 7, loss: 0.7599, train acc: 0.6715\n",
      "epoch: 7, loss: 0.7623, train acc: 0.6702\n",
      "epoch: 7, loss: 0.7558, train acc: 0.6768\n",
      "epoch: 7, loss: 0.7591, train acc: 0.6788\n",
      "epoch: 7, loss: 0.7688, train acc: 0.6747\n",
      "epoch: 7, loss: 0.7635, train acc: 0.6754\n",
      ">epoch: 7, val_acc: 0.7234, val_f1: 0.2798\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 8\n",
      "epoch: 8, loss: 0.7383, train acc: 0.6250\n",
      "epoch: 8, loss: 0.6883, train acc: 0.7054\n",
      "epoch: 8, loss: 0.7076, train acc: 0.7240\n",
      "epoch: 8, loss: 0.6993, train acc: 0.7206\n",
      "epoch: 8, loss: 0.7022, train acc: 0.7102\n",
      "epoch: 8, loss: 0.7219, train acc: 0.7083\n",
      "epoch: 8, loss: 0.7220, train acc: 0.7012\n",
      "epoch: 8, loss: 0.7004, train acc: 0.7061\n",
      "epoch: 8, loss: 0.7371, train acc: 0.7009\n",
      "epoch: 8, loss: 0.7433, train acc: 0.6941\n",
      "epoch: 8, loss: 0.7357, train acc: 0.6971\n",
      "epoch: 8, loss: 0.7309, train acc: 0.6974\n",
      "epoch: 8, loss: 0.7244, train acc: 0.7026\n",
      "epoch: 8, loss: 0.7312, train acc: 0.6959\n",
      "epoch: 8, loss: 0.7313, train acc: 0.6918\n",
      "epoch: 8, loss: 0.7243, train acc: 0.6964\n",
      "epoch: 8, loss: 0.7144, train acc: 0.6997\n",
      "epoch: 8, loss: 0.7044, train acc: 0.7047\n",
      "epoch: 8, loss: 0.6998, train acc: 0.7086\n",
      "epoch: 8, loss: 0.7045, train acc: 0.7075\n",
      "epoch: 8, loss: 0.6933, train acc: 0.7132\n",
      ">epoch: 8, val_acc: 0.8138, val_f1: 0.5180\n",
      ">> epoch: 8, test_acc: 0.7969, test_f1: 0.5027\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 9\n",
      "epoch: 9, loss: 0.9001, train acc: 0.5625\n",
      "epoch: 9, loss: 0.5919, train acc: 0.7292\n",
      "epoch: 9, loss: 0.5935, train acc: 0.7386\n",
      "epoch: 9, loss: 0.6074, train acc: 0.7539\n",
      "epoch: 9, loss: 0.6030, train acc: 0.7560\n",
      "epoch: 9, loss: 0.5959, train acc: 0.7596\n",
      "epoch: 9, loss: 0.5588, train acc: 0.7742\n",
      "epoch: 9, loss: 0.5442, train acc: 0.7830\n",
      "epoch: 9, loss: 0.5398, train acc: 0.7896\n",
      "epoch: 9, loss: 0.5373, train acc: 0.7880\n",
      "epoch: 9, loss: 0.5352, train acc: 0.7892\n",
      "epoch: 9, loss: 0.5250, train acc: 0.7924\n",
      "epoch: 9, loss: 0.5143, train acc: 0.7961\n",
      "epoch: 9, loss: 0.5131, train acc: 0.8011\n",
      "epoch: 9, loss: 0.5133, train acc: 0.7993\n",
      "epoch: 9, loss: 0.5092, train acc: 0.7993\n",
      "epoch: 9, loss: 0.5076, train acc: 0.8002\n",
      "epoch: 9, loss: 0.5045, train acc: 0.8009\n",
      "epoch: 9, loss: 0.5027, train acc: 0.8008\n",
      "epoch: 9, loss: 0.5094, train acc: 0.8001\n",
      "epoch: 9, loss: 0.5089, train acc: 0.8001\n",
      "epoch: 9, loss: 0.5054, train acc: 0.8014\n",
      ">epoch: 9, val_acc: 0.8564, val_f1: 0.5428\n",
      ">> epoch: 9, test_acc: 0.8123, test_f1: 0.4806\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 10\n",
      "epoch: 10, loss: 0.4511, train acc: 0.8000\n",
      "epoch: 10, loss: 0.3653, train acc: 0.8500\n",
      "epoch: 10, loss: 0.3766, train acc: 0.8625\n",
      "epoch: 10, loss: 0.3543, train acc: 0.8656\n",
      "epoch: 10, loss: 0.3544, train acc: 0.8650\n",
      "epoch: 10, loss: 0.3910, train acc: 0.8604\n",
      "epoch: 10, loss: 0.4005, train acc: 0.8607\n",
      "epoch: 10, loss: 0.3947, train acc: 0.8594\n",
      "epoch: 10, loss: 0.3977, train acc: 0.8597\n",
      "epoch: 10, loss: 0.3902, train acc: 0.8600\n",
      "epoch: 10, loss: 0.4271, train acc: 0.8568\n",
      "epoch: 10, loss: 0.4317, train acc: 0.8510\n",
      "epoch: 10, loss: 0.4384, train acc: 0.8442\n",
      "epoch: 10, loss: 0.4492, train acc: 0.8384\n",
      "epoch: 10, loss: 0.4667, train acc: 0.8317\n",
      "epoch: 10, loss: 0.4644, train acc: 0.8320\n",
      "epoch: 10, loss: 0.4658, train acc: 0.8338\n",
      "epoch: 10, loss: 0.4667, train acc: 0.8354\n",
      "epoch: 10, loss: 0.4679, train acc: 0.8349\n",
      "epoch: 10, loss: 0.4615, train acc: 0.8375\n",
      "epoch: 10, loss: 0.4596, train acc: 0.8405\n",
      ">epoch: 10, val_acc: 0.8511, val_f1: 0.5524\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 11\n",
      "epoch: 11, loss: 0.1914, train acc: 0.9375\n",
      "epoch: 11, loss: 0.3118, train acc: 0.9097\n",
      "epoch: 11, loss: 0.3240, train acc: 0.8973\n",
      "epoch: 11, loss: 0.3852, train acc: 0.8783\n",
      "epoch: 11, loss: 0.3637, train acc: 0.8906\n",
      "epoch: 11, loss: 0.3674, train acc: 0.8922\n",
      "epoch: 11, loss: 0.3904, train acc: 0.8805\n",
      "epoch: 11, loss: 0.3902, train acc: 0.8798\n",
      "epoch: 11, loss: 0.3788, train acc: 0.8821\n",
      "epoch: 11, loss: 0.3632, train acc: 0.8890\n",
      "epoch: 11, loss: 0.3652, train acc: 0.8877\n",
      "epoch: 11, loss: 0.3564, train acc: 0.8909\n",
      "epoch: 11, loss: 0.3507, train acc: 0.8916\n",
      "epoch: 11, loss: 0.3649, train acc: 0.8868\n",
      "epoch: 11, loss: 0.3624, train acc: 0.8877\n",
      "epoch: 11, loss: 0.3680, train acc: 0.8877\n",
      "epoch: 11, loss: 0.3680, train acc: 0.8869\n",
      "epoch: 11, loss: 0.3770, train acc: 0.8820\n",
      "epoch: 11, loss: 0.3803, train acc: 0.8803\n",
      "epoch: 11, loss: 0.3789, train acc: 0.8807\n",
      "epoch: 11, loss: 0.3736, train acc: 0.8810\n",
      ">epoch: 11, val_acc: 0.8564, val_f1: 0.5530\n",
      ">> epoch: 11, test_acc: 0.8462, test_f1: 0.5274\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 12\n",
      "epoch: 12, loss: 0.2574, train acc: 0.9375\n",
      "epoch: 12, loss: 0.3141, train acc: 0.9141\n",
      "epoch: 12, loss: 0.3510, train acc: 0.8798\n",
      "epoch: 12, loss: 0.3401, train acc: 0.8889\n",
      "epoch: 12, loss: 0.3200, train acc: 0.8940\n",
      "epoch: 12, loss: 0.2979, train acc: 0.9018\n",
      "epoch: 12, loss: 0.2945, train acc: 0.9034\n",
      "epoch: 12, loss: 0.3063, train acc: 0.8964\n",
      "epoch: 12, loss: 0.3139, train acc: 0.8968\n",
      "epoch: 12, loss: 0.3109, train acc: 0.8971\n",
      "epoch: 12, loss: 0.3171, train acc: 0.8950\n",
      "epoch: 12, loss: 0.3240, train acc: 0.8901\n",
      "epoch: 12, loss: 0.3318, train acc: 0.8869\n",
      "epoch: 12, loss: 0.3261, train acc: 0.8860\n",
      "epoch: 12, loss: 0.3327, train acc: 0.8844\n",
      "epoch: 12, loss: 0.3230, train acc: 0.8886\n",
      "epoch: 12, loss: 0.3242, train acc: 0.8886\n",
      "epoch: 12, loss: 0.3270, train acc: 0.8871\n",
      "epoch: 12, loss: 0.3307, train acc: 0.8851\n",
      "epoch: 12, loss: 0.3276, train acc: 0.8858\n",
      "epoch: 12, loss: 0.3268, train acc: 0.8841\n",
      ">epoch: 12, val_acc: 0.8670, val_f1: 0.5615\n",
      ">> epoch: 12, test_acc: 0.8446, test_f1: 0.5231\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 13\n",
      "epoch: 13, loss: 0.0652, train acc: 1.0000\n",
      "epoch: 13, loss: 0.3114, train acc: 0.9018\n",
      "epoch: 13, loss: 0.2779, train acc: 0.9062\n",
      "epoch: 13, loss: 0.2813, train acc: 0.8934\n",
      "epoch: 13, loss: 0.2745, train acc: 0.8977\n",
      "epoch: 13, loss: 0.2632, train acc: 0.9028\n",
      "epoch: 13, loss: 0.2476, train acc: 0.9102\n",
      "epoch: 13, loss: 0.2588, train acc: 0.9037\n",
      "epoch: 13, loss: 0.2670, train acc: 0.8958\n",
      "epoch: 13, loss: 0.2590, train acc: 0.9016\n",
      "epoch: 13, loss: 0.2480, train acc: 0.9075\n",
      "epoch: 13, loss: 0.2523, train acc: 0.9101\n",
      "epoch: 13, loss: 0.2523, train acc: 0.9093\n",
      "epoch: 13, loss: 0.2498, train acc: 0.9086\n",
      "epoch: 13, loss: 0.2726, train acc: 0.9036\n",
      "epoch: 13, loss: 0.2701, train acc: 0.9042\n",
      "epoch: 13, loss: 0.2847, train acc: 0.8994\n",
      "epoch: 13, loss: 0.2882, train acc: 0.8980\n",
      "epoch: 13, loss: 0.2821, train acc: 0.9008\n",
      "epoch: 13, loss: 0.2809, train acc: 0.9008\n",
      "epoch: 13, loss: 0.2745, train acc: 0.9026\n",
      ">epoch: 13, val_acc: 0.8617, val_f1: 0.5516\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 14\n",
      "epoch: 14, loss: 0.0138, train acc: 1.0000\n",
      "epoch: 14, loss: 0.1790, train acc: 0.9375\n",
      "epoch: 14, loss: 0.2540, train acc: 0.9034\n",
      "epoch: 14, loss: 0.2477, train acc: 0.9062\n",
      "epoch: 14, loss: 0.2347, train acc: 0.9077\n",
      "epoch: 14, loss: 0.2244, train acc: 0.9111\n",
      "epoch: 14, loss: 0.2157, train acc: 0.9133\n",
      "epoch: 14, loss: 0.2109, train acc: 0.9132\n",
      "epoch: 14, loss: 0.2109, train acc: 0.9162\n",
      "epoch: 14, loss: 0.2537, train acc: 0.9090\n",
      "epoch: 14, loss: 0.2518, train acc: 0.9105\n",
      "epoch: 14, loss: 0.2530, train acc: 0.9107\n",
      "epoch: 14, loss: 0.2549, train acc: 0.9088\n",
      "epoch: 14, loss: 0.2598, train acc: 0.9044\n",
      "epoch: 14, loss: 0.2568, train acc: 0.9049\n",
      "epoch: 14, loss: 0.2604, train acc: 0.9021\n",
      "epoch: 14, loss: 0.2678, train acc: 0.9005\n",
      "epoch: 14, loss: 0.2807, train acc: 0.8961\n",
      "epoch: 14, loss: 0.2767, train acc: 0.8984\n",
      "epoch: 14, loss: 0.2749, train acc: 0.8965\n",
      "epoch: 14, loss: 0.2735, train acc: 0.8960\n",
      "epoch: 14, loss: 0.2798, train acc: 0.8918\n",
      ">epoch: 14, val_acc: 0.8564, val_f1: 0.5564\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 15\n",
      "epoch: 15, loss: 0.1549, train acc: 0.9375\n",
      "epoch: 15, loss: 0.1800, train acc: 0.9313\n",
      "epoch: 15, loss: 0.1835, train acc: 0.9250\n",
      "epoch: 15, loss: 0.2110, train acc: 0.9219\n",
      "epoch: 15, loss: 0.2147, train acc: 0.9150\n",
      "epoch: 15, loss: 0.2425, train acc: 0.9083\n",
      "epoch: 15, loss: 0.2331, train acc: 0.9107\n",
      "epoch: 15, loss: 0.2425, train acc: 0.9047\n",
      "epoch: 15, loss: 0.2385, train acc: 0.9069\n",
      "epoch: 15, loss: 0.2366, train acc: 0.9075\n",
      "epoch: 15, loss: 0.2412, train acc: 0.9045\n",
      "epoch: 15, loss: 0.2348, train acc: 0.9052\n",
      "epoch: 15, loss: 0.2306, train acc: 0.9048\n",
      "epoch: 15, loss: 0.2307, train acc: 0.9036\n",
      "epoch: 15, loss: 0.2305, train acc: 0.9050\n",
      "epoch: 15, loss: 0.2347, train acc: 0.9023\n",
      "epoch: 15, loss: 0.2326, train acc: 0.9037\n",
      "epoch: 15, loss: 0.2299, train acc: 0.9035\n",
      "epoch: 15, loss: 0.2260, train acc: 0.9053\n",
      "epoch: 15, loss: 0.2268, train acc: 0.9050\n",
      "epoch: 15, loss: 0.2217, train acc: 0.9077\n",
      ">epoch: 15, val_acc: 0.8564, val_f1: 0.5564\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 16\n",
      "epoch: 16, loss: 0.1210, train acc: 0.9531\n",
      "epoch: 16, loss: 0.1740, train acc: 0.9306\n",
      "epoch: 16, loss: 0.1897, train acc: 0.9196\n",
      "epoch: 16, loss: 0.2345, train acc: 0.9013\n",
      "epoch: 16, loss: 0.2449, train acc: 0.9010\n",
      "epoch: 16, loss: 0.2339, train acc: 0.9030\n",
      "epoch: 16, loss: 0.2349, train acc: 0.8989\n",
      "epoch: 16, loss: 0.2356, train acc: 0.8958\n",
      "epoch: 16, loss: 0.2237, train acc: 0.9006\n",
      "epoch: 16, loss: 0.2138, train acc: 0.9082\n",
      "epoch: 16, loss: 0.2208, train acc: 0.9074\n",
      "epoch: 16, loss: 0.2186, train acc: 0.9089\n",
      "epoch: 16, loss: 0.2127, train acc: 0.9121\n",
      "epoch: 16, loss: 0.2196, train acc: 0.9076\n",
      "epoch: 16, loss: 0.2159, train acc: 0.9088\n",
      "epoch: 16, loss: 0.2097, train acc: 0.9114\n",
      "epoch: 16, loss: 0.2175, train acc: 0.9055\n",
      "epoch: 16, loss: 0.2213, train acc: 0.9059\n",
      "epoch: 16, loss: 0.2187, train acc: 0.9069\n",
      "epoch: 16, loss: 0.2221, train acc: 0.9078\n",
      "epoch: 16, loss: 0.2222, train acc: 0.9081\n",
      ">epoch: 16, val_acc: 0.8457, val_f1: 0.5499\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 17\n",
      "epoch: 17, loss: 0.2207, train acc: 0.9583\n",
      "epoch: 17, loss: 0.2587, train acc: 0.9141\n",
      "epoch: 17, loss: 0.2443, train acc: 0.9183\n",
      "epoch: 17, loss: 0.2755, train acc: 0.9062\n",
      "epoch: 17, loss: 0.2758, train acc: 0.8967\n",
      "epoch: 17, loss: 0.2639, train acc: 0.8996\n",
      "epoch: 17, loss: 0.2606, train acc: 0.8977\n",
      "epoch: 17, loss: 0.2643, train acc: 0.8964\n",
      "epoch: 17, loss: 0.2616, train acc: 0.8953\n",
      "epoch: 17, loss: 0.2501, train acc: 0.8997\n",
      "epoch: 17, loss: 0.2532, train acc: 0.8974\n",
      "epoch: 17, loss: 0.2547, train acc: 0.8976\n",
      "epoch: 17, loss: 0.2440, train acc: 0.9018\n",
      "epoch: 17, loss: 0.2381, train acc: 0.9026\n",
      "epoch: 17, loss: 0.2323, train acc: 0.9041\n",
      "epoch: 17, loss: 0.2323, train acc: 0.9006\n",
      "epoch: 17, loss: 0.2407, train acc: 0.8998\n",
      "epoch: 17, loss: 0.2339, train acc: 0.9020\n",
      "epoch: 17, loss: 0.2314, train acc: 0.9019\n",
      "epoch: 17, loss: 0.2259, train acc: 0.9043\n",
      "epoch: 17, loss: 0.2238, train acc: 0.9041\n",
      ">epoch: 17, val_acc: 0.8351, val_f1: 0.5333\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 18\n",
      "epoch: 18, loss: 0.1289, train acc: 0.9062\n",
      "epoch: 18, loss: 0.1662, train acc: 0.9107\n",
      "epoch: 18, loss: 0.1539, train acc: 0.9115\n",
      "epoch: 18, loss: 0.1412, train acc: 0.9154\n",
      "epoch: 18, loss: 0.1765, train acc: 0.9006\n",
      "epoch: 18, loss: 0.1722, train acc: 0.9074\n",
      "epoch: 18, loss: 0.1770, train acc: 0.9043\n",
      "epoch: 18, loss: 0.1682, train acc: 0.9088\n",
      "epoch: 18, loss: 0.1607, train acc: 0.9137\n",
      "epoch: 18, loss: 0.1699, train acc: 0.9162\n",
      "epoch: 18, loss: 0.1793, train acc: 0.9123\n",
      "epoch: 18, loss: 0.1818, train acc: 0.9101\n",
      "epoch: 18, loss: 0.1882, train acc: 0.9123\n",
      "epoch: 18, loss: 0.1962, train acc: 0.9086\n",
      "epoch: 18, loss: 0.1966, train acc: 0.9097\n",
      "epoch: 18, loss: 0.1957, train acc: 0.9131\n",
      "epoch: 18, loss: 0.2012, train acc: 0.9116\n",
      "epoch: 18, loss: 0.2037, train acc: 0.9109\n",
      "epoch: 18, loss: 0.2077, train acc: 0.9124\n",
      "epoch: 18, loss: 0.2132, train acc: 0.9104\n",
      "epoch: 18, loss: 0.2141, train acc: 0.9112\n",
      ">epoch: 18, val_acc: 0.8511, val_f1: 0.5509\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 19\n",
      "epoch: 19, loss: 0.2163, train acc: 0.9375\n",
      "epoch: 19, loss: 0.1590, train acc: 0.9271\n",
      "epoch: 19, loss: 0.1622, train acc: 0.9148\n",
      "epoch: 19, loss: 0.1727, train acc: 0.9102\n",
      "epoch: 19, loss: 0.2010, train acc: 0.9018\n",
      "epoch: 19, loss: 0.1981, train acc: 0.9038\n",
      "epoch: 19, loss: 0.1884, train acc: 0.9153\n",
      "epoch: 19, loss: 0.1906, train acc: 0.9115\n",
      "epoch: 19, loss: 0.1912, train acc: 0.9146\n",
      "epoch: 19, loss: 0.1898, train acc: 0.9144\n",
      "epoch: 19, loss: 0.1997, train acc: 0.9093\n",
      "epoch: 19, loss: 0.1993, train acc: 0.9096\n",
      "epoch: 19, loss: 0.2103, train acc: 0.9068\n",
      "epoch: 19, loss: 0.2134, train acc: 0.9053\n",
      "epoch: 19, loss: 0.2135, train acc: 0.9040\n",
      "epoch: 19, loss: 0.2070, train acc: 0.9079\n",
      "epoch: 19, loss: 0.2021, train acc: 0.9090\n",
      "epoch: 19, loss: 0.1976, train acc: 0.9099\n",
      "epoch: 19, loss: 0.1966, train acc: 0.9107\n",
      "epoch: 19, loss: 0.1966, train acc: 0.9108\n",
      "epoch: 19, loss: 0.2026, train acc: 0.9103\n",
      "epoch: 19, loss: 0.2053, train acc: 0.9078\n",
      ">epoch: 19, val_acc: 0.8245, val_f1: 0.5864\n",
      "Namespace(model_type='tri_gcn', modules={'tgcn': True, 'semgcn': True, 'lexgcn': True}, num_layers={'tgcn': 2, 'semgcn': 2, 'lexgcn': 2}, year='2016', train_file='data/train2016restaurant.txt', test_file='data/test2016restaurant.txt', model_path='test_models/2016tri_gcn+concat_seed658_reg0.0005_drop0.4_cdrop0.4_lr2.5e-05_epochs20_adam', val_file='val.txt', log='log', bert_model='bert_large_uncased', cooc_path='cooc_matrix.csv', cooc=       100       1000   10000  10003     10005     10007     10009     1001   \\\n",
      "100      0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "1000     0.0  0.000000    0.0    0.0  0.000606  0.000303  0.000606  0.035120   \n",
      "10000    0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.333333   \n",
      "10003    0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "10005    0.0  0.333333    0.0    0.0  0.000000  0.166667  0.000000  0.166667   \n",
      "...      ...       ...    ...    ...       ...       ...       ...       ...   \n",
      "9992     0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "9993     0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "9996     0.0  0.500000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "9997     0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "9998     0.0  0.400000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "       10010  10015  ...     9986      9987      999    9990      9991   \\\n",
      "100      0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "1000     0.0    0.0  ...  0.002422  0.000303  0.062368    0.0  0.000606   \n",
      "10000    0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "10003    0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "10005    0.0    0.0  ...  0.000000  0.000000  0.333333    0.0  0.000000   \n",
      "...      ...    ...  ...       ...       ...       ...    ...       ...   \n",
      "9992     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "9993     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "9996     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "9997     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "9998     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "\n",
      "       9992   9993      9996   9997      9998   \n",
      "100      0.0    0.0  0.000000    0.0  0.000000  \n",
      "1000     0.0    0.0  0.000606    0.0  0.001211  \n",
      "10000    0.0    0.0  0.000000    0.0  0.000000  \n",
      "10003    0.0    0.0  0.000000    0.0  0.000000  \n",
      "10005    0.0    0.0  0.000000    0.0  0.000000  \n",
      "...      ...    ...       ...    ...       ...  \n",
      "9992     0.0    0.0  0.000000    0.0  0.000000  \n",
      "9993     0.0    0.0  0.000000    0.0  0.000000  \n",
      "9996     0.0    0.0  0.000000    0.0  0.000000  \n",
      "9997     0.0    0.0  0.000000    0.0  0.000000  \n",
      "9998     0.0    0.0  0.000000    0.0  0.000000  \n",
      "\n",
      "[13075 rows x 13075 columns], learning_rate=2.5e-05, dropout=0.4, concat_dropout=0.4, bert_dropout=0.2, l2reg=0.0005, num_epoch=20, batch_size=16, log_step=5, max_seq_len=100, polarities_dim=3, device=device(type='cuda'), seed=658, valset_ratio=0.1, do_train=True, do_eval=True, eval_epoch_num=0, fusion_type='concat', use_ensemble=True, save_models='none', print_sent=False, optim='adam', n_gpu=1)\n",
      "{'none': 0, 'acl': 1, 'acl:relcl': 2, 'advcl': 3, 'advcl:relcl': 4, 'advmod': 5, 'amod': 6, 'appos': 7, 'aux': 8, 'aux:pass': 9, 'case': 10, 'cc': 11, 'cc:preconj': 12, 'ccomp': 13, 'compound': 14, 'compound:prt': 15, 'conj': 16, 'cop': 17, 'csubj': 18, 'dep': 19, 'det': 20, 'det:predet': 21, 'discourse': 22, 'dislocated': 23, 'expl': 24, 'fixed': 25, 'flat': 26, 'iobj': 27, 'list': 28, 'mark': 29, 'nmod': 30, 'nmod:npmod': 31, 'nmod:poss': 32, 'nmod:tmod': 33, 'nsubj': 34, 'nsubj:outer': 35, 'nsubj:pass': 36, 'nummod': 37, 'obj': 38, 'obl': 39, 'obl:agent': 40, 'obl:npmod': 41, 'obl:tmod': 42, 'parataxis': 43, 'punct': 44, 'reparandum': 45, 'root': 46, 'vocative': 47, 'xcomp': 48}\n",
      "{'-1': 0, '0': 1, '1': 2}\n",
      "Model name 'bert_large_uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming 'bert_large_uncased' is a path or url to a directory containing tokenizer files.\n",
      "Didn't find file bert_large_uncased/added_tokens.json. We won't load it.\n",
      "Didn't find file bert_large_uncased/special_tokens_map.json. We won't load it.\n",
      "Didn't find file bert_large_uncased/tokenizer_config.json. We won't load it.\n",
      "loading file bert_large_uncased/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "{\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 3,\n",
      "  \"num_types\": 49,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file bert_large_uncased/pytorch_model.bin\n",
      "Weights of AsaTgcnSem not initialized from pretrained model: ['gate_weight', 'gate_bias', 'ensemble', 'TGCNLayers.0.weight', 'TGCNLayers.0.bias', 'TGCNLayers.0.dense.weight', 'TGCNLayers.1.weight', 'TGCNLayers.1.bias', 'TGCNLayers.1.dense.weight', 'SemGCNLayers.0.weight', 'SemGCNLayers.0.bias', 'SemGCNLayers.1.weight', 'SemGCNLayers.1.bias', 'LexGCNLayers.0.weight', 'LexGCNLayers.0.bias', 'LexGCNLayers.1.weight', 'LexGCNLayers.1.bias', 'fc_single.weight', 'fc_single.bias', 'ensemble_linear_tgcn.weight', 'ensemble_linear_tgcn.bias', 'ensemble_linear_semgcn.weight', 'ensemble_linear_semgcn.bias', 'ensemble_linear_lexgcn.weight', 'ensemble_linear_lexgcn.bias', 'dep_embedding.weight']\n",
      "Weights from pretrained model not used in AsaTgcnSem: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "[{'input_ids': tensor([  101, 13325,  2013,  3025,  8466,  2023,  2109,  2000,  2022,  1037,\n",
      "         2204,  2173,  1010,  2021,  2025,  2151,  2936,  1012,   102,  2173,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'valid_ids': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'segment_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'mem_valid_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'dep_adj_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'dep_value_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'polarity': 0, 'raw_text': 'judging from previous posts this used to be a good place , but not any longer .', 'aspect': 'place'}]\n",
      "[{'input_ids': tensor([  101, 13325,  2013,  3025,  8466,  2023,  2109,  2000,  2022,  1037,\n",
      "         2204,  2173,  1010,  2021,  2025,  2151,  2936,  1012,   102,  2173,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'valid_ids': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'segment_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'mem_valid_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'dep_adj_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'dep_value_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'polarity': 0, 'raw_text': 'judging from previous posts this used to be a good place , but not any longer .', 'aspect': 'place'}]\n",
      "[{'input_ids': tensor([  101,  4240,  2428,  2204, 10514,  6182,  1012,   102, 10514,  6182,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'valid_ids': tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'segment_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'mem_valid_ids': tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'dep_adj_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'dep_value_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 5,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'polarity': 2, 'raw_text': 'serves really good sushi .', 'aspect': 'sushi'}]\n",
      "cuda memory allocated: 1400893440\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 0\n",
      "epoch: 0, loss: 1.0444, train acc: 0.5125\n",
      "epoch: 0, loss: 0.9124, train acc: 0.6062\n",
      "epoch: 0, loss: 0.8901, train acc: 0.6292\n",
      "epoch: 0, loss: 0.8664, train acc: 0.6469\n",
      "epoch: 0, loss: 0.8258, train acc: 0.6625\n",
      "epoch: 0, loss: 0.8169, train acc: 0.6646\n",
      "epoch: 0, loss: 0.8155, train acc: 0.6607\n",
      "epoch: 0, loss: 0.8048, train acc: 0.6672\n",
      "epoch: 0, loss: 0.8080, train acc: 0.6639\n",
      "epoch: 0, loss: 0.8007, train acc: 0.6650\n",
      "epoch: 0, loss: 0.8070, train acc: 0.6693\n",
      "epoch: 0, loss: 0.7980, train acc: 0.6750\n",
      "epoch: 0, loss: 0.7943, train acc: 0.6769\n",
      "epoch: 0, loss: 0.7914, train acc: 0.6768\n",
      "epoch: 0, loss: 0.7864, train acc: 0.6775\n",
      "epoch: 0, loss: 0.8169, train acc: 0.6648\n",
      "epoch: 0, loss: 0.8234, train acc: 0.6618\n",
      "epoch: 0, loss: 0.8333, train acc: 0.6549\n",
      "epoch: 0, loss: 0.8383, train acc: 0.6553\n",
      "epoch: 0, loss: 0.8366, train acc: 0.6562\n",
      "epoch: 0, loss: 0.8464, train acc: 0.6548\n",
      ">epoch: 0, val_acc: 0.2394, val_f1: 0.1319\n",
      ">> epoch: 0, test_acc: 0.2446, test_f1: 0.1498\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 1\n",
      "epoch: 1, loss: 0.8464, train acc: 0.6250\n",
      "epoch: 1, loss: 0.9104, train acc: 0.6667\n",
      "epoch: 1, loss: 0.9001, train acc: 0.6429\n",
      "epoch: 1, loss: 0.8933, train acc: 0.6316\n",
      "epoch: 1, loss: 0.9147, train acc: 0.6146\n",
      "epoch: 1, loss: 0.8898, train acc: 0.6358\n",
      "epoch: 1, loss: 0.8970, train acc: 0.6415\n",
      "epoch: 1, loss: 0.9087, train acc: 0.6298\n",
      "epoch: 1, loss: 0.9111, train acc: 0.6293\n",
      "epoch: 1, loss: 0.9233, train acc: 0.6237\n",
      "epoch: 1, loss: 0.9075, train acc: 0.6319\n",
      "epoch: 1, loss: 0.9166, train acc: 0.6324\n",
      "epoch: 1, loss: 0.9210, train acc: 0.6250\n",
      "epoch: 1, loss: 0.9130, train acc: 0.6277\n",
      "epoch: 1, loss: 0.9051, train acc: 0.6326\n",
      "epoch: 1, loss: 0.9064, train acc: 0.6337\n",
      "epoch: 1, loss: 0.9064, train acc: 0.6369\n",
      "epoch: 1, loss: 0.8994, train acc: 0.6397\n",
      "epoch: 1, loss: 0.9110, train acc: 0.6323\n",
      "epoch: 1, loss: 0.9033, train acc: 0.6326\n",
      "epoch: 1, loss: 0.8997, train acc: 0.6304\n",
      ">epoch: 1, val_acc: 0.7181, val_f1: 0.2928\n",
      ">> epoch: 1, test_acc: 0.7338, test_f1: 0.2867\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 2\n",
      "epoch: 2, loss: 0.7613, train acc: 0.7500\n",
      "epoch: 2, loss: 0.8396, train acc: 0.7031\n",
      "epoch: 2, loss: 0.8720, train acc: 0.6538\n",
      "epoch: 2, loss: 0.8788, train acc: 0.6389\n",
      "epoch: 2, loss: 0.8548, train acc: 0.6413\n",
      "epoch: 2, loss: 0.8390, train acc: 0.6518\n",
      "epoch: 2, loss: 0.8386, train acc: 0.6648\n",
      "epoch: 2, loss: 0.8269, train acc: 0.6678\n",
      "epoch: 2, loss: 0.8378, train acc: 0.6599\n",
      "epoch: 2, loss: 0.8459, train acc: 0.6549\n",
      "epoch: 2, loss: 0.8405, train acc: 0.6545\n",
      "epoch: 2, loss: 0.8298, train acc: 0.6595\n",
      "epoch: 2, loss: 0.8270, train acc: 0.6647\n",
      "epoch: 2, loss: 0.8308, train acc: 0.6664\n",
      "epoch: 2, loss: 0.8267, train acc: 0.6670\n",
      "epoch: 2, loss: 0.8276, train acc: 0.6667\n",
      "epoch: 2, loss: 0.8440, train acc: 0.6619\n",
      "epoch: 2, loss: 0.8441, train acc: 0.6598\n",
      "epoch: 2, loss: 0.8411, train acc: 0.6586\n",
      "epoch: 2, loss: 0.8528, train acc: 0.6550\n",
      "epoch: 2, loss: 0.8535, train acc: 0.6547\n",
      ">epoch: 2, val_acc: 0.7181, val_f1: 0.2928\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 3\n",
      "epoch: 3, loss: 0.8935, train acc: 0.6562\n",
      "epoch: 3, loss: 1.0000, train acc: 0.5536\n",
      "epoch: 3, loss: 0.9087, train acc: 0.6094\n",
      "epoch: 3, loss: 0.8532, train acc: 0.6360\n",
      "epoch: 3, loss: 0.8767, train acc: 0.6136\n",
      "epoch: 3, loss: 0.8741, train acc: 0.6019\n",
      "epoch: 3, loss: 0.8604, train acc: 0.5957\n",
      "epoch: 3, loss: 0.8468, train acc: 0.6081\n",
      "epoch: 3, loss: 0.8579, train acc: 0.6176\n",
      "epoch: 3, loss: 0.8480, train acc: 0.6290\n",
      "epoch: 3, loss: 0.8232, train acc: 0.6430\n",
      "epoch: 3, loss: 0.8245, train acc: 0.6480\n",
      "epoch: 3, loss: 0.8117, train acc: 0.6552\n",
      "epoch: 3, loss: 0.8124, train acc: 0.6539\n",
      "epoch: 3, loss: 0.8215, train acc: 0.6528\n",
      "epoch: 3, loss: 0.8206, train acc: 0.6518\n",
      "epoch: 3, loss: 0.8311, train acc: 0.6433\n",
      "epoch: 3, loss: 0.8223, train acc: 0.6473\n",
      "epoch: 3, loss: 0.8257, train acc: 0.6454\n",
      "epoch: 3, loss: 0.8233, train acc: 0.6469\n",
      "epoch: 3, loss: 0.8180, train acc: 0.6452\n",
      ">epoch: 3, val_acc: 0.7181, val_f1: 0.2928\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 4\n",
      "epoch: 4, loss: 0.6101, train acc: 0.8125\n",
      "epoch: 4, loss: 0.7214, train acc: 0.7500\n",
      "epoch: 4, loss: 0.6357, train acc: 0.7841\n",
      "epoch: 4, loss: 0.7208, train acc: 0.7539\n",
      "epoch: 4, loss: 0.7568, train acc: 0.7232\n",
      "epoch: 4, loss: 0.7841, train acc: 0.7284\n",
      "epoch: 4, loss: 0.7864, train acc: 0.7117\n",
      "epoch: 4, loss: 0.7785, train acc: 0.7118\n",
      "epoch: 4, loss: 0.7703, train acc: 0.7088\n",
      "epoch: 4, loss: 0.7778, train acc: 0.6997\n",
      "epoch: 4, loss: 0.7807, train acc: 0.6949\n",
      "epoch: 4, loss: 0.7805, train acc: 0.6931\n",
      "epoch: 4, loss: 0.7817, train acc: 0.6906\n",
      "epoch: 4, loss: 0.7891, train acc: 0.6809\n",
      "epoch: 4, loss: 0.8038, train acc: 0.6717\n",
      "epoch: 4, loss: 0.8095, train acc: 0.6686\n",
      "epoch: 4, loss: 0.8098, train acc: 0.6674\n",
      "epoch: 4, loss: 0.8038, train acc: 0.6672\n",
      "epoch: 4, loss: 0.8018, train acc: 0.6669\n",
      "epoch: 4, loss: 0.8044, train acc: 0.6680\n",
      "epoch: 4, loss: 0.7978, train acc: 0.6714\n",
      "epoch: 4, loss: 0.8012, train acc: 0.6726\n",
      ">epoch: 4, val_acc: 0.7181, val_f1: 0.2928\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 5\n",
      "epoch: 5, loss: 0.7966, train acc: 0.6875\n",
      "epoch: 5, loss: 0.8791, train acc: 0.6500\n",
      "epoch: 5, loss: 0.8631, train acc: 0.6458\n",
      "epoch: 5, loss: 0.8524, train acc: 0.6438\n",
      "epoch: 5, loss: 0.8621, train acc: 0.6350\n",
      "epoch: 5, loss: 0.8470, train acc: 0.6292\n",
      "epoch: 5, loss: 0.8465, train acc: 0.6179\n",
      "epoch: 5, loss: 0.8307, train acc: 0.6125\n",
      "epoch: 5, loss: 0.8328, train acc: 0.6194\n",
      "epoch: 5, loss: 0.8124, train acc: 0.6338\n",
      "epoch: 5, loss: 0.8065, train acc: 0.6432\n",
      "epoch: 5, loss: 0.8044, train acc: 0.6521\n",
      "epoch: 5, loss: 0.8019, train acc: 0.6548\n",
      "epoch: 5, loss: 0.8031, train acc: 0.6536\n",
      "epoch: 5, loss: 0.8049, train acc: 0.6567\n",
      "epoch: 5, loss: 0.8046, train acc: 0.6562\n",
      "epoch: 5, loss: 0.7966, train acc: 0.6618\n",
      "epoch: 5, loss: 0.7932, train acc: 0.6632\n",
      "epoch: 5, loss: 0.7874, train acc: 0.6684\n",
      "epoch: 5, loss: 0.7934, train acc: 0.6656\n",
      "epoch: 5, loss: 0.7897, train acc: 0.6643\n",
      ">epoch: 5, val_acc: 0.7181, val_f1: 0.2928\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 6\n",
      "epoch: 6, loss: 0.5032, train acc: 0.8281\n",
      "epoch: 6, loss: 0.5923, train acc: 0.7917\n",
      "epoch: 6, loss: 0.7008, train acc: 0.7500\n",
      "epoch: 6, loss: 0.7127, train acc: 0.7270\n",
      "epoch: 6, loss: 0.7286, train acc: 0.7109\n",
      "epoch: 6, loss: 0.7464, train acc: 0.7026\n",
      "epoch: 6, loss: 0.7494, train acc: 0.6930\n",
      "epoch: 6, loss: 0.7482, train acc: 0.6971\n",
      "epoch: 6, loss: 0.7475, train acc: 0.6946\n",
      "epoch: 6, loss: 0.7519, train acc: 0.6875\n",
      "epoch: 6, loss: 0.7680, train acc: 0.6748\n",
      "epoch: 6, loss: 0.7777, train acc: 0.6642\n",
      "epoch: 6, loss: 0.7836, train acc: 0.6602\n",
      "epoch: 6, loss: 0.7759, train acc: 0.6658\n",
      "epoch: 6, loss: 0.7769, train acc: 0.6647\n",
      "epoch: 6, loss: 0.7778, train acc: 0.6669\n",
      "epoch: 6, loss: 0.7752, train acc: 0.6674\n",
      "epoch: 6, loss: 0.7779, train acc: 0.6671\n",
      "epoch: 6, loss: 0.7875, train acc: 0.6609\n",
      "epoch: 6, loss: 0.7867, train acc: 0.6585\n",
      "epoch: 6, loss: 0.7845, train acc: 0.6599\n",
      ">epoch: 6, val_acc: 0.7181, val_f1: 0.2928\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 7\n",
      "epoch: 7, loss: 0.7643, train acc: 0.7500\n",
      "epoch: 7, loss: 0.7661, train acc: 0.7188\n",
      "epoch: 7, loss: 0.7800, train acc: 0.7260\n",
      "epoch: 7, loss: 0.7686, train acc: 0.7118\n",
      "epoch: 7, loss: 0.8025, train acc: 0.7038\n",
      "epoch: 7, loss: 0.8228, train acc: 0.6830\n",
      "epoch: 7, loss: 0.8301, train acc: 0.6648\n",
      "epoch: 7, loss: 0.8344, train acc: 0.6546\n",
      "epoch: 7, loss: 0.8166, train acc: 0.6570\n",
      "epoch: 7, loss: 0.7988, train acc: 0.6628\n",
      "epoch: 7, loss: 0.7895, train acc: 0.6710\n",
      "epoch: 7, loss: 0.7918, train acc: 0.6681\n",
      "epoch: 7, loss: 0.7905, train acc: 0.6637\n",
      "epoch: 7, loss: 0.7891, train acc: 0.6618\n",
      "epoch: 7, loss: 0.7961, train acc: 0.6524\n",
      "epoch: 7, loss: 0.7976, train acc: 0.6514\n",
      "epoch: 7, loss: 0.7889, train acc: 0.6559\n",
      "epoch: 7, loss: 0.7939, train acc: 0.6562\n",
      "epoch: 7, loss: 0.7845, train acc: 0.6606\n",
      "epoch: 7, loss: 0.7749, train acc: 0.6677\n",
      "epoch: 7, loss: 0.7791, train acc: 0.6663\n",
      ">epoch: 7, val_acc: 0.7181, val_f1: 0.2928\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 8\n",
      "epoch: 8, loss: 1.0214, train acc: 0.5625\n",
      "epoch: 8, loss: 0.7857, train acc: 0.6786\n",
      "epoch: 8, loss: 0.7616, train acc: 0.6927\n",
      "epoch: 8, loss: 0.7948, train acc: 0.6581\n",
      "epoch: 8, loss: 0.7878, train acc: 0.6534\n",
      "epoch: 8, loss: 0.8021, train acc: 0.6644\n",
      "epoch: 8, loss: 0.7885, train acc: 0.6699\n",
      "epoch: 8, loss: 0.7768, train acc: 0.6757\n",
      "epoch: 8, loss: 0.7770, train acc: 0.6815\n",
      "epoch: 8, loss: 0.7742, train acc: 0.6875\n",
      "epoch: 8, loss: 0.7728, train acc: 0.6899\n",
      "epoch: 8, loss: 0.7706, train acc: 0.6919\n",
      "epoch: 8, loss: 0.7708, train acc: 0.6915\n",
      "epoch: 8, loss: 0.7753, train acc: 0.6838\n",
      "epoch: 8, loss: 0.7685, train acc: 0.6884\n",
      "epoch: 8, loss: 0.7670, train acc: 0.6899\n",
      "epoch: 8, loss: 0.7667, train acc: 0.6883\n",
      "epoch: 8, loss: 0.7714, train acc: 0.6846\n",
      "epoch: 8, loss: 0.7665, train acc: 0.6841\n",
      "epoch: 8, loss: 0.7656, train acc: 0.6849\n",
      "epoch: 8, loss: 0.7675, train acc: 0.6832\n",
      ">epoch: 8, val_acc: 0.7181, val_f1: 0.2928\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 9\n",
      "epoch: 9, loss: 0.7944, train acc: 0.5625\n",
      "epoch: 9, loss: 0.7440, train acc: 0.6250\n",
      "epoch: 9, loss: 0.7440, train acc: 0.6307\n",
      "epoch: 9, loss: 0.7417, train acc: 0.6445\n",
      "epoch: 9, loss: 0.7897, train acc: 0.6429\n",
      "epoch: 9, loss: 0.7913, train acc: 0.6466\n",
      "epoch: 9, loss: 0.7829, train acc: 0.6593\n",
      "epoch: 9, loss: 0.7742, train acc: 0.6649\n",
      "epoch: 9, loss: 0.7598, train acc: 0.6738\n",
      "epoch: 9, loss: 0.7574, train acc: 0.6793\n",
      "epoch: 9, loss: 0.7594, train acc: 0.6850\n",
      "epoch: 9, loss: 0.7690, train acc: 0.6819\n",
      "epoch: 9, loss: 0.7732, train acc: 0.6783\n",
      "epoch: 9, loss: 0.7678, train acc: 0.6780\n",
      "epoch: 9, loss: 0.7671, train acc: 0.6752\n",
      "epoch: 9, loss: 0.7654, train acc: 0.6768\n",
      "epoch: 9, loss: 0.7670, train acc: 0.6759\n",
      "epoch: 9, loss: 0.7684, train acc: 0.6751\n",
      "epoch: 9, loss: 0.7750, train acc: 0.6758\n",
      "epoch: 9, loss: 0.7737, train acc: 0.6784\n",
      "epoch: 9, loss: 0.7761, train acc: 0.6801\n",
      "epoch: 9, loss: 0.7753, train acc: 0.6803\n",
      ">epoch: 9, val_acc: 0.7181, val_f1: 0.2928\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 10\n",
      "epoch: 10, loss: 0.6680, train acc: 0.7250\n",
      "epoch: 10, loss: 0.6826, train acc: 0.7500\n",
      "epoch: 10, loss: 0.7074, train acc: 0.7292\n",
      "epoch: 10, loss: 0.6775, train acc: 0.7344\n",
      "epoch: 10, loss: 0.7158, train acc: 0.7125\n",
      "epoch: 10, loss: 0.7618, train acc: 0.6937\n",
      "epoch: 10, loss: 0.7722, train acc: 0.6786\n",
      "epoch: 10, loss: 0.7656, train acc: 0.6797\n",
      "epoch: 10, loss: 0.7504, train acc: 0.6903\n",
      "epoch: 10, loss: 0.7461, train acc: 0.6937\n",
      "epoch: 10, loss: 0.7605, train acc: 0.6909\n",
      "epoch: 10, loss: 0.7494, train acc: 0.6948\n",
      "epoch: 10, loss: 0.7572, train acc: 0.6942\n",
      "epoch: 10, loss: 0.7585, train acc: 0.6902\n",
      "epoch: 10, loss: 0.7577, train acc: 0.6875\n",
      "epoch: 10, loss: 0.7525, train acc: 0.6875\n",
      "epoch: 10, loss: 0.7526, train acc: 0.6853\n",
      "epoch: 10, loss: 0.7595, train acc: 0.6813\n",
      "epoch: 10, loss: 0.7611, train acc: 0.6816\n",
      "epoch: 10, loss: 0.7603, train acc: 0.6825\n",
      "epoch: 10, loss: 0.7639, train acc: 0.6815\n",
      ">epoch: 10, val_acc: 0.7181, val_f1: 0.2928\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 11\n",
      "epoch: 11, loss: 0.8081, train acc: 0.5938\n",
      "epoch: 11, loss: 0.7513, train acc: 0.6319\n",
      "epoch: 11, loss: 0.7904, train acc: 0.6429\n",
      "epoch: 11, loss: 0.7771, train acc: 0.6678\n",
      "epoch: 11, loss: 0.7550, train acc: 0.6771\n",
      "epoch: 11, loss: 0.7356, train acc: 0.6875\n",
      "epoch: 11, loss: 0.7345, train acc: 0.6912\n",
      "epoch: 11, loss: 0.7490, train acc: 0.6811\n",
      "epoch: 11, loss: 0.7627, train acc: 0.6733\n",
      "epoch: 11, loss: 0.7597, train acc: 0.6722\n",
      "epoch: 11, loss: 0.7648, train acc: 0.6713\n",
      "epoch: 11, loss: 0.7520, train acc: 0.6780\n",
      "epoch: 11, loss: 0.7590, train acc: 0.6768\n",
      "epoch: 11, loss: 0.7503, train acc: 0.6812\n",
      "epoch: 11, loss: 0.7499, train acc: 0.6841\n",
      "epoch: 11, loss: 0.7635, train acc: 0.6772\n",
      "epoch: 11, loss: 0.7695, train acc: 0.6756\n",
      "epoch: 11, loss: 0.7790, train acc: 0.6685\n",
      "epoch: 11, loss: 0.7749, train acc: 0.6709\n",
      "epoch: 11, loss: 0.7765, train acc: 0.6723\n",
      "epoch: 11, loss: 0.7704, train acc: 0.6743\n",
      ">epoch: 11, val_acc: 0.7181, val_f1: 0.2928\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 12\n",
      "epoch: 12, loss: 0.7937, train acc: 0.6458\n",
      "epoch: 12, loss: 0.7765, train acc: 0.6562\n",
      "epoch: 12, loss: 0.7508, train acc: 0.6875\n",
      "epoch: 12, loss: 0.7226, train acc: 0.7083\n",
      "epoch: 12, loss: 0.7339, train acc: 0.7120\n",
      "epoch: 12, loss: 0.7519, train acc: 0.6964\n",
      "epoch: 12, loss: 0.7585, train acc: 0.6818\n",
      "epoch: 12, loss: 0.7530, train acc: 0.6826\n",
      "epoch: 12, loss: 0.7608, train acc: 0.6701\n",
      "epoch: 12, loss: 0.7497, train acc: 0.6693\n",
      "epoch: 12, loss: 0.7464, train acc: 0.6733\n",
      "epoch: 12, loss: 0.7499, train acc: 0.6724\n",
      "epoch: 12, loss: 0.7518, train acc: 0.6696\n",
      "epoch: 12, loss: 0.7571, train acc: 0.6691\n",
      "epoch: 12, loss: 0.7503, train acc: 0.6747\n",
      "epoch: 12, loss: 0.7537, train acc: 0.6747\n",
      "epoch: 12, loss: 0.7496, train acc: 0.6785\n",
      "epoch: 12, loss: 0.7541, train acc: 0.6761\n",
      "epoch: 12, loss: 0.7563, train acc: 0.6774\n",
      "epoch: 12, loss: 0.7648, train acc: 0.6779\n",
      "epoch: 12, loss: 0.7611, train acc: 0.6790\n",
      ">epoch: 12, val_acc: 0.7181, val_f1: 0.2928\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 13\n",
      "epoch: 13, loss: 0.7081, train acc: 0.7500\n",
      "epoch: 13, loss: 0.6905, train acc: 0.7232\n",
      "epoch: 13, loss: 0.7050, train acc: 0.7292\n",
      "epoch: 13, loss: 0.7423, train acc: 0.7022\n",
      "epoch: 13, loss: 0.7553, train acc: 0.6989\n",
      "epoch: 13, loss: 0.7472, train acc: 0.6991\n",
      "epoch: 13, loss: 0.7671, train acc: 0.6875\n",
      "epoch: 13, loss: 0.7577, train acc: 0.6875\n",
      "epoch: 13, loss: 0.7490, train acc: 0.6875\n",
      "epoch: 13, loss: 0.7506, train acc: 0.6848\n",
      "epoch: 13, loss: 0.7554, train acc: 0.6767\n",
      "epoch: 13, loss: 0.7546, train acc: 0.6809\n",
      "epoch: 13, loss: 0.7525, train acc: 0.6804\n",
      "epoch: 13, loss: 0.7612, train acc: 0.6754\n",
      "epoch: 13, loss: 0.7682, train acc: 0.6753\n",
      "epoch: 13, loss: 0.7619, train acc: 0.6794\n",
      "epoch: 13, loss: 0.7598, train acc: 0.6791\n",
      "epoch: 13, loss: 0.7593, train acc: 0.6810\n",
      "epoch: 13, loss: 0.7655, train acc: 0.6793\n",
      "epoch: 13, loss: 0.7683, train acc: 0.6785\n",
      "epoch: 13, loss: 0.7660, train acc: 0.6820\n",
      ">epoch: 13, val_acc: 0.7181, val_f1: 0.2928\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 14\n",
      "epoch: 14, loss: 0.7910, train acc: 0.6875\n",
      "epoch: 14, loss: 0.7941, train acc: 0.6771\n",
      "epoch: 14, loss: 0.7935, train acc: 0.6705\n",
      "epoch: 14, loss: 0.7728, train acc: 0.6875\n",
      "epoch: 14, loss: 0.7441, train acc: 0.6964\n",
      "epoch: 14, loss: 0.7410, train acc: 0.6899\n",
      "epoch: 14, loss: 0.7211, train acc: 0.6976\n",
      "epoch: 14, loss: 0.7177, train acc: 0.6997\n",
      "epoch: 14, loss: 0.7432, train acc: 0.6814\n",
      "epoch: 14, loss: 0.7336, train acc: 0.6902\n",
      "epoch: 14, loss: 0.7575, train acc: 0.6765\n",
      "epoch: 14, loss: 0.7594, train acc: 0.6786\n",
      "epoch: 14, loss: 0.7605, train acc: 0.6773\n",
      "epoch: 14, loss: 0.7493, train acc: 0.6847\n",
      "epoch: 14, loss: 0.7570, train acc: 0.6849\n",
      "epoch: 14, loss: 0.7566, train acc: 0.6883\n",
      "epoch: 14, loss: 0.7671, train acc: 0.6852\n",
      "epoch: 14, loss: 0.7654, train acc: 0.6846\n",
      "epoch: 14, loss: 0.7651, train acc: 0.6820\n",
      "epoch: 14, loss: 0.7607, train acc: 0.6842\n",
      "epoch: 14, loss: 0.7527, train acc: 0.6869\n",
      "epoch: 14, loss: 0.7491, train acc: 0.6909\n",
      ">epoch: 14, val_acc: 0.7181, val_f1: 0.2928\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 15\n",
      "epoch: 15, loss: 0.7613, train acc: 0.6875\n",
      "epoch: 15, loss: 0.7722, train acc: 0.6937\n",
      "epoch: 15, loss: 0.7756, train acc: 0.6875\n",
      "epoch: 15, loss: 0.7637, train acc: 0.6844\n",
      "epoch: 15, loss: 0.7792, train acc: 0.6700\n",
      "epoch: 15, loss: 0.7743, train acc: 0.6750\n",
      "epoch: 15, loss: 0.7708, train acc: 0.6804\n",
      "epoch: 15, loss: 0.7677, train acc: 0.6875\n",
      "epoch: 15, loss: 0.7710, train acc: 0.6917\n",
      "epoch: 15, loss: 0.7646, train acc: 0.6887\n",
      "epoch: 15, loss: 0.7669, train acc: 0.6886\n",
      "epoch: 15, loss: 0.7714, train acc: 0.6844\n",
      "epoch: 15, loss: 0.7670, train acc: 0.6817\n",
      "epoch: 15, loss: 0.7637, train acc: 0.6804\n",
      "epoch: 15, loss: 0.7627, train acc: 0.6808\n",
      "epoch: 15, loss: 0.7610, train acc: 0.6852\n",
      "epoch: 15, loss: 0.7656, train acc: 0.6816\n",
      "epoch: 15, loss: 0.7600, train acc: 0.6833\n",
      "epoch: 15, loss: 0.7556, train acc: 0.6829\n",
      "epoch: 15, loss: 0.7520, train acc: 0.6831\n",
      "epoch: 15, loss: 0.7484, train acc: 0.6875\n",
      ">epoch: 15, val_acc: 0.7181, val_f1: 0.2928\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 16\n",
      "epoch: 16, loss: 0.6042, train acc: 0.7500\n",
      "epoch: 16, loss: 0.6600, train acc: 0.7292\n",
      "epoch: 16, loss: 0.7191, train acc: 0.6920\n",
      "epoch: 16, loss: 0.7198, train acc: 0.6941\n",
      "epoch: 16, loss: 0.7207, train acc: 0.6953\n",
      "epoch: 16, loss: 0.7124, train acc: 0.7004\n",
      "epoch: 16, loss: 0.7046, train acc: 0.7114\n",
      "epoch: 16, loss: 0.7122, train acc: 0.7067\n",
      "epoch: 16, loss: 0.7106, train acc: 0.7045\n",
      "epoch: 16, loss: 0.7045, train acc: 0.7092\n",
      "epoch: 16, loss: 0.7091, train acc: 0.7083\n",
      "epoch: 16, loss: 0.7190, train acc: 0.7066\n",
      "epoch: 16, loss: 0.7286, train acc: 0.7031\n",
      "epoch: 16, loss: 0.7407, train acc: 0.6947\n",
      "epoch: 16, loss: 0.7338, train acc: 0.6993\n",
      "epoch: 16, loss: 0.7328, train acc: 0.6954\n",
      "epoch: 16, loss: 0.7273, train acc: 0.7001\n",
      "epoch: 16, loss: 0.7348, train acc: 0.6980\n",
      "epoch: 16, loss: 0.7370, train acc: 0.6968\n",
      "epoch: 16, loss: 0.7399, train acc: 0.6951\n",
      "epoch: 16, loss: 0.7449, train acc: 0.6905\n",
      ">epoch: 16, val_acc: 0.7128, val_f1: 0.3039\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 17\n",
      "epoch: 17, loss: 0.8035, train acc: 0.7083\n",
      "epoch: 17, loss: 0.7480, train acc: 0.6719\n",
      "epoch: 17, loss: 0.7158, train acc: 0.6731\n",
      "epoch: 17, loss: 0.6931, train acc: 0.6875\n",
      "epoch: 17, loss: 0.7221, train acc: 0.6875\n",
      "epoch: 17, loss: 0.7266, train acc: 0.6853\n",
      "epoch: 17, loss: 0.7370, train acc: 0.6761\n",
      "epoch: 17, loss: 0.7327, train acc: 0.6776\n",
      "epoch: 17, loss: 0.7431, train acc: 0.6715\n",
      "epoch: 17, loss: 0.7364, train acc: 0.6745\n",
      "epoch: 17, loss: 0.7329, train acc: 0.6840\n",
      "epoch: 17, loss: 0.7310, train acc: 0.6810\n",
      "epoch: 17, loss: 0.7298, train acc: 0.6865\n",
      "epoch: 17, loss: 0.7290, train acc: 0.6893\n",
      "epoch: 17, loss: 0.7351, train acc: 0.6875\n",
      "epoch: 17, loss: 0.7353, train acc: 0.6867\n",
      "epoch: 17, loss: 0.7345, train acc: 0.6890\n",
      "epoch: 17, loss: 0.7340, train acc: 0.6903\n",
      "epoch: 17, loss: 0.7327, train acc: 0.6909\n",
      "epoch: 17, loss: 0.7355, train acc: 0.6926\n",
      "epoch: 17, loss: 0.7409, train acc: 0.6905\n",
      ">epoch: 17, val_acc: 0.7181, val_f1: 0.2928\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 18\n",
      "epoch: 18, loss: 0.7550, train acc: 0.6875\n",
      "epoch: 18, loss: 0.8376, train acc: 0.6518\n",
      "epoch: 18, loss: 0.7469, train acc: 0.6927\n",
      "epoch: 18, loss: 0.7929, train acc: 0.6838\n",
      "epoch: 18, loss: 0.7782, train acc: 0.6875\n",
      "epoch: 18, loss: 0.7698, train acc: 0.6852\n",
      "epoch: 18, loss: 0.7585, train acc: 0.6816\n",
      "epoch: 18, loss: 0.7634, train acc: 0.6807\n",
      "epoch: 18, loss: 0.7447, train acc: 0.6920\n",
      "epoch: 18, loss: 0.7581, train acc: 0.6795\n",
      "epoch: 18, loss: 0.7548, train acc: 0.6815\n",
      "epoch: 18, loss: 0.7445, train acc: 0.6842\n",
      "epoch: 18, loss: 0.7406, train acc: 0.6835\n",
      "epoch: 18, loss: 0.7357, train acc: 0.6875\n",
      "epoch: 18, loss: 0.7411, train acc: 0.6875\n",
      "epoch: 18, loss: 0.7333, train acc: 0.6916\n",
      "epoch: 18, loss: 0.7370, train acc: 0.6928\n",
      "epoch: 18, loss: 0.7345, train acc: 0.6954\n",
      "epoch: 18, loss: 0.7339, train acc: 0.6963\n",
      "epoch: 18, loss: 0.7341, train acc: 0.6978\n",
      "epoch: 18, loss: 0.7414, train acc: 0.6949\n",
      ">epoch: 18, val_acc: 0.7128, val_f1: 0.3039\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 19\n",
      "epoch: 19, loss: 0.6548, train acc: 0.7500\n",
      "epoch: 19, loss: 0.6943, train acc: 0.7188\n",
      "epoch: 19, loss: 0.6650, train acc: 0.7216\n",
      "epoch: 19, loss: 0.7208, train acc: 0.6914\n",
      "epoch: 19, loss: 0.7445, train acc: 0.6756\n",
      "epoch: 19, loss: 0.7262, train acc: 0.6803\n",
      "epoch: 19, loss: 0.7122, train acc: 0.6875\n",
      "epoch: 19, loss: 0.7103, train acc: 0.6823\n",
      "epoch: 19, loss: 0.7124, train acc: 0.6890\n",
      "epoch: 19, loss: 0.7100, train acc: 0.6902\n",
      "epoch: 19, loss: 0.7258, train acc: 0.6875\n",
      "epoch: 19, loss: 0.7191, train acc: 0.6953\n",
      "epoch: 19, loss: 0.7171, train acc: 0.6957\n",
      "epoch: 19, loss: 0.7222, train acc: 0.6913\n",
      "epoch: 19, loss: 0.7201, train acc: 0.6919\n",
      "epoch: 19, loss: 0.7225, train acc: 0.6883\n",
      "epoch: 19, loss: 0.7268, train acc: 0.6898\n",
      "epoch: 19, loss: 0.7286, train acc: 0.6875\n",
      "epoch: 19, loss: 0.7311, train acc: 0.6861\n",
      "epoch: 19, loss: 0.7324, train acc: 0.6882\n",
      "epoch: 19, loss: 0.7345, train acc: 0.6881\n",
      "epoch: 19, loss: 0.7326, train acc: 0.6915\n",
      ">epoch: 19, val_acc: 0.7181, val_f1: 0.2928\n",
      "Namespace(model_type='tri_gcn', modules={'tgcn': True, 'semgcn': True, 'lexgcn': True}, num_layers={'tgcn': 2, 'semgcn': 2, 'lexgcn': 2}, year='2016', train_file='data/train2016restaurant.txt', test_file='data/test2016restaurant.txt', model_path='test_models/2016tri_gcn+concat_seed454_reg0.0005_drop0.4_cdrop0.4_lr2.5e-05_epochs20_adam', val_file='val.txt', log='log', bert_model='bert_large_uncased', cooc_path='cooc_matrix.csv', cooc=       100       1000   10000  10003     10005     10007     10009     1001   \\\n",
      "100      0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "1000     0.0  0.000000    0.0    0.0  0.000606  0.000303  0.000606  0.035120   \n",
      "10000    0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.333333   \n",
      "10003    0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "10005    0.0  0.333333    0.0    0.0  0.000000  0.166667  0.000000  0.166667   \n",
      "...      ...       ...    ...    ...       ...       ...       ...       ...   \n",
      "9992     0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "9993     0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "9996     0.0  0.500000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "9997     0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "9998     0.0  0.400000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "       10010  10015  ...     9986      9987      999    9990      9991   \\\n",
      "100      0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "1000     0.0    0.0  ...  0.002422  0.000303  0.062368    0.0  0.000606   \n",
      "10000    0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "10003    0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "10005    0.0    0.0  ...  0.000000  0.000000  0.333333    0.0  0.000000   \n",
      "...      ...    ...  ...       ...       ...       ...    ...       ...   \n",
      "9992     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "9993     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "9996     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "9997     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "9998     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "\n",
      "       9992   9993      9996   9997      9998   \n",
      "100      0.0    0.0  0.000000    0.0  0.000000  \n",
      "1000     0.0    0.0  0.000606    0.0  0.001211  \n",
      "10000    0.0    0.0  0.000000    0.0  0.000000  \n",
      "10003    0.0    0.0  0.000000    0.0  0.000000  \n",
      "10005    0.0    0.0  0.000000    0.0  0.000000  \n",
      "...      ...    ...       ...    ...       ...  \n",
      "9992     0.0    0.0  0.000000    0.0  0.000000  \n",
      "9993     0.0    0.0  0.000000    0.0  0.000000  \n",
      "9996     0.0    0.0  0.000000    0.0  0.000000  \n",
      "9997     0.0    0.0  0.000000    0.0  0.000000  \n",
      "9998     0.0    0.0  0.000000    0.0  0.000000  \n",
      "\n",
      "[13075 rows x 13075 columns], learning_rate=2.5e-05, dropout=0.4, concat_dropout=0.4, bert_dropout=0.2, l2reg=0.0005, num_epoch=20, batch_size=16, log_step=5, max_seq_len=100, polarities_dim=3, device=device(type='cuda'), seed=454, valset_ratio=0.1, do_train=True, do_eval=True, eval_epoch_num=0, fusion_type='concat', use_ensemble=True, save_models='none', print_sent=False, optim='adam', n_gpu=1)\n",
      "{'none': 0, 'acl': 1, 'acl:relcl': 2, 'advcl': 3, 'advcl:relcl': 4, 'advmod': 5, 'amod': 6, 'appos': 7, 'aux': 8, 'aux:pass': 9, 'case': 10, 'cc': 11, 'cc:preconj': 12, 'ccomp': 13, 'compound': 14, 'compound:prt': 15, 'conj': 16, 'cop': 17, 'csubj': 18, 'dep': 19, 'det': 20, 'det:predet': 21, 'discourse': 22, 'dislocated': 23, 'expl': 24, 'fixed': 25, 'flat': 26, 'iobj': 27, 'list': 28, 'mark': 29, 'nmod': 30, 'nmod:npmod': 31, 'nmod:poss': 32, 'nmod:tmod': 33, 'nsubj': 34, 'nsubj:outer': 35, 'nsubj:pass': 36, 'nummod': 37, 'obj': 38, 'obl': 39, 'obl:agent': 40, 'obl:npmod': 41, 'obl:tmod': 42, 'parataxis': 43, 'punct': 44, 'reparandum': 45, 'root': 46, 'vocative': 47, 'xcomp': 48}\n",
      "{'-1': 0, '0': 1, '1': 2}\n",
      "Model name 'bert_large_uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming 'bert_large_uncased' is a path or url to a directory containing tokenizer files.\n",
      "Didn't find file bert_large_uncased/added_tokens.json. We won't load it.\n",
      "Didn't find file bert_large_uncased/special_tokens_map.json. We won't load it.\n",
      "Didn't find file bert_large_uncased/tokenizer_config.json. We won't load it.\n",
      "loading file bert_large_uncased/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "{\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 3,\n",
      "  \"num_types\": 49,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file bert_large_uncased/pytorch_model.bin\n",
      "Weights of AsaTgcnSem not initialized from pretrained model: ['gate_weight', 'gate_bias', 'ensemble', 'TGCNLayers.0.weight', 'TGCNLayers.0.bias', 'TGCNLayers.0.dense.weight', 'TGCNLayers.1.weight', 'TGCNLayers.1.bias', 'TGCNLayers.1.dense.weight', 'SemGCNLayers.0.weight', 'SemGCNLayers.0.bias', 'SemGCNLayers.1.weight', 'SemGCNLayers.1.bias', 'LexGCNLayers.0.weight', 'LexGCNLayers.0.bias', 'LexGCNLayers.1.weight', 'LexGCNLayers.1.bias', 'fc_single.weight', 'fc_single.bias', 'ensemble_linear_tgcn.weight', 'ensemble_linear_tgcn.bias', 'ensemble_linear_semgcn.weight', 'ensemble_linear_semgcn.bias', 'ensemble_linear_lexgcn.weight', 'ensemble_linear_lexgcn.bias', 'dep_embedding.weight']\n",
      "Weights from pretrained model not used in AsaTgcnSem: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "[{'input_ids': tensor([  101, 13325,  2013,  3025,  8466,  2023,  2109,  2000,  2022,  1037,\n",
      "         2204,  2173,  1010,  2021,  2025,  2151,  2936,  1012,   102,  2173,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'valid_ids': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'segment_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'mem_valid_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'dep_adj_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'dep_value_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'polarity': 0, 'raw_text': 'judging from previous posts this used to be a good place , but not any longer .', 'aspect': 'place'}]\n",
      "[{'input_ids': tensor([  101, 13325,  2013,  3025,  8466,  2023,  2109,  2000,  2022,  1037,\n",
      "         2204,  2173,  1010,  2021,  2025,  2151,  2936,  1012,   102,  2173,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'valid_ids': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'segment_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'mem_valid_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'dep_adj_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'dep_value_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'polarity': 0, 'raw_text': 'judging from previous posts this used to be a good place , but not any longer .', 'aspect': 'place'}]\n",
      "[{'input_ids': tensor([  101,  4240,  2428,  2204, 10514,  6182,  1012,   102, 10514,  6182,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'valid_ids': tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'segment_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'mem_valid_ids': tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'dep_adj_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'dep_value_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 5,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'polarity': 2, 'raw_text': 'serves really good sushi .', 'aspect': 'sushi'}]\n",
      "cuda memory allocated: 1400893440\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 0\n",
      "epoch: 0, loss: 1.0297, train acc: 0.5000\n",
      "epoch: 0, loss: 0.8775, train acc: 0.6125\n",
      "epoch: 0, loss: 0.8644, train acc: 0.6375\n",
      "epoch: 0, loss: 0.8340, train acc: 0.6500\n",
      "epoch: 0, loss: 0.8201, train acc: 0.6450\n",
      "epoch: 0, loss: 0.8487, train acc: 0.6271\n",
      "epoch: 0, loss: 0.8272, train acc: 0.6286\n",
      "epoch: 0, loss: 0.8226, train acc: 0.6250\n",
      "epoch: 0, loss: 0.8186, train acc: 0.6319\n",
      "epoch: 0, loss: 0.7962, train acc: 0.6438\n",
      "epoch: 0, loss: 0.7871, train acc: 0.6500\n",
      "epoch: 0, loss: 0.7787, train acc: 0.6542\n",
      "epoch: 0, loss: 0.7871, train acc: 0.6481\n",
      "epoch: 0, loss: 0.7791, train acc: 0.6545\n",
      "epoch: 0, loss: 0.7799, train acc: 0.6517\n",
      "epoch: 0, loss: 0.7790, train acc: 0.6562\n",
      "epoch: 0, loss: 0.7786, train acc: 0.6529\n",
      "epoch: 0, loss: 0.7810, train acc: 0.6521\n",
      "epoch: 0, loss: 0.7776, train acc: 0.6553\n",
      "epoch: 0, loss: 0.7755, train acc: 0.6575\n",
      "epoch: 0, loss: 0.7763, train acc: 0.6560\n",
      ">epoch: 0, val_acc: 0.8245, val_f1: 0.5188\n",
      ">> epoch: 0, test_acc: 0.8246, test_f1: 0.5146\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 1\n",
      "epoch: 1, loss: 0.6978, train acc: 0.6875\n",
      "epoch: 1, loss: 0.6222, train acc: 0.7500\n",
      "epoch: 1, loss: 0.6408, train acc: 0.7232\n",
      "epoch: 1, loss: 0.6467, train acc: 0.7401\n",
      "epoch: 1, loss: 0.6288, train acc: 0.7448\n",
      "epoch: 1, loss: 0.6099, train acc: 0.7522\n",
      "epoch: 1, loss: 0.5763, train acc: 0.7665\n",
      "epoch: 1, loss: 0.5444, train acc: 0.7788\n",
      "epoch: 1, loss: 0.5574, train acc: 0.7770\n",
      "epoch: 1, loss: 0.5583, train acc: 0.7781\n",
      "epoch: 1, loss: 0.5544, train acc: 0.7801\n",
      "epoch: 1, loss: 0.5359, train acc: 0.7860\n",
      "epoch: 1, loss: 0.5306, train acc: 0.7891\n",
      "epoch: 1, loss: 0.5140, train acc: 0.7953\n",
      "epoch: 1, loss: 0.5173, train acc: 0.7956\n",
      "epoch: 1, loss: 0.5010, train acc: 0.8030\n",
      "epoch: 1, loss: 0.4941, train acc: 0.8043\n",
      "epoch: 1, loss: 0.4906, train acc: 0.8076\n",
      "epoch: 1, loss: 0.4992, train acc: 0.8025\n",
      "epoch: 1, loss: 0.4956, train acc: 0.8049\n",
      "epoch: 1, loss: 0.4912, train acc: 0.8065\n",
      ">epoch: 1, val_acc: 0.8670, val_f1: 0.5712\n",
      ">> epoch: 1, test_acc: 0.8754, test_f1: 0.5654\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 2\n",
      "epoch: 2, loss: 0.4596, train acc: 0.8542\n",
      "epoch: 2, loss: 0.3769, train acc: 0.8750\n",
      "epoch: 2, loss: 0.3672, train acc: 0.8702\n",
      "epoch: 2, loss: 0.3737, train acc: 0.8715\n",
      "epoch: 2, loss: 0.3459, train acc: 0.8832\n",
      "epoch: 2, loss: 0.3274, train acc: 0.8862\n",
      "epoch: 2, loss: 0.3326, train acc: 0.8826\n",
      "epoch: 2, loss: 0.3464, train acc: 0.8799\n",
      "epoch: 2, loss: 0.3440, train acc: 0.8779\n",
      "epoch: 2, loss: 0.3394, train acc: 0.8802\n",
      "epoch: 2, loss: 0.3484, train acc: 0.8797\n",
      "epoch: 2, loss: 0.3410, train acc: 0.8825\n",
      "epoch: 2, loss: 0.3410, train acc: 0.8849\n",
      "epoch: 2, loss: 0.3363, train acc: 0.8869\n",
      "epoch: 2, loss: 0.3239, train acc: 0.8913\n",
      "epoch: 2, loss: 0.3228, train acc: 0.8910\n",
      "epoch: 2, loss: 0.3134, train acc: 0.8938\n",
      "epoch: 2, loss: 0.3092, train acc: 0.8956\n",
      "epoch: 2, loss: 0.3147, train acc: 0.8945\n",
      "epoch: 2, loss: 0.3157, train acc: 0.8941\n",
      "epoch: 2, loss: 0.3135, train acc: 0.8950\n",
      ">epoch: 2, val_acc: 0.8723, val_f1: 0.5789\n",
      ">> epoch: 2, test_acc: 0.8846, test_f1: 0.5730\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 3\n",
      "epoch: 3, loss: 0.2897, train acc: 0.8750\n",
      "epoch: 3, loss: 0.1818, train acc: 0.9375\n",
      "epoch: 3, loss: 0.1581, train acc: 0.9531\n",
      "epoch: 3, loss: 0.2236, train acc: 0.9265\n",
      "epoch: 3, loss: 0.2181, train acc: 0.9261\n",
      "epoch: 3, loss: 0.1968, train acc: 0.9329\n",
      "epoch: 3, loss: 0.1841, train acc: 0.9395\n",
      "epoch: 3, loss: 0.1800, train acc: 0.9392\n",
      "epoch: 3, loss: 0.1752, train acc: 0.9405\n",
      "epoch: 3, loss: 0.1706, train acc: 0.9415\n",
      "epoch: 3, loss: 0.1775, train acc: 0.9411\n",
      "epoch: 3, loss: 0.1858, train acc: 0.9408\n",
      "epoch: 3, loss: 0.1880, train acc: 0.9415\n",
      "epoch: 3, loss: 0.1829, train acc: 0.9412\n",
      "epoch: 3, loss: 0.1870, train acc: 0.9392\n",
      "epoch: 3, loss: 0.1978, train acc: 0.9343\n",
      "epoch: 3, loss: 0.2007, train acc: 0.9329\n",
      "epoch: 3, loss: 0.2038, train acc: 0.9325\n",
      "epoch: 3, loss: 0.2053, train acc: 0.9327\n",
      "epoch: 3, loss: 0.2030, train acc: 0.9336\n",
      "epoch: 3, loss: 0.2094, train acc: 0.9326\n",
      ">epoch: 3, val_acc: 0.8617, val_f1: 0.6334\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 4\n",
      "epoch: 4, loss: 0.0475, train acc: 1.0000\n",
      "epoch: 4, loss: 0.2133, train acc: 0.9167\n",
      "epoch: 4, loss: 0.1904, train acc: 0.9205\n",
      "epoch: 4, loss: 0.1734, train acc: 0.9219\n",
      "epoch: 4, loss: 0.1710, train acc: 0.9226\n",
      "epoch: 4, loss: 0.1627, train acc: 0.9327\n",
      "epoch: 4, loss: 0.1694, train acc: 0.9335\n",
      "epoch: 4, loss: 0.1557, train acc: 0.9358\n",
      "epoch: 4, loss: 0.1537, train acc: 0.9360\n",
      "epoch: 4, loss: 0.1483, train acc: 0.9375\n",
      "epoch: 4, loss: 0.1430, train acc: 0.9400\n",
      "epoch: 4, loss: 0.1450, train acc: 0.9386\n",
      "epoch: 4, loss: 0.1591, train acc: 0.9365\n",
      "epoch: 4, loss: 0.1568, train acc: 0.9394\n",
      "epoch: 4, loss: 0.1561, train acc: 0.9401\n",
      "epoch: 4, loss: 0.1550, train acc: 0.9408\n",
      "epoch: 4, loss: 0.1573, train acc: 0.9414\n",
      "epoch: 4, loss: 0.1575, train acc: 0.9411\n",
      "epoch: 4, loss: 0.1580, train acc: 0.9409\n",
      "epoch: 4, loss: 0.1620, train acc: 0.9408\n",
      "epoch: 4, loss: 0.1669, train acc: 0.9387\n",
      "epoch: 4, loss: 0.1652, train acc: 0.9403\n",
      ">epoch: 4, val_acc: 0.8830, val_f1: 0.6451\n",
      ">> epoch: 4, test_acc: 0.8908, test_f1: 0.7224\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 5\n",
      "epoch: 5, loss: 0.1308, train acc: 0.9250\n",
      "epoch: 5, loss: 0.1132, train acc: 0.9437\n",
      "epoch: 5, loss: 0.1073, train acc: 0.9500\n",
      "epoch: 5, loss: 0.1089, train acc: 0.9563\n",
      "epoch: 5, loss: 0.1099, train acc: 0.9525\n",
      "epoch: 5, loss: 0.1101, train acc: 0.9542\n",
      "epoch: 5, loss: 0.1005, train acc: 0.9589\n",
      "epoch: 5, loss: 0.1031, train acc: 0.9578\n",
      "epoch: 5, loss: 0.1021, train acc: 0.9597\n",
      "epoch: 5, loss: 0.1045, train acc: 0.9575\n",
      "epoch: 5, loss: 0.1083, train acc: 0.9545\n",
      "epoch: 5, loss: 0.1062, train acc: 0.9563\n",
      "epoch: 5, loss: 0.1028, train acc: 0.9577\n",
      "epoch: 5, loss: 0.1013, train acc: 0.9580\n",
      "epoch: 5, loss: 0.0978, train acc: 0.9600\n",
      "epoch: 5, loss: 0.0951, train acc: 0.9602\n",
      "epoch: 5, loss: 0.1007, train acc: 0.9574\n",
      "epoch: 5, loss: 0.0992, train acc: 0.9590\n",
      "epoch: 5, loss: 0.0989, train acc: 0.9592\n",
      "epoch: 5, loss: 0.0993, train acc: 0.9594\n",
      "epoch: 5, loss: 0.1027, train acc: 0.9589\n",
      ">epoch: 5, val_acc: 0.8777, val_f1: 0.6774\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 6\n",
      "epoch: 6, loss: 0.0473, train acc: 0.9844\n",
      "epoch: 6, loss: 0.0632, train acc: 0.9792\n",
      "epoch: 6, loss: 0.0587, train acc: 0.9777\n",
      "epoch: 6, loss: 0.0689, train acc: 0.9704\n",
      "epoch: 6, loss: 0.0939, train acc: 0.9688\n",
      "epoch: 6, loss: 0.0881, train acc: 0.9698\n",
      "epoch: 6, loss: 0.0826, train acc: 0.9724\n",
      "epoch: 6, loss: 0.0814, train acc: 0.9744\n",
      "epoch: 6, loss: 0.0752, train acc: 0.9773\n",
      "epoch: 6, loss: 0.0819, train acc: 0.9745\n",
      "epoch: 6, loss: 0.0806, train acc: 0.9745\n",
      "epoch: 6, loss: 0.0814, train acc: 0.9735\n",
      "epoch: 6, loss: 0.0769, train acc: 0.9756\n",
      "epoch: 6, loss: 0.0737, train acc: 0.9764\n",
      "epoch: 6, loss: 0.0739, train acc: 0.9764\n",
      "epoch: 6, loss: 0.0758, train acc: 0.9755\n",
      "epoch: 6, loss: 0.0761, train acc: 0.9754\n",
      "epoch: 6, loss: 0.0810, train acc: 0.9747\n",
      "epoch: 6, loss: 0.0798, train acc: 0.9747\n",
      "epoch: 6, loss: 0.0777, train acc: 0.9747\n",
      "epoch: 6, loss: 0.0810, train acc: 0.9736\n",
      ">epoch: 6, val_acc: 0.8883, val_f1: 0.7078\n",
      ">> epoch: 6, test_acc: 0.9015, test_f1: 0.7122\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 7\n",
      "epoch: 7, loss: 0.0410, train acc: 0.9583\n",
      "epoch: 7, loss: 0.0536, train acc: 0.9766\n",
      "epoch: 7, loss: 0.0547, train acc: 0.9712\n",
      "epoch: 7, loss: 0.0518, train acc: 0.9757\n",
      "epoch: 7, loss: 0.0719, train acc: 0.9674\n",
      "epoch: 7, loss: 0.0673, train acc: 0.9710\n",
      "epoch: 7, loss: 0.0701, train acc: 0.9697\n",
      "epoch: 7, loss: 0.0622, train acc: 0.9737\n",
      "epoch: 7, loss: 0.0600, train acc: 0.9753\n",
      "epoch: 7, loss: 0.0610, train acc: 0.9740\n",
      "epoch: 7, loss: 0.0579, train acc: 0.9752\n",
      "epoch: 7, loss: 0.0554, train acc: 0.9774\n",
      "epoch: 7, loss: 0.0525, train acc: 0.9792\n",
      "epoch: 7, loss: 0.0599, train acc: 0.9752\n",
      "epoch: 7, loss: 0.0575, train acc: 0.9760\n",
      "epoch: 7, loss: 0.0573, train acc: 0.9768\n",
      "epoch: 7, loss: 0.0553, train acc: 0.9774\n",
      "epoch: 7, loss: 0.0532, train acc: 0.9780\n",
      "epoch: 7, loss: 0.0517, train acc: 0.9792\n",
      "epoch: 7, loss: 0.0510, train acc: 0.9790\n",
      "epoch: 7, loss: 0.0568, train acc: 0.9782\n",
      ">epoch: 7, val_acc: 0.8777, val_f1: 0.6412\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 8\n",
      "epoch: 8, loss: 0.0163, train acc: 1.0000\n",
      "epoch: 8, loss: 0.0709, train acc: 0.9554\n",
      "epoch: 8, loss: 0.0477, train acc: 0.9688\n",
      "epoch: 8, loss: 0.0421, train acc: 0.9743\n",
      "epoch: 8, loss: 0.0377, train acc: 0.9773\n",
      "epoch: 8, loss: 0.0359, train acc: 0.9792\n",
      "epoch: 8, loss: 0.0367, train acc: 0.9785\n",
      "epoch: 8, loss: 0.0435, train acc: 0.9780\n",
      "epoch: 8, loss: 0.0419, train acc: 0.9792\n",
      "epoch: 8, loss: 0.0402, train acc: 0.9801\n",
      "epoch: 8, loss: 0.0378, train acc: 0.9820\n",
      "epoch: 8, loss: 0.0359, train acc: 0.9825\n",
      "epoch: 8, loss: 0.0411, train acc: 0.9808\n",
      "epoch: 8, loss: 0.0501, train acc: 0.9785\n",
      "epoch: 8, loss: 0.0492, train acc: 0.9792\n",
      "epoch: 8, loss: 0.0564, train acc: 0.9781\n",
      "epoch: 8, loss: 0.0589, train acc: 0.9771\n",
      "epoch: 8, loss: 0.0616, train acc: 0.9749\n",
      "epoch: 8, loss: 0.0611, train acc: 0.9755\n",
      "epoch: 8, loss: 0.0620, train acc: 0.9762\n",
      "epoch: 8, loss: 0.0604, train acc: 0.9767\n",
      ">epoch: 8, val_acc: 0.8617, val_f1: 0.6606\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 9\n",
      "epoch: 9, loss: 0.0536, train acc: 1.0000\n",
      "epoch: 9, loss: 0.0182, train acc: 1.0000\n",
      "epoch: 9, loss: 0.0190, train acc: 0.9943\n",
      "epoch: 9, loss: 0.0177, train acc: 0.9961\n",
      "epoch: 9, loss: 0.0378, train acc: 0.9881\n",
      "epoch: 9, loss: 0.0342, train acc: 0.9880\n",
      "epoch: 9, loss: 0.0370, train acc: 0.9879\n",
      "epoch: 9, loss: 0.0379, train acc: 0.9878\n",
      "epoch: 9, loss: 0.0381, train acc: 0.9878\n",
      "epoch: 9, loss: 0.0395, train acc: 0.9851\n",
      "epoch: 9, loss: 0.0392, train acc: 0.9853\n",
      "epoch: 9, loss: 0.0378, train acc: 0.9855\n",
      "epoch: 9, loss: 0.0374, train acc: 0.9846\n",
      "epoch: 9, loss: 0.0359, train acc: 0.9848\n",
      "epoch: 9, loss: 0.0394, train acc: 0.9824\n",
      "epoch: 9, loss: 0.0374, train acc: 0.9836\n",
      "epoch: 9, loss: 0.0378, train acc: 0.9830\n",
      "epoch: 9, loss: 0.0366, train acc: 0.9833\n",
      "epoch: 9, loss: 0.0377, train acc: 0.9828\n",
      "epoch: 9, loss: 0.0405, train acc: 0.9824\n",
      "epoch: 9, loss: 0.0433, train acc: 0.9814\n",
      "epoch: 9, loss: 0.0446, train acc: 0.9811\n",
      ">epoch: 9, val_acc: 0.8564, val_f1: 0.6567\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 10\n",
      "epoch: 10, loss: 0.0744, train acc: 0.9500\n",
      "epoch: 10, loss: 0.0548, train acc: 0.9750\n",
      "epoch: 10, loss: 0.0449, train acc: 0.9833\n",
      "epoch: 10, loss: 0.0479, train acc: 0.9781\n",
      "epoch: 10, loss: 0.0479, train acc: 0.9775\n",
      "epoch: 10, loss: 0.0457, train acc: 0.9792\n",
      "epoch: 10, loss: 0.0453, train acc: 0.9786\n",
      "epoch: 10, loss: 0.0430, train acc: 0.9797\n",
      "epoch: 10, loss: 0.0391, train acc: 0.9819\n",
      "epoch: 10, loss: 0.0373, train acc: 0.9825\n",
      "epoch: 10, loss: 0.0420, train acc: 0.9807\n",
      "epoch: 10, loss: 0.0398, train acc: 0.9823\n",
      "epoch: 10, loss: 0.0381, train acc: 0.9827\n",
      "epoch: 10, loss: 0.0410, train acc: 0.9821\n",
      "epoch: 10, loss: 0.0441, train acc: 0.9800\n",
      "epoch: 10, loss: 0.0464, train acc: 0.9797\n",
      "epoch: 10, loss: 0.0456, train acc: 0.9801\n",
      "epoch: 10, loss: 0.0440, train acc: 0.9806\n",
      "epoch: 10, loss: 0.0436, train acc: 0.9809\n",
      "epoch: 10, loss: 0.0428, train acc: 0.9812\n",
      "epoch: 10, loss: 0.0426, train acc: 0.9810\n",
      ">epoch: 10, val_acc: 0.8723, val_f1: 0.6715\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 11\n",
      "epoch: 11, loss: 0.0305, train acc: 0.9844\n",
      "epoch: 11, loss: 0.0241, train acc: 0.9861\n",
      "epoch: 11, loss: 0.0532, train acc: 0.9732\n",
      "epoch: 11, loss: 0.0465, train acc: 0.9770\n",
      "epoch: 11, loss: 0.0512, train acc: 0.9792\n",
      "epoch: 11, loss: 0.0429, train acc: 0.9828\n",
      "epoch: 11, loss: 0.0442, train acc: 0.9798\n",
      "epoch: 11, loss: 0.0409, train acc: 0.9808\n",
      "epoch: 11, loss: 0.0397, train acc: 0.9801\n",
      "epoch: 11, loss: 0.0403, train acc: 0.9809\n",
      "epoch: 11, loss: 0.0366, train acc: 0.9826\n",
      "epoch: 11, loss: 0.0350, train acc: 0.9831\n",
      "epoch: 11, loss: 0.0331, train acc: 0.9844\n",
      "epoch: 11, loss: 0.0337, train acc: 0.9846\n",
      "epoch: 11, loss: 0.0327, train acc: 0.9856\n",
      "epoch: 11, loss: 0.0386, train acc: 0.9834\n",
      "epoch: 11, loss: 0.0375, train acc: 0.9836\n",
      "epoch: 11, loss: 0.0397, train acc: 0.9824\n",
      "epoch: 11, loss: 0.0393, train acc: 0.9820\n",
      "epoch: 11, loss: 0.0379, train acc: 0.9823\n",
      "epoch: 11, loss: 0.0391, train acc: 0.9820\n",
      ">epoch: 11, val_acc: 0.8564, val_f1: 0.6498\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 12\n",
      "epoch: 12, loss: 0.0679, train acc: 0.9792\n",
      "epoch: 12, loss: 0.0433, train acc: 0.9844\n",
      "epoch: 12, loss: 0.0475, train acc: 0.9760\n",
      "epoch: 12, loss: 0.0535, train acc: 0.9688\n",
      "epoch: 12, loss: 0.0484, train acc: 0.9701\n",
      "epoch: 12, loss: 0.0493, train acc: 0.9710\n",
      "epoch: 12, loss: 0.0422, train acc: 0.9754\n",
      "epoch: 12, loss: 0.0416, train acc: 0.9753\n",
      "epoch: 12, loss: 0.0393, train acc: 0.9767\n",
      "epoch: 12, loss: 0.0404, train acc: 0.9753\n",
      "epoch: 12, loss: 0.0381, train acc: 0.9764\n",
      "epoch: 12, loss: 0.0389, train acc: 0.9763\n",
      "epoch: 12, loss: 0.0366, train acc: 0.9782\n",
      "epoch: 12, loss: 0.0426, train acc: 0.9752\n",
      "epoch: 12, loss: 0.0402, train acc: 0.9769\n",
      "epoch: 12, loss: 0.0450, train acc: 0.9760\n",
      "epoch: 12, loss: 0.0439, train acc: 0.9774\n",
      "epoch: 12, loss: 0.0430, train acc: 0.9780\n",
      "epoch: 12, loss: 0.0415, train acc: 0.9792\n",
      "epoch: 12, loss: 0.0407, train acc: 0.9802\n",
      "epoch: 12, loss: 0.0403, train acc: 0.9806\n",
      ">epoch: 12, val_acc: 0.8723, val_f1: 0.6751\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 13\n",
      "epoch: 13, loss: 0.0373, train acc: 1.0000\n",
      "epoch: 13, loss: 0.0353, train acc: 0.9821\n",
      "epoch: 13, loss: 0.0443, train acc: 0.9740\n",
      "epoch: 13, loss: 0.0362, train acc: 0.9779\n",
      "epoch: 13, loss: 0.0301, train acc: 0.9830\n",
      "epoch: 13, loss: 0.0250, train acc: 0.9861\n",
      "epoch: 13, loss: 0.0215, train acc: 0.9883\n",
      "epoch: 13, loss: 0.0213, train acc: 0.9882\n",
      "epoch: 13, loss: 0.0234, train acc: 0.9881\n",
      "epoch: 13, loss: 0.0237, train acc: 0.9880\n",
      "epoch: 13, loss: 0.0218, train acc: 0.9892\n",
      "epoch: 13, loss: 0.0206, train acc: 0.9901\n",
      "epoch: 13, loss: 0.0190, train acc: 0.9909\n",
      "epoch: 13, loss: 0.0196, train acc: 0.9907\n",
      "epoch: 13, loss: 0.0223, train acc: 0.9887\n",
      "epoch: 13, loss: 0.0221, train acc: 0.9886\n",
      "epoch: 13, loss: 0.0265, train acc: 0.9878\n",
      "epoch: 13, loss: 0.0390, train acc: 0.9856\n",
      "epoch: 13, loss: 0.0416, train acc: 0.9857\n",
      "epoch: 13, loss: 0.0510, train acc: 0.9832\n",
      "epoch: 13, loss: 0.0544, train acc: 0.9822\n",
      ">epoch: 13, val_acc: 0.8564, val_f1: 0.6501\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 14\n",
      "epoch: 14, loss: 0.1622, train acc: 0.9375\n",
      "epoch: 14, loss: 0.1279, train acc: 0.9583\n",
      "epoch: 14, loss: 0.1134, train acc: 0.9602\n",
      "epoch: 14, loss: 0.1229, train acc: 0.9492\n",
      "epoch: 14, loss: 0.1287, train acc: 0.9494\n",
      "epoch: 14, loss: 0.1346, train acc: 0.9543\n",
      "epoch: 14, loss: 0.1371, train acc: 0.9536\n",
      "epoch: 14, loss: 0.1299, train acc: 0.9583\n",
      "epoch: 14, loss: 0.1461, train acc: 0.9558\n",
      "epoch: 14, loss: 0.1503, train acc: 0.9538\n",
      "epoch: 14, loss: 0.1590, train acc: 0.9485\n",
      "epoch: 14, loss: 0.1596, train acc: 0.9475\n",
      "epoch: 14, loss: 0.1671, train acc: 0.9467\n",
      "epoch: 14, loss: 0.1662, train acc: 0.9470\n",
      "epoch: 14, loss: 0.1616, train acc: 0.9489\n",
      "epoch: 14, loss: 0.1611, train acc: 0.9482\n",
      "epoch: 14, loss: 0.1573, train acc: 0.9483\n",
      "epoch: 14, loss: 0.1580, train acc: 0.9477\n",
      "epoch: 14, loss: 0.1562, train acc: 0.9485\n",
      "epoch: 14, loss: 0.1545, train acc: 0.9473\n",
      "epoch: 14, loss: 0.1515, train acc: 0.9480\n",
      "epoch: 14, loss: 0.1499, train acc: 0.9480\n",
      ">epoch: 14, val_acc: 0.8670, val_f1: 0.6755\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 15\n",
      "epoch: 15, loss: 0.1595, train acc: 0.9625\n",
      "epoch: 15, loss: 0.1364, train acc: 0.9688\n",
      "epoch: 15, loss: 0.1350, train acc: 0.9625\n",
      "epoch: 15, loss: 0.1251, train acc: 0.9656\n",
      "epoch: 15, loss: 0.1119, train acc: 0.9675\n",
      "epoch: 15, loss: 0.1146, train acc: 0.9667\n",
      "epoch: 15, loss: 0.1046, train acc: 0.9679\n",
      "epoch: 15, loss: 0.1025, train acc: 0.9672\n",
      "epoch: 15, loss: 0.1003, train acc: 0.9667\n",
      "epoch: 15, loss: 0.0996, train acc: 0.9650\n",
      "epoch: 15, loss: 0.0962, train acc: 0.9659\n",
      "epoch: 15, loss: 0.0950, train acc: 0.9656\n",
      "epoch: 15, loss: 0.0906, train acc: 0.9673\n",
      "epoch: 15, loss: 0.0903, train acc: 0.9661\n",
      "epoch: 15, loss: 0.0862, train acc: 0.9667\n",
      "epoch: 15, loss: 0.0858, train acc: 0.9648\n",
      "epoch: 15, loss: 0.0815, train acc: 0.9662\n",
      "epoch: 15, loss: 0.0780, train acc: 0.9674\n",
      "epoch: 15, loss: 0.0801, train acc: 0.9671\n",
      "epoch: 15, loss: 0.0789, train acc: 0.9681\n",
      "epoch: 15, loss: 0.0771, train acc: 0.9690\n",
      ">epoch: 15, val_acc: 0.8617, val_f1: 0.6500\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 16\n",
      "epoch: 16, loss: 0.0819, train acc: 0.9688\n",
      "epoch: 16, loss: 0.0695, train acc: 0.9722\n",
      "epoch: 16, loss: 0.0545, train acc: 0.9777\n",
      "epoch: 16, loss: 0.0479, train acc: 0.9803\n",
      "epoch: 16, loss: 0.0515, train acc: 0.9766\n",
      "epoch: 16, loss: 0.0526, train acc: 0.9741\n",
      "epoch: 16, loss: 0.0464, train acc: 0.9779\n",
      "epoch: 16, loss: 0.0584, train acc: 0.9744\n",
      "epoch: 16, loss: 0.0737, train acc: 0.9716\n",
      "epoch: 16, loss: 0.0784, train acc: 0.9707\n",
      "epoch: 16, loss: 0.0829, train acc: 0.9699\n",
      "epoch: 16, loss: 0.0791, train acc: 0.9703\n",
      "epoch: 16, loss: 0.0776, train acc: 0.9717\n",
      "epoch: 16, loss: 0.0750, train acc: 0.9719\n",
      "epoch: 16, loss: 0.0743, train acc: 0.9721\n",
      "epoch: 16, loss: 0.0739, train acc: 0.9723\n",
      "epoch: 16, loss: 0.0739, train acc: 0.9732\n",
      "epoch: 16, loss: 0.0722, train acc: 0.9733\n",
      "epoch: 16, loss: 0.0738, train acc: 0.9734\n",
      "epoch: 16, loss: 0.0735, train acc: 0.9735\n",
      "epoch: 16, loss: 0.0707, train acc: 0.9748\n",
      ">epoch: 16, val_acc: 0.8777, val_f1: 0.6863\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 17\n",
      "epoch: 17, loss: 0.0121, train acc: 1.0000\n",
      "epoch: 17, loss: 0.0100, train acc: 1.0000\n",
      "epoch: 17, loss: 0.0254, train acc: 0.9904\n",
      "epoch: 17, loss: 0.0249, train acc: 0.9896\n",
      "epoch: 17, loss: 0.0208, train acc: 0.9918\n",
      "epoch: 17, loss: 0.0267, train acc: 0.9911\n",
      "epoch: 17, loss: 0.0351, train acc: 0.9905\n",
      "epoch: 17, loss: 0.0377, train acc: 0.9868\n",
      "epoch: 17, loss: 0.0359, train acc: 0.9869\n",
      "epoch: 17, loss: 0.0324, train acc: 0.9883\n",
      "epoch: 17, loss: 0.0313, train acc: 0.9882\n",
      "epoch: 17, loss: 0.0306, train acc: 0.9881\n",
      "epoch: 17, loss: 0.0297, train acc: 0.9881\n",
      "epoch: 17, loss: 0.0296, train acc: 0.9881\n",
      "epoch: 17, loss: 0.0330, train acc: 0.9872\n",
      "epoch: 17, loss: 0.0353, train acc: 0.9864\n",
      "epoch: 17, loss: 0.0421, train acc: 0.9857\n",
      "epoch: 17, loss: 0.0456, train acc: 0.9837\n",
      "epoch: 17, loss: 0.0454, train acc: 0.9825\n",
      "epoch: 17, loss: 0.0469, train acc: 0.9815\n",
      "epoch: 17, loss: 0.0478, train acc: 0.9812\n",
      ">epoch: 17, val_acc: 0.8670, val_f1: 0.6281\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 18\n",
      "epoch: 18, loss: 0.0329, train acc: 1.0000\n",
      "epoch: 18, loss: 0.0283, train acc: 0.9911\n",
      "epoch: 18, loss: 0.0255, train acc: 0.9896\n",
      "epoch: 18, loss: 0.0404, train acc: 0.9779\n",
      "epoch: 18, loss: 0.0395, train acc: 0.9801\n",
      "epoch: 18, loss: 0.0362, train acc: 0.9815\n",
      "epoch: 18, loss: 0.0366, train acc: 0.9824\n",
      "epoch: 18, loss: 0.0342, train acc: 0.9831\n",
      "epoch: 18, loss: 0.0418, train acc: 0.9821\n",
      "epoch: 18, loss: 0.0496, train acc: 0.9827\n",
      "epoch: 18, loss: 0.0492, train acc: 0.9832\n",
      "epoch: 18, loss: 0.0479, train acc: 0.9836\n",
      "epoch: 18, loss: 0.0443, train acc: 0.9849\n",
      "epoch: 18, loss: 0.0434, train acc: 0.9851\n",
      "epoch: 18, loss: 0.0414, train acc: 0.9852\n",
      "epoch: 18, loss: 0.0416, train acc: 0.9846\n",
      "epoch: 18, loss: 0.0396, train acc: 0.9855\n",
      "epoch: 18, loss: 0.0377, train acc: 0.9864\n",
      "epoch: 18, loss: 0.0377, train acc: 0.9864\n",
      "epoch: 18, loss: 0.0423, train acc: 0.9839\n",
      "epoch: 18, loss: 0.0441, train acc: 0.9822\n",
      ">epoch: 18, val_acc: 0.8830, val_f1: 0.6818\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 19\n",
      "epoch: 19, loss: 0.0230, train acc: 1.0000\n",
      "epoch: 19, loss: 0.0722, train acc: 0.9792\n",
      "epoch: 19, loss: 0.0524, train acc: 0.9886\n",
      "epoch: 19, loss: 0.0468, train acc: 0.9844\n",
      "epoch: 19, loss: 0.0405, train acc: 0.9851\n",
      "epoch: 19, loss: 0.0425, train acc: 0.9856\n",
      "epoch: 19, loss: 0.0387, train acc: 0.9879\n",
      "epoch: 19, loss: 0.0404, train acc: 0.9861\n",
      "epoch: 19, loss: 0.0372, train acc: 0.9878\n",
      "epoch: 19, loss: 0.0365, train acc: 0.9864\n",
      "epoch: 19, loss: 0.0358, train acc: 0.9865\n",
      "epoch: 19, loss: 0.0378, train acc: 0.9844\n",
      "epoch: 19, loss: 0.0387, train acc: 0.9826\n",
      "epoch: 19, loss: 0.0390, train acc: 0.9820\n",
      "epoch: 19, loss: 0.0364, train acc: 0.9833\n",
      "epoch: 19, loss: 0.0367, train acc: 0.9827\n",
      "epoch: 19, loss: 0.0387, train acc: 0.9815\n",
      "epoch: 19, loss: 0.0400, train acc: 0.9811\n",
      "epoch: 19, loss: 0.0407, train acc: 0.9808\n",
      "epoch: 19, loss: 0.0425, train acc: 0.9805\n",
      "epoch: 19, loss: 0.0432, train acc: 0.9802\n",
      "epoch: 19, loss: 0.0451, train acc: 0.9799\n",
      ">epoch: 19, val_acc: 0.8777, val_f1: 0.6751\n",
      "Namespace(model_type='tri_gcn', modules={'tgcn': True, 'semgcn': True, 'lexgcn': True}, num_layers={'tgcn': 2, 'semgcn': 2, 'lexgcn': 2}, year='2016', train_file='data/train2016restaurant.txt', test_file='data/test2016restaurant.txt', model_path='test_models/2016tri_gcn+concat_seed699_reg0.0005_drop0.4_cdrop0.4_lr2.5e-05_epochs20_adam', val_file='val.txt', log='log', bert_model='bert_large_uncased', cooc_path='cooc_matrix.csv', cooc=       100       1000   10000  10003     10005     10007     10009     1001   \\\n",
      "100      0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "1000     0.0  0.000000    0.0    0.0  0.000606  0.000303  0.000606  0.035120   \n",
      "10000    0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.333333   \n",
      "10003    0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "10005    0.0  0.333333    0.0    0.0  0.000000  0.166667  0.000000  0.166667   \n",
      "...      ...       ...    ...    ...       ...       ...       ...       ...   \n",
      "9992     0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "9993     0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "9996     0.0  0.500000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "9997     0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "9998     0.0  0.400000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "       10010  10015  ...     9986      9987      999    9990      9991   \\\n",
      "100      0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "1000     0.0    0.0  ...  0.002422  0.000303  0.062368    0.0  0.000606   \n",
      "10000    0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "10003    0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "10005    0.0    0.0  ...  0.000000  0.000000  0.333333    0.0  0.000000   \n",
      "...      ...    ...  ...       ...       ...       ...    ...       ...   \n",
      "9992     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "9993     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "9996     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "9997     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "9998     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "\n",
      "       9992   9993      9996   9997      9998   \n",
      "100      0.0    0.0  0.000000    0.0  0.000000  \n",
      "1000     0.0    0.0  0.000606    0.0  0.001211  \n",
      "10000    0.0    0.0  0.000000    0.0  0.000000  \n",
      "10003    0.0    0.0  0.000000    0.0  0.000000  \n",
      "10005    0.0    0.0  0.000000    0.0  0.000000  \n",
      "...      ...    ...       ...    ...       ...  \n",
      "9992     0.0    0.0  0.000000    0.0  0.000000  \n",
      "9993     0.0    0.0  0.000000    0.0  0.000000  \n",
      "9996     0.0    0.0  0.000000    0.0  0.000000  \n",
      "9997     0.0    0.0  0.000000    0.0  0.000000  \n",
      "9998     0.0    0.0  0.000000    0.0  0.000000  \n",
      "\n",
      "[13075 rows x 13075 columns], learning_rate=2.5e-05, dropout=0.4, concat_dropout=0.4, bert_dropout=0.2, l2reg=0.0005, num_epoch=20, batch_size=16, log_step=5, max_seq_len=100, polarities_dim=3, device=device(type='cuda'), seed=699, valset_ratio=0.1, do_train=True, do_eval=True, eval_epoch_num=0, fusion_type='concat', use_ensemble=True, save_models='none', print_sent=False, optim='adam', n_gpu=1)\n",
      "{'none': 0, 'acl': 1, 'acl:relcl': 2, 'advcl': 3, 'advcl:relcl': 4, 'advmod': 5, 'amod': 6, 'appos': 7, 'aux': 8, 'aux:pass': 9, 'case': 10, 'cc': 11, 'cc:preconj': 12, 'ccomp': 13, 'compound': 14, 'compound:prt': 15, 'conj': 16, 'cop': 17, 'csubj': 18, 'dep': 19, 'det': 20, 'det:predet': 21, 'discourse': 22, 'dislocated': 23, 'expl': 24, 'fixed': 25, 'flat': 26, 'iobj': 27, 'list': 28, 'mark': 29, 'nmod': 30, 'nmod:npmod': 31, 'nmod:poss': 32, 'nmod:tmod': 33, 'nsubj': 34, 'nsubj:outer': 35, 'nsubj:pass': 36, 'nummod': 37, 'obj': 38, 'obl': 39, 'obl:agent': 40, 'obl:npmod': 41, 'obl:tmod': 42, 'parataxis': 43, 'punct': 44, 'reparandum': 45, 'root': 46, 'vocative': 47, 'xcomp': 48}\n",
      "{'-1': 0, '0': 1, '1': 2}\n",
      "Model name 'bert_large_uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming 'bert_large_uncased' is a path or url to a directory containing tokenizer files.\n",
      "Didn't find file bert_large_uncased/added_tokens.json. We won't load it.\n",
      "Didn't find file bert_large_uncased/special_tokens_map.json. We won't load it.\n",
      "Didn't find file bert_large_uncased/tokenizer_config.json. We won't load it.\n",
      "loading file bert_large_uncased/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "{\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 3,\n",
      "  \"num_types\": 49,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file bert_large_uncased/pytorch_model.bin\n",
      "Weights of AsaTgcnSem not initialized from pretrained model: ['gate_weight', 'gate_bias', 'ensemble', 'TGCNLayers.0.weight', 'TGCNLayers.0.bias', 'TGCNLayers.0.dense.weight', 'TGCNLayers.1.weight', 'TGCNLayers.1.bias', 'TGCNLayers.1.dense.weight', 'SemGCNLayers.0.weight', 'SemGCNLayers.0.bias', 'SemGCNLayers.1.weight', 'SemGCNLayers.1.bias', 'LexGCNLayers.0.weight', 'LexGCNLayers.0.bias', 'LexGCNLayers.1.weight', 'LexGCNLayers.1.bias', 'fc_single.weight', 'fc_single.bias', 'ensemble_linear_tgcn.weight', 'ensemble_linear_tgcn.bias', 'ensemble_linear_semgcn.weight', 'ensemble_linear_semgcn.bias', 'ensemble_linear_lexgcn.weight', 'ensemble_linear_lexgcn.bias', 'dep_embedding.weight']\n",
      "Weights from pretrained model not used in AsaTgcnSem: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "[{'input_ids': tensor([  101, 13325,  2013,  3025,  8466,  2023,  2109,  2000,  2022,  1037,\n",
      "         2204,  2173,  1010,  2021,  2025,  2151,  2936,  1012,   102,  2173,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'valid_ids': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'segment_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'mem_valid_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'dep_adj_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'dep_value_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'polarity': 0, 'raw_text': 'judging from previous posts this used to be a good place , but not any longer .', 'aspect': 'place'}]\n",
      "[{'input_ids': tensor([  101, 13325,  2013,  3025,  8466,  2023,  2109,  2000,  2022,  1037,\n",
      "         2204,  2173,  1010,  2021,  2025,  2151,  2936,  1012,   102,  2173,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'valid_ids': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'segment_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'mem_valid_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'dep_adj_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'dep_value_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'polarity': 0, 'raw_text': 'judging from previous posts this used to be a good place , but not any longer .', 'aspect': 'place'}]\n",
      "[{'input_ids': tensor([  101,  4240,  2428,  2204, 10514,  6182,  1012,   102, 10514,  6182,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'valid_ids': tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'segment_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'mem_valid_ids': tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'dep_adj_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'dep_value_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 5,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'polarity': 2, 'raw_text': 'serves really good sushi .', 'aspect': 'sushi'}]\n",
      "cuda memory allocated: 1400893440\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 0\n",
      "epoch: 0, loss: 0.9373, train acc: 0.5375\n",
      "epoch: 0, loss: 0.8974, train acc: 0.5875\n",
      "epoch: 0, loss: 0.8679, train acc: 0.6083\n",
      "epoch: 0, loss: 0.8891, train acc: 0.6031\n",
      "epoch: 0, loss: 0.8784, train acc: 0.6000\n",
      "epoch: 0, loss: 0.8472, train acc: 0.6208\n",
      "epoch: 0, loss: 0.8432, train acc: 0.6286\n",
      "epoch: 0, loss: 0.8365, train acc: 0.6422\n",
      "epoch: 0, loss: 0.8281, train acc: 0.6431\n",
      "epoch: 0, loss: 0.8431, train acc: 0.6388\n",
      "epoch: 0, loss: 0.8326, train acc: 0.6443\n",
      "epoch: 0, loss: 0.8225, train acc: 0.6510\n",
      "epoch: 0, loss: 0.8287, train acc: 0.6519\n",
      "epoch: 0, loss: 0.8264, train acc: 0.6491\n",
      "epoch: 0, loss: 0.8200, train acc: 0.6558\n",
      "epoch: 0, loss: 0.8084, train acc: 0.6570\n",
      "epoch: 0, loss: 0.8038, train acc: 0.6581\n",
      "epoch: 0, loss: 0.8045, train acc: 0.6604\n",
      "epoch: 0, loss: 0.8094, train acc: 0.6605\n",
      "epoch: 0, loss: 0.8002, train acc: 0.6656\n",
      "epoch: 0, loss: 0.7955, train acc: 0.6667\n",
      ">epoch: 0, val_acc: 0.6755, val_f1: 0.2688\n",
      ">> epoch: 0, test_acc: 0.7508, test_f1: 0.3093\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 1\n",
      "epoch: 1, loss: 0.5985, train acc: 0.7344\n",
      "epoch: 1, loss: 0.7280, train acc: 0.6806\n",
      "epoch: 1, loss: 0.6915, train acc: 0.7188\n",
      "epoch: 1, loss: 0.6767, train acc: 0.7171\n",
      "epoch: 1, loss: 0.6902, train acc: 0.7083\n",
      "epoch: 1, loss: 0.6839, train acc: 0.7112\n",
      "epoch: 1, loss: 0.6783, train acc: 0.7096\n",
      "epoch: 1, loss: 0.6768, train acc: 0.7051\n",
      "epoch: 1, loss: 0.6753, train acc: 0.6989\n",
      "epoch: 1, loss: 0.6709, train acc: 0.7079\n",
      "epoch: 1, loss: 0.6618, train acc: 0.7188\n",
      "epoch: 1, loss: 0.6529, train acc: 0.7256\n",
      "epoch: 1, loss: 0.6392, train acc: 0.7354\n",
      "epoch: 1, loss: 0.6268, train acc: 0.7418\n",
      "epoch: 1, loss: 0.6233, train acc: 0.7432\n",
      "epoch: 1, loss: 0.6177, train acc: 0.7460\n",
      "epoch: 1, loss: 0.5979, train acc: 0.7560\n",
      "epoch: 1, loss: 0.5836, train acc: 0.7647\n",
      "epoch: 1, loss: 0.5808, train acc: 0.7673\n",
      "epoch: 1, loss: 0.5778, train acc: 0.7696\n",
      "epoch: 1, loss: 0.5691, train acc: 0.7746\n",
      ">epoch: 1, val_acc: 0.8564, val_f1: 0.5676\n",
      ">> epoch: 1, test_acc: 0.8800, test_f1: 0.5710\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 2\n",
      "epoch: 2, loss: 0.3415, train acc: 0.9167\n",
      "epoch: 2, loss: 0.3916, train acc: 0.8594\n",
      "epoch: 2, loss: 0.3916, train acc: 0.8510\n",
      "epoch: 2, loss: 0.3546, train acc: 0.8646\n",
      "epoch: 2, loss: 0.4024, train acc: 0.8505\n",
      "epoch: 2, loss: 0.4047, train acc: 0.8549\n",
      "epoch: 2, loss: 0.4062, train acc: 0.8485\n",
      "epoch: 2, loss: 0.3985, train acc: 0.8503\n",
      "epoch: 2, loss: 0.3878, train acc: 0.8590\n",
      "epoch: 2, loss: 0.3946, train acc: 0.8568\n",
      "epoch: 2, loss: 0.3834, train acc: 0.8608\n",
      "epoch: 2, loss: 0.3762, train acc: 0.8653\n",
      "epoch: 2, loss: 0.3613, train acc: 0.8700\n",
      "epoch: 2, loss: 0.3629, train acc: 0.8695\n",
      "epoch: 2, loss: 0.3652, train acc: 0.8682\n",
      "epoch: 2, loss: 0.3605, train acc: 0.8710\n",
      "epoch: 2, loss: 0.3578, train acc: 0.8712\n",
      "epoch: 2, loss: 0.3514, train acc: 0.8736\n",
      "epoch: 2, loss: 0.3531, train acc: 0.8730\n",
      "epoch: 2, loss: 0.3575, train acc: 0.8705\n",
      "epoch: 2, loss: 0.3586, train acc: 0.8689\n",
      ">epoch: 2, val_acc: 0.8670, val_f1: 0.5764\n",
      ">> epoch: 2, test_acc: 0.8892, test_f1: 0.6171\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 3\n",
      "epoch: 3, loss: 0.3447, train acc: 0.9062\n",
      "epoch: 3, loss: 0.2365, train acc: 0.9464\n",
      "epoch: 3, loss: 0.2029, train acc: 0.9479\n",
      "epoch: 3, loss: 0.1752, train acc: 0.9522\n",
      "epoch: 3, loss: 0.1497, train acc: 0.9631\n",
      "epoch: 3, loss: 0.1833, train acc: 0.9560\n",
      "epoch: 3, loss: 0.1784, train acc: 0.9531\n",
      "epoch: 3, loss: 0.2001, train acc: 0.9426\n",
      "epoch: 3, loss: 0.2003, train acc: 0.9420\n",
      "epoch: 3, loss: 0.1988, train acc: 0.9415\n",
      "epoch: 3, loss: 0.1893, train acc: 0.9447\n",
      "epoch: 3, loss: 0.2066, train acc: 0.9364\n",
      "epoch: 3, loss: 0.2084, train acc: 0.9355\n",
      "epoch: 3, loss: 0.2146, train acc: 0.9310\n",
      "epoch: 3, loss: 0.2230, train acc: 0.9280\n",
      "epoch: 3, loss: 0.2217, train acc: 0.9286\n",
      "epoch: 3, loss: 0.2212, train acc: 0.9268\n",
      "epoch: 3, loss: 0.2265, train acc: 0.9239\n",
      "epoch: 3, loss: 0.2318, train acc: 0.9219\n",
      "epoch: 3, loss: 0.2280, train acc: 0.9233\n",
      "epoch: 3, loss: 0.2290, train acc: 0.9234\n",
      ">epoch: 3, val_acc: 0.8830, val_f1: 0.7462\n",
      ">> epoch: 3, test_acc: 0.9062, test_f1: 0.6802\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 4\n",
      "epoch: 4, loss: 0.0771, train acc: 1.0000\n",
      "epoch: 4, loss: 0.2120, train acc: 0.9167\n",
      "epoch: 4, loss: 0.1779, train acc: 0.9318\n",
      "epoch: 4, loss: 0.1538, train acc: 0.9414\n",
      "epoch: 4, loss: 0.1368, train acc: 0.9494\n",
      "epoch: 4, loss: 0.1253, train acc: 0.9567\n",
      "epoch: 4, loss: 0.1582, train acc: 0.9496\n",
      "epoch: 4, loss: 0.1628, train acc: 0.9497\n",
      "epoch: 4, loss: 0.1578, train acc: 0.9512\n",
      "epoch: 4, loss: 0.1540, train acc: 0.9511\n",
      "epoch: 4, loss: 0.1656, train acc: 0.9461\n",
      "epoch: 4, loss: 0.1558, train acc: 0.9498\n",
      "epoch: 4, loss: 0.1546, train acc: 0.9508\n",
      "epoch: 4, loss: 0.1508, train acc: 0.9527\n",
      "epoch: 4, loss: 0.1509, train acc: 0.9507\n",
      "epoch: 4, loss: 0.1615, train acc: 0.9482\n",
      "epoch: 4, loss: 0.1651, train acc: 0.9460\n",
      "epoch: 4, loss: 0.1688, train acc: 0.9448\n",
      "epoch: 4, loss: 0.1743, train acc: 0.9430\n",
      "epoch: 4, loss: 0.1743, train acc: 0.9427\n",
      "epoch: 4, loss: 0.1759, train acc: 0.9418\n",
      "epoch: 4, loss: 0.1758, train acc: 0.9409\n",
      ">epoch: 4, val_acc: 0.8670, val_f1: 0.6505\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 5\n",
      "epoch: 5, loss: 0.1061, train acc: 0.9750\n",
      "epoch: 5, loss: 0.1242, train acc: 0.9563\n",
      "epoch: 5, loss: 0.1310, train acc: 0.9458\n",
      "epoch: 5, loss: 0.1262, train acc: 0.9469\n",
      "epoch: 5, loss: 0.1246, train acc: 0.9450\n",
      "epoch: 5, loss: 0.1321, train acc: 0.9437\n",
      "epoch: 5, loss: 0.1336, train acc: 0.9464\n",
      "epoch: 5, loss: 0.1396, train acc: 0.9437\n",
      "epoch: 5, loss: 0.1501, train acc: 0.9431\n",
      "epoch: 5, loss: 0.1449, train acc: 0.9463\n",
      "epoch: 5, loss: 0.1394, train acc: 0.9489\n",
      "epoch: 5, loss: 0.1362, train acc: 0.9510\n",
      "epoch: 5, loss: 0.1333, train acc: 0.9519\n",
      "epoch: 5, loss: 0.1351, train acc: 0.9509\n",
      "epoch: 5, loss: 0.1351, train acc: 0.9517\n",
      "epoch: 5, loss: 0.1340, train acc: 0.9531\n",
      "epoch: 5, loss: 0.1303, train acc: 0.9544\n",
      "epoch: 5, loss: 0.1370, train acc: 0.9514\n",
      "epoch: 5, loss: 0.1411, train acc: 0.9500\n",
      "epoch: 5, loss: 0.1464, train acc: 0.9469\n",
      "epoch: 5, loss: 0.1481, train acc: 0.9452\n",
      ">epoch: 5, val_acc: 0.8670, val_f1: 0.7585\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 6\n",
      "epoch: 6, loss: 0.0563, train acc: 0.9844\n",
      "epoch: 6, loss: 0.1001, train acc: 0.9722\n",
      "epoch: 6, loss: 0.0896, train acc: 0.9732\n",
      "epoch: 6, loss: 0.0906, train acc: 0.9704\n",
      "epoch: 6, loss: 0.0898, train acc: 0.9714\n",
      "epoch: 6, loss: 0.0861, train acc: 0.9720\n",
      "epoch: 6, loss: 0.0773, train acc: 0.9743\n",
      "epoch: 6, loss: 0.0840, train acc: 0.9728\n",
      "epoch: 6, loss: 0.0818, train acc: 0.9730\n",
      "epoch: 6, loss: 0.0856, train acc: 0.9719\n",
      "epoch: 6, loss: 0.0817, train acc: 0.9734\n",
      "epoch: 6, loss: 0.0947, train acc: 0.9693\n",
      "epoch: 6, loss: 0.0955, train acc: 0.9688\n",
      "epoch: 6, loss: 0.0985, train acc: 0.9683\n",
      "epoch: 6, loss: 0.0966, train acc: 0.9679\n",
      "epoch: 6, loss: 0.0975, train acc: 0.9676\n",
      "epoch: 6, loss: 0.0986, train acc: 0.9680\n",
      "epoch: 6, loss: 0.0976, train acc: 0.9670\n",
      "epoch: 6, loss: 0.0971, train acc: 0.9674\n",
      "epoch: 6, loss: 0.0946, train acc: 0.9678\n",
      "epoch: 6, loss: 0.0931, train acc: 0.9675\n",
      ">epoch: 6, val_acc: 0.8883, val_f1: 0.8209\n",
      ">> epoch: 6, test_acc: 0.9015, test_f1: 0.6724\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 7\n",
      "epoch: 7, loss: 0.0559, train acc: 0.9792\n",
      "epoch: 7, loss: 0.0517, train acc: 0.9766\n",
      "epoch: 7, loss: 0.0391, train acc: 0.9856\n",
      "epoch: 7, loss: 0.0407, train acc: 0.9861\n",
      "epoch: 7, loss: 0.0427, train acc: 0.9864\n",
      "epoch: 7, loss: 0.0405, train acc: 0.9866\n",
      "epoch: 7, loss: 0.0489, train acc: 0.9830\n",
      "epoch: 7, loss: 0.0517, train acc: 0.9803\n",
      "epoch: 7, loss: 0.0521, train acc: 0.9811\n",
      "epoch: 7, loss: 0.0535, train acc: 0.9818\n",
      "epoch: 7, loss: 0.0607, train acc: 0.9788\n",
      "epoch: 7, loss: 0.0596, train acc: 0.9784\n",
      "epoch: 7, loss: 0.0614, train acc: 0.9762\n",
      "epoch: 7, loss: 0.0637, train acc: 0.9752\n",
      "epoch: 7, loss: 0.0633, train acc: 0.9760\n",
      "epoch: 7, loss: 0.0669, train acc: 0.9744\n",
      "epoch: 7, loss: 0.0718, train acc: 0.9736\n",
      "epoch: 7, loss: 0.0688, train acc: 0.9751\n",
      "epoch: 7, loss: 0.0691, train acc: 0.9751\n",
      "epoch: 7, loss: 0.0688, train acc: 0.9751\n",
      "epoch: 7, loss: 0.0677, train acc: 0.9751\n",
      ">epoch: 7, val_acc: 0.8936, val_f1: 0.8123\n",
      ">> epoch: 7, test_acc: 0.9062, test_f1: 0.7426\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 8\n",
      "epoch: 8, loss: 0.0351, train acc: 0.9688\n",
      "epoch: 8, loss: 0.0244, train acc: 0.9821\n",
      "epoch: 8, loss: 0.0267, train acc: 0.9844\n",
      "epoch: 8, loss: 0.0280, train acc: 0.9890\n",
      "epoch: 8, loss: 0.0383, train acc: 0.9858\n",
      "epoch: 8, loss: 0.0480, train acc: 0.9815\n",
      "epoch: 8, loss: 0.0525, train acc: 0.9785\n",
      "epoch: 8, loss: 0.0524, train acc: 0.9780\n",
      "epoch: 8, loss: 0.0573, train acc: 0.9747\n",
      "epoch: 8, loss: 0.0528, train acc: 0.9774\n",
      "epoch: 8, loss: 0.0520, train acc: 0.9772\n",
      "epoch: 8, loss: 0.0643, train acc: 0.9759\n",
      "epoch: 8, loss: 0.0672, train acc: 0.9748\n",
      "epoch: 8, loss: 0.0703, train acc: 0.9729\n",
      "epoch: 8, loss: 0.0705, train acc: 0.9731\n",
      "epoch: 8, loss: 0.0740, train acc: 0.9724\n",
      "epoch: 8, loss: 0.0757, train acc: 0.9726\n",
      "epoch: 8, loss: 0.0763, train acc: 0.9720\n",
      "epoch: 8, loss: 0.0749, train acc: 0.9721\n",
      "epoch: 8, loss: 0.0726, train acc: 0.9736\n",
      "epoch: 8, loss: 0.0726, train acc: 0.9724\n",
      ">epoch: 8, val_acc: 0.8777, val_f1: 0.8094\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 9\n",
      "epoch: 9, loss: 0.0039, train acc: 1.0000\n",
      "epoch: 9, loss: 0.0051, train acc: 1.0000\n",
      "epoch: 9, loss: 0.0069, train acc: 1.0000\n",
      "epoch: 9, loss: 0.0187, train acc: 0.9922\n",
      "epoch: 9, loss: 0.0192, train acc: 0.9911\n",
      "epoch: 9, loss: 0.0198, train acc: 0.9904\n",
      "epoch: 9, loss: 0.0180, train acc: 0.9919\n",
      "epoch: 9, loss: 0.0219, train acc: 0.9896\n",
      "epoch: 9, loss: 0.0372, train acc: 0.9863\n",
      "epoch: 9, loss: 0.0526, train acc: 0.9810\n",
      "epoch: 9, loss: 0.0562, train acc: 0.9779\n",
      "epoch: 9, loss: 0.0564, train acc: 0.9777\n",
      "epoch: 9, loss: 0.0568, train acc: 0.9785\n",
      "epoch: 9, loss: 0.0539, train acc: 0.9801\n",
      "epoch: 9, loss: 0.0591, train acc: 0.9789\n",
      "epoch: 9, loss: 0.0598, train acc: 0.9778\n",
      "epoch: 9, loss: 0.0642, train acc: 0.9761\n",
      "epoch: 9, loss: 0.0697, train acc: 0.9746\n",
      "epoch: 9, loss: 0.0747, train acc: 0.9746\n",
      "epoch: 9, loss: 0.0769, train acc: 0.9733\n",
      "epoch: 9, loss: 0.0765, train acc: 0.9734\n",
      "epoch: 9, loss: 0.0780, train acc: 0.9722\n",
      ">epoch: 9, val_acc: 0.8723, val_f1: 0.7730\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 10\n",
      "epoch: 10, loss: 0.0608, train acc: 0.9750\n",
      "epoch: 10, loss: 0.0631, train acc: 0.9750\n",
      "epoch: 10, loss: 0.0566, train acc: 0.9750\n",
      "epoch: 10, loss: 0.0515, train acc: 0.9812\n",
      "epoch: 10, loss: 0.0544, train acc: 0.9775\n",
      "epoch: 10, loss: 0.0476, train acc: 0.9812\n",
      "epoch: 10, loss: 0.0458, train acc: 0.9804\n",
      "epoch: 10, loss: 0.0587, train acc: 0.9781\n",
      "epoch: 10, loss: 0.0562, train acc: 0.9778\n",
      "epoch: 10, loss: 0.0528, train acc: 0.9788\n",
      "epoch: 10, loss: 0.0540, train acc: 0.9784\n",
      "epoch: 10, loss: 0.0527, train acc: 0.9792\n",
      "epoch: 10, loss: 0.0596, train acc: 0.9769\n",
      "epoch: 10, loss: 0.0660, train acc: 0.9759\n",
      "epoch: 10, loss: 0.0668, train acc: 0.9767\n",
      "epoch: 10, loss: 0.0642, train acc: 0.9781\n",
      "epoch: 10, loss: 0.0631, train acc: 0.9779\n",
      "epoch: 10, loss: 0.0644, train acc: 0.9778\n",
      "epoch: 10, loss: 0.0637, train acc: 0.9776\n",
      "epoch: 10, loss: 0.0656, train acc: 0.9762\n",
      "epoch: 10, loss: 0.0652, train acc: 0.9768\n",
      ">epoch: 10, val_acc: 0.8723, val_f1: 0.7864\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 11\n",
      "epoch: 11, loss: 0.0151, train acc: 1.0000\n",
      "epoch: 11, loss: 0.0513, train acc: 0.9722\n",
      "epoch: 11, loss: 0.0356, train acc: 0.9821\n",
      "epoch: 11, loss: 0.0359, train acc: 0.9836\n",
      "epoch: 11, loss: 0.0326, train acc: 0.9870\n",
      "epoch: 11, loss: 0.0301, train acc: 0.9871\n",
      "epoch: 11, loss: 0.0284, train acc: 0.9871\n",
      "epoch: 11, loss: 0.0370, train acc: 0.9856\n",
      "epoch: 11, loss: 0.0373, train acc: 0.9858\n",
      "epoch: 11, loss: 0.0404, train acc: 0.9847\n",
      "epoch: 11, loss: 0.0401, train acc: 0.9838\n",
      "epoch: 11, loss: 0.0382, train acc: 0.9841\n",
      "epoch: 11, loss: 0.0389, train acc: 0.9844\n",
      "epoch: 11, loss: 0.0431, train acc: 0.9837\n",
      "epoch: 11, loss: 0.0456, train acc: 0.9831\n",
      "epoch: 11, loss: 0.0473, train acc: 0.9818\n",
      "epoch: 11, loss: 0.0479, train acc: 0.9821\n",
      "epoch: 11, loss: 0.0476, train acc: 0.9824\n",
      "epoch: 11, loss: 0.0489, train acc: 0.9820\n",
      "epoch: 11, loss: 0.0482, train acc: 0.9823\n",
      "epoch: 11, loss: 0.0490, train acc: 0.9820\n",
      ">epoch: 11, val_acc: 0.8457, val_f1: 0.7433\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 12\n",
      "epoch: 12, loss: 0.0469, train acc: 0.9792\n",
      "epoch: 12, loss: 0.0401, train acc: 0.9844\n",
      "epoch: 12, loss: 0.0301, train acc: 0.9904\n",
      "epoch: 12, loss: 0.0581, train acc: 0.9861\n",
      "epoch: 12, loss: 0.0524, train acc: 0.9864\n",
      "epoch: 12, loss: 0.0479, train acc: 0.9866\n",
      "epoch: 12, loss: 0.0542, train acc: 0.9830\n",
      "epoch: 12, loss: 0.0524, train acc: 0.9819\n",
      "epoch: 12, loss: 0.0565, train acc: 0.9811\n",
      "epoch: 12, loss: 0.0595, train acc: 0.9792\n",
      "epoch: 12, loss: 0.0582, train acc: 0.9800\n",
      "epoch: 12, loss: 0.0556, train acc: 0.9806\n",
      "epoch: 12, loss: 0.0548, train acc: 0.9812\n",
      "epoch: 12, loss: 0.0531, train acc: 0.9816\n",
      "epoch: 12, loss: 0.0531, train acc: 0.9812\n",
      "epoch: 12, loss: 0.0548, train acc: 0.9800\n",
      "epoch: 12, loss: 0.0550, train acc: 0.9797\n",
      "epoch: 12, loss: 0.0540, train acc: 0.9801\n",
      "epoch: 12, loss: 0.0525, train acc: 0.9805\n",
      "epoch: 12, loss: 0.0519, train acc: 0.9809\n",
      "epoch: 12, loss: 0.0534, train acc: 0.9800\n",
      ">epoch: 12, val_acc: 0.8617, val_f1: 0.7568\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 13\n",
      "epoch: 13, loss: 0.0008, train acc: 1.0000\n",
      "epoch: 13, loss: 0.0400, train acc: 0.9911\n",
      "epoch: 13, loss: 0.0371, train acc: 0.9844\n",
      "epoch: 13, loss: 0.0342, train acc: 0.9853\n",
      "epoch: 13, loss: 0.0329, train acc: 0.9830\n",
      "epoch: 13, loss: 0.0316, train acc: 0.9861\n",
      "epoch: 13, loss: 0.0365, train acc: 0.9824\n",
      "epoch: 13, loss: 0.0383, train acc: 0.9797\n",
      "epoch: 13, loss: 0.0358, train acc: 0.9807\n",
      "epoch: 13, loss: 0.0334, train acc: 0.9814\n",
      "epoch: 13, loss: 0.0305, train acc: 0.9832\n",
      "epoch: 13, loss: 0.0321, train acc: 0.9825\n",
      "epoch: 13, loss: 0.0333, train acc: 0.9829\n",
      "epoch: 13, loss: 0.0318, train acc: 0.9832\n",
      "epoch: 13, loss: 0.0360, train acc: 0.9809\n",
      "epoch: 13, loss: 0.0389, train acc: 0.9797\n",
      "epoch: 13, loss: 0.0399, train acc: 0.9794\n",
      "epoch: 13, loss: 0.0407, train acc: 0.9792\n",
      "epoch: 13, loss: 0.0413, train acc: 0.9783\n",
      "epoch: 13, loss: 0.0402, train acc: 0.9794\n",
      "epoch: 13, loss: 0.0391, train acc: 0.9804\n",
      ">epoch: 13, val_acc: 0.8617, val_f1: 0.7665\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 14\n",
      "epoch: 14, loss: 0.0003, train acc: 1.0000\n",
      "epoch: 14, loss: 0.0181, train acc: 0.9896\n",
      "epoch: 14, loss: 0.0457, train acc: 0.9773\n",
      "epoch: 14, loss: 0.0412, train acc: 0.9766\n",
      "epoch: 14, loss: 0.0345, train acc: 0.9821\n",
      "epoch: 14, loss: 0.0375, train acc: 0.9808\n",
      "epoch: 14, loss: 0.0325, train acc: 0.9839\n",
      "epoch: 14, loss: 0.0352, train acc: 0.9809\n",
      "epoch: 14, loss: 0.0452, train acc: 0.9771\n",
      "epoch: 14, loss: 0.0457, train acc: 0.9783\n",
      "epoch: 14, loss: 0.0446, train acc: 0.9779\n",
      "epoch: 14, loss: 0.0450, train acc: 0.9777\n",
      "epoch: 14, loss: 0.0426, train acc: 0.9795\n",
      "epoch: 14, loss: 0.0437, train acc: 0.9792\n",
      "epoch: 14, loss: 0.0433, train acc: 0.9798\n",
      "epoch: 14, loss: 0.0454, train acc: 0.9778\n",
      "epoch: 14, loss: 0.0442, train acc: 0.9784\n",
      "epoch: 14, loss: 0.0502, train acc: 0.9782\n",
      "epoch: 14, loss: 0.0491, train acc: 0.9787\n",
      "epoch: 14, loss: 0.0476, train acc: 0.9798\n",
      "epoch: 14, loss: 0.0476, train acc: 0.9802\n",
      "epoch: 14, loss: 0.0471, train acc: 0.9805\n",
      ">epoch: 14, val_acc: 0.8777, val_f1: 0.8249\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 15\n",
      "epoch: 15, loss: 0.0279, train acc: 0.9750\n",
      "epoch: 15, loss: 0.0170, train acc: 0.9875\n",
      "epoch: 15, loss: 0.0269, train acc: 0.9833\n",
      "epoch: 15, loss: 0.0268, train acc: 0.9844\n",
      "epoch: 15, loss: 0.0289, train acc: 0.9850\n",
      "epoch: 15, loss: 0.0293, train acc: 0.9854\n",
      "epoch: 15, loss: 0.0347, train acc: 0.9821\n",
      "epoch: 15, loss: 0.0356, train acc: 0.9812\n",
      "epoch: 15, loss: 0.0343, train acc: 0.9819\n",
      "epoch: 15, loss: 0.0334, train acc: 0.9825\n",
      "epoch: 15, loss: 0.0407, train acc: 0.9807\n",
      "epoch: 15, loss: 0.0422, train acc: 0.9802\n",
      "epoch: 15, loss: 0.0485, train acc: 0.9788\n",
      "epoch: 15, loss: 0.0469, train acc: 0.9795\n",
      "epoch: 15, loss: 0.0444, train acc: 0.9808\n",
      "epoch: 15, loss: 0.0468, train acc: 0.9797\n",
      "epoch: 15, loss: 0.0448, train acc: 0.9809\n",
      "epoch: 15, loss: 0.0443, train acc: 0.9812\n",
      "epoch: 15, loss: 0.0464, train acc: 0.9796\n",
      "epoch: 15, loss: 0.0457, train acc: 0.9800\n",
      "epoch: 15, loss: 0.0467, train acc: 0.9798\n",
      ">epoch: 15, val_acc: 0.8511, val_f1: 0.7739\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 16\n",
      "epoch: 16, loss: 0.0438, train acc: 0.9844\n",
      "epoch: 16, loss: 0.0437, train acc: 0.9722\n",
      "epoch: 16, loss: 0.0363, train acc: 0.9777\n",
      "epoch: 16, loss: 0.0398, train acc: 0.9770\n",
      "epoch: 16, loss: 0.0327, train acc: 0.9818\n",
      "epoch: 16, loss: 0.0527, train acc: 0.9784\n",
      "epoch: 16, loss: 0.0466, train acc: 0.9816\n",
      "epoch: 16, loss: 0.0452, train acc: 0.9808\n",
      "epoch: 16, loss: 0.0426, train acc: 0.9815\n",
      "epoch: 16, loss: 0.0419, train acc: 0.9821\n",
      "epoch: 16, loss: 0.0393, train acc: 0.9826\n",
      "epoch: 16, loss: 0.0393, train acc: 0.9831\n",
      "epoch: 16, loss: 0.0388, train acc: 0.9834\n",
      "epoch: 16, loss: 0.0404, train acc: 0.9819\n",
      "epoch: 16, loss: 0.0387, train acc: 0.9823\n",
      "epoch: 16, loss: 0.0446, train acc: 0.9802\n",
      "epoch: 16, loss: 0.0422, train acc: 0.9814\n",
      "epoch: 16, loss: 0.0431, train acc: 0.9817\n",
      "epoch: 16, loss: 0.0419, train acc: 0.9820\n",
      "epoch: 16, loss: 0.0419, train acc: 0.9823\n",
      "epoch: 16, loss: 0.0423, train acc: 0.9826\n",
      ">epoch: 16, val_acc: 0.8777, val_f1: 0.7762\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 17\n",
      "epoch: 17, loss: 0.0371, train acc: 0.9792\n",
      "epoch: 17, loss: 0.0379, train acc: 0.9766\n",
      "epoch: 17, loss: 0.0340, train acc: 0.9808\n",
      "epoch: 17, loss: 0.0311, train acc: 0.9826\n",
      "epoch: 17, loss: 0.0432, train acc: 0.9783\n",
      "epoch: 17, loss: 0.0449, train acc: 0.9754\n",
      "epoch: 17, loss: 0.0444, train acc: 0.9754\n",
      "epoch: 17, loss: 0.0456, train acc: 0.9737\n",
      "epoch: 17, loss: 0.0483, train acc: 0.9738\n",
      "epoch: 17, loss: 0.0450, train acc: 0.9766\n",
      "epoch: 17, loss: 0.0421, train acc: 0.9776\n",
      "epoch: 17, loss: 0.0413, train acc: 0.9784\n",
      "epoch: 17, loss: 0.0401, train acc: 0.9792\n",
      "epoch: 17, loss: 0.0401, train acc: 0.9798\n",
      "epoch: 17, loss: 0.0375, train acc: 0.9812\n",
      "epoch: 17, loss: 0.0389, train acc: 0.9816\n",
      "epoch: 17, loss: 0.0377, train acc: 0.9819\n",
      "epoch: 17, loss: 0.0392, train acc: 0.9808\n",
      "epoch: 17, loss: 0.0376, train acc: 0.9819\n",
      "epoch: 17, loss: 0.0375, train acc: 0.9815\n",
      "epoch: 17, loss: 0.0392, train acc: 0.9806\n",
      ">epoch: 17, val_acc: 0.8564, val_f1: 0.7853\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 18\n",
      "epoch: 18, loss: 0.0154, train acc: 1.0000\n",
      "epoch: 18, loss: 0.0311, train acc: 0.9821\n",
      "epoch: 18, loss: 0.0336, train acc: 0.9792\n",
      "epoch: 18, loss: 0.0251, train acc: 0.9853\n",
      "epoch: 18, loss: 0.0213, train acc: 0.9886\n",
      "epoch: 18, loss: 0.0210, train acc: 0.9884\n",
      "epoch: 18, loss: 0.0268, train acc: 0.9863\n",
      "epoch: 18, loss: 0.0232, train acc: 0.9882\n",
      "epoch: 18, loss: 0.0559, train acc: 0.9792\n",
      "epoch: 18, loss: 0.0568, train acc: 0.9761\n",
      "epoch: 18, loss: 0.0623, train acc: 0.9760\n",
      "epoch: 18, loss: 0.0635, train acc: 0.9748\n",
      "epoch: 18, loss: 0.0673, train acc: 0.9738\n",
      "epoch: 18, loss: 0.0666, train acc: 0.9748\n",
      "epoch: 18, loss: 0.0743, train acc: 0.9722\n",
      "epoch: 18, loss: 0.0769, train acc: 0.9716\n",
      "epoch: 18, loss: 0.0815, train acc: 0.9688\n",
      "epoch: 18, loss: 0.0790, train acc: 0.9698\n",
      "epoch: 18, loss: 0.0791, train acc: 0.9701\n",
      "epoch: 18, loss: 0.0830, train acc: 0.9684\n",
      "epoch: 18, loss: 0.0858, train acc: 0.9669\n",
      ">epoch: 18, val_acc: 0.8564, val_f1: 0.7277\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 19\n",
      "epoch: 19, loss: 0.0009, train acc: 1.0000\n",
      "epoch: 19, loss: 0.0186, train acc: 1.0000\n",
      "epoch: 19, loss: 0.0682, train acc: 0.9716\n",
      "epoch: 19, loss: 0.0772, train acc: 0.9688\n",
      "epoch: 19, loss: 0.0915, train acc: 0.9702\n",
      "epoch: 19, loss: 0.1027, train acc: 0.9615\n",
      "epoch: 19, loss: 0.0983, train acc: 0.9637\n",
      "epoch: 19, loss: 0.0909, train acc: 0.9670\n",
      "epoch: 19, loss: 0.0867, train acc: 0.9665\n",
      "epoch: 19, loss: 0.0840, train acc: 0.9674\n",
      "epoch: 19, loss: 0.0916, train acc: 0.9657\n",
      "epoch: 19, loss: 0.0872, train acc: 0.9654\n",
      "epoch: 19, loss: 0.0958, train acc: 0.9621\n",
      "epoch: 19, loss: 0.0922, train acc: 0.9640\n",
      "epoch: 19, loss: 0.0939, train acc: 0.9648\n",
      "epoch: 19, loss: 0.0974, train acc: 0.9646\n",
      "epoch: 19, loss: 0.0968, train acc: 0.9645\n",
      "epoch: 19, loss: 0.0952, train acc: 0.9644\n",
      "epoch: 19, loss: 0.0913, train acc: 0.9663\n",
      "epoch: 19, loss: 0.0888, train acc: 0.9674\n",
      "epoch: 19, loss: 0.0857, train acc: 0.9684\n",
      "epoch: 19, loss: 0.0836, train acc: 0.9693\n",
      ">epoch: 19, val_acc: 0.8830, val_f1: 0.8172\n",
      "Namespace(model_type='tri_gcn', modules={'tgcn': True, 'semgcn': True, 'lexgcn': True}, num_layers={'tgcn': 2, 'semgcn': 2, 'lexgcn': 2}, year='2016', train_file='data/train2016restaurant.txt', test_file='data/test2016restaurant.txt', model_path='test_models/2016tri_gcn+concat_seed217_reg0.0005_drop0.4_cdrop0.4_lr2.5e-05_epochs20_adam', val_file='val.txt', log='log', bert_model='bert_large_uncased', cooc_path='cooc_matrix.csv', cooc=       100       1000   10000  10003     10005     10007     10009     1001   \\\n",
      "100      0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "1000     0.0  0.000000    0.0    0.0  0.000606  0.000303  0.000606  0.035120   \n",
      "10000    0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.333333   \n",
      "10003    0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "10005    0.0  0.333333    0.0    0.0  0.000000  0.166667  0.000000  0.166667   \n",
      "...      ...       ...    ...    ...       ...       ...       ...       ...   \n",
      "9992     0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "9993     0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "9996     0.0  0.500000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "9997     0.0  0.000000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "9998     0.0  0.400000    0.0    0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "       10010  10015  ...     9986      9987      999    9990      9991   \\\n",
      "100      0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "1000     0.0    0.0  ...  0.002422  0.000303  0.062368    0.0  0.000606   \n",
      "10000    0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "10003    0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "10005    0.0    0.0  ...  0.000000  0.000000  0.333333    0.0  0.000000   \n",
      "...      ...    ...  ...       ...       ...       ...    ...       ...   \n",
      "9992     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "9993     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "9996     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "9997     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "9998     0.0    0.0  ...  0.000000  0.000000  0.000000    0.0  0.000000   \n",
      "\n",
      "       9992   9993      9996   9997      9998   \n",
      "100      0.0    0.0  0.000000    0.0  0.000000  \n",
      "1000     0.0    0.0  0.000606    0.0  0.001211  \n",
      "10000    0.0    0.0  0.000000    0.0  0.000000  \n",
      "10003    0.0    0.0  0.000000    0.0  0.000000  \n",
      "10005    0.0    0.0  0.000000    0.0  0.000000  \n",
      "...      ...    ...       ...    ...       ...  \n",
      "9992     0.0    0.0  0.000000    0.0  0.000000  \n",
      "9993     0.0    0.0  0.000000    0.0  0.000000  \n",
      "9996     0.0    0.0  0.000000    0.0  0.000000  \n",
      "9997     0.0    0.0  0.000000    0.0  0.000000  \n",
      "9998     0.0    0.0  0.000000    0.0  0.000000  \n",
      "\n",
      "[13075 rows x 13075 columns], learning_rate=2.5e-05, dropout=0.4, concat_dropout=0.4, bert_dropout=0.2, l2reg=0.0005, num_epoch=20, batch_size=16, log_step=5, max_seq_len=100, polarities_dim=3, device=device(type='cuda'), seed=217, valset_ratio=0.1, do_train=True, do_eval=True, eval_epoch_num=0, fusion_type='concat', use_ensemble=True, save_models='none', print_sent=False, optim='adam', n_gpu=1)\n",
      "{'none': 0, 'acl': 1, 'acl:relcl': 2, 'advcl': 3, 'advcl:relcl': 4, 'advmod': 5, 'amod': 6, 'appos': 7, 'aux': 8, 'aux:pass': 9, 'case': 10, 'cc': 11, 'cc:preconj': 12, 'ccomp': 13, 'compound': 14, 'compound:prt': 15, 'conj': 16, 'cop': 17, 'csubj': 18, 'dep': 19, 'det': 20, 'det:predet': 21, 'discourse': 22, 'dislocated': 23, 'expl': 24, 'fixed': 25, 'flat': 26, 'iobj': 27, 'list': 28, 'mark': 29, 'nmod': 30, 'nmod:npmod': 31, 'nmod:poss': 32, 'nmod:tmod': 33, 'nsubj': 34, 'nsubj:outer': 35, 'nsubj:pass': 36, 'nummod': 37, 'obj': 38, 'obl': 39, 'obl:agent': 40, 'obl:npmod': 41, 'obl:tmod': 42, 'parataxis': 43, 'punct': 44, 'reparandum': 45, 'root': 46, 'vocative': 47, 'xcomp': 48}\n",
      "{'-1': 0, '0': 1, '1': 2}\n",
      "Model name 'bert_large_uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming 'bert_large_uncased' is a path or url to a directory containing tokenizer files.\n",
      "Didn't find file bert_large_uncased/added_tokens.json. We won't load it.\n",
      "Didn't find file bert_large_uncased/special_tokens_map.json. We won't load it.\n",
      "Didn't find file bert_large_uncased/tokenizer_config.json. We won't load it.\n",
      "loading file bert_large_uncased/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "{\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 3,\n",
      "  \"num_types\": 49,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file bert_large_uncased/pytorch_model.bin\n",
      "Weights of AsaTgcnSem not initialized from pretrained model: ['gate_weight', 'gate_bias', 'ensemble', 'TGCNLayers.0.weight', 'TGCNLayers.0.bias', 'TGCNLayers.0.dense.weight', 'TGCNLayers.1.weight', 'TGCNLayers.1.bias', 'TGCNLayers.1.dense.weight', 'SemGCNLayers.0.weight', 'SemGCNLayers.0.bias', 'SemGCNLayers.1.weight', 'SemGCNLayers.1.bias', 'LexGCNLayers.0.weight', 'LexGCNLayers.0.bias', 'LexGCNLayers.1.weight', 'LexGCNLayers.1.bias', 'fc_single.weight', 'fc_single.bias', 'ensemble_linear_tgcn.weight', 'ensemble_linear_tgcn.bias', 'ensemble_linear_semgcn.weight', 'ensemble_linear_semgcn.bias', 'ensemble_linear_lexgcn.weight', 'ensemble_linear_lexgcn.bias', 'dep_embedding.weight']\n",
      "Weights from pretrained model not used in AsaTgcnSem: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "[{'input_ids': tensor([  101, 13325,  2013,  3025,  8466,  2023,  2109,  2000,  2022,  1037,\n",
      "         2204,  2173,  1010,  2021,  2025,  2151,  2936,  1012,   102,  2173,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'valid_ids': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'segment_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'mem_valid_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'dep_adj_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'dep_value_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'polarity': 0, 'raw_text': 'judging from previous posts this used to be a good place , but not any longer .', 'aspect': 'place'}]\n",
      "[{'input_ids': tensor([  101, 13325,  2013,  3025,  8466,  2023,  2109,  2000,  2022,  1037,\n",
      "         2204,  2173,  1010,  2021,  2025,  2151,  2936,  1012,   102,  2173,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'valid_ids': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'segment_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'mem_valid_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'dep_adj_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'dep_value_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'polarity': 0, 'raw_text': 'judging from previous posts this used to be a good place , but not any longer .', 'aspect': 'place'}]\n",
      "[{'input_ids': tensor([  101,  4240,  2428,  2204, 10514,  6182,  1012,   102, 10514,  6182,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'valid_ids': tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'segment_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'mem_valid_ids': tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'dep_adj_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'dep_value_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 5,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'polarity': 2, 'raw_text': 'serves really good sushi .', 'aspect': 'sushi'}]\n",
      "cuda memory allocated: 1400893440\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 0\n",
      "epoch: 0, loss: 0.9474, train acc: 0.5625\n",
      "epoch: 0, loss: 0.8249, train acc: 0.6375\n",
      "epoch: 0, loss: 0.8673, train acc: 0.6375\n",
      "epoch: 0, loss: 0.8622, train acc: 0.6531\n",
      "epoch: 0, loss: 0.8559, train acc: 0.6450\n",
      "epoch: 0, loss: 0.8374, train acc: 0.6521\n",
      "epoch: 0, loss: 0.8180, train acc: 0.6589\n",
      "epoch: 0, loss: 0.8372, train acc: 0.6531\n",
      "epoch: 0, loss: 0.8413, train acc: 0.6514\n",
      "epoch: 0, loss: 0.8426, train acc: 0.6500\n",
      "epoch: 0, loss: 0.8386, train acc: 0.6568\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "csv_file = 'final_results.csv'\n",
    "\n",
    "\n",
    "for i in range(20):\n",
    "    lr = 25e-6\n",
    "    d = 0.4\n",
    "    cdrop = 0.4\n",
    "    w_decay = 5e-4\n",
    "    seed = np.random.randint(1000)\n",
    "    year = '2016'\n",
    "    opt = get_args(batch_size = 16, seed = seed, dropout = d,\n",
    "                  l2reg = w_decay, learning_rate = lr, year = year,\n",
    "                  num_epoch = 20, model_type = 'tri_gcn', save_models = 'none', fusion_type = 'concat',\n",
    "                  concat_dropout = cdrop, cooc = cooc, tgcn = True, semgcn = True, lexgcn = True, use_ensemble = True,\n",
    "                  tgcn_layers = 2, semgcn_layers = 2, lexgcn_layers = 2, optim = 'adam')\n",
    "    opt.device = torch.device('cuda')\n",
    "    max_val_acc, test_acc, test_f1 = main(opt)\n",
    "\n",
    "    with open(csv_file, 'a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow((year, max_val_acc, test_acc, test_f1, seed, lr, d, cdrop, w_decay))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbce8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def log_sampler(min_lr, max_lr, num_samples):\n",
    "    # Generate random values from a logarithmic scale\n",
    "    log_min_lr = np.log10(min_lr)\n",
    "    log_max_lr = np.log10(max_lr)\n",
    "    log_lr_samples = np.random.uniform(log_min_lr, log_max_lr, num_samples)\n",
    "\n",
    "    # Convert back to linear scale\n",
    "    lr_samples = np.power(10, log_lr_samples)\n",
    "\n",
    "    return lr_samples\n",
    "\n",
    "num_trials = 100\n",
    "\n",
    "lr_space = np.linspace(5e-6, 2e-5, num = 50)\n",
    "cd_space = np.linspace(0, 0.6, 14)\n",
    "d_space = np.linspace(0, 0.4, 8)\n",
    "reg_space = np.logspace(-2.5, -1.2, num=100)\n",
    "for i in range(num_trials):\n",
    "    c_d = np.random.choice(cd_space)\n",
    "    d = np.random.choice(d_space)\n",
    "    reg = np.random.choice(reg_space)\n",
    "    lr = np.random.choice(lr_space)\n",
    "    fusion_type = 'concat'\n",
    "    opt = get_args(batch_size = 16, seed = np.random.randint(1000), dropout = d,\n",
    "              l2reg = reg, learning_rate = lr, year = '2015',\n",
    "              num_epoch = 25, model_type = 'tgcn+sem', save_models = 'none', fusion_type = fusion_type,\n",
    "              concat_dropout = d, tgcn = True, semgcn = True, lexgcn = False)\n",
    "    main(opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43901d7a",
   "metadata": {},
   "source": [
    "# Exploration/testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36be75ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = get_args(batch_size = 16, seed = 10, dropout = 0.2, \n",
    "              l2reg = 0.01, learning_rate = 2e-5, year = '2015',\n",
    "              num_epoch = 30, model_type = 'tgcn+sem', save_models = 'none', cooc = cooc)\n",
    "opt.device = torch.device('cuda')\n",
    "Ins = Instructor(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b977898",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddc389e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(dataset=Ins.trainset, batch_size=Ins.opt.batch_size, shuffle=True)\n",
    "batch_0 = list(train_data_loader)[0]\n",
    "# print(batch_0)\n",
    "input_ids = batch_0['input_ids'].to('cuda')\n",
    "segment_ids = batch_0['segment_ids'].to('cuda')\n",
    "valid_ids = batch_0['valid_ids'].to('cuda')\n",
    "raw_text = batch_0['raw_text']\n",
    "\n",
    "sequence_output, pooled_output = Ins.model.bert(input_ids, segment_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d221c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_example = Ins.model.get_lex_adj(input_ids, 16, 100)\n",
    "print(adj_example[0,:15,:15])\n",
    "\n",
    "print('datatype: ', Ins.model.cooc.dtypes)\n",
    "\n",
    "print(2833 in Ins.model.cooc)\n",
    "Ins.model.cooc[2833][2833]\n",
    "\n",
    "# for i in input_ids[0]:\n",
    "#     for j in input_ids[0]:\n",
    "#         print(int(i), int(j), str(i.item()) in Ins.model.cooc, str(j.item()) in Ins.model.cooc)\n",
    "        \n",
    "\n",
    "# Ins.model.cooc[float(2833)][np.float(2833)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78267890",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch_0['segment_ids'][0])\n",
    "print(batch_0['valid_ids'][0])\n",
    "print(batch_0['mem_valid_ids'][0])\n",
    "print(batch_0['input_ids'][0])\n",
    "print(batch_0['raw_text'][0])\n",
    "\n",
    "def non_zero(tensor):\n",
    "    # Find the index of the last non-zero element\n",
    "    last_nonzero_index = torch.nonzero(tensor).max().item()\n",
    "\n",
    "    # Strip the tensor\n",
    "    stripped_tensor = tensor[:last_nonzero_index + 1]\n",
    "    \n",
    "    return stripped_tensor\n",
    "\n",
    "print(non_zero(batch_0['valid_ids'][0]))\n",
    "print(non_zero(batch_0['input_ids'][0]))\n",
    "\n",
    "for valid, inp in zip(batch_0['valid_ids'][0], batch_0['input_ids'][0]):\n",
    "    if valid == 0 and inp != 0:\n",
    "        print(inp)\n",
    "\n",
    "Ins.tokenizer.tokenizer.tokenize(batch_0['raw_text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a576f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, max_len, feat_dim = sequence_output.shape\n",
    "print(f\"Batch size: {batch_size}\\nMax length: {max_len}\\nNumber of features: {feat_dim}\")\n",
    "valid_output = torch.zeros(batch_size, max_len, feat_dim, device=input_ids.device).type_as(sequence_output)\n",
    "for i in range(batch_size):\n",
    "    temp = sequence_output[i][valid_ids[i] == 1]\n",
    "    valid_output[i][:temp.size(0)] = temp\n",
    "valid_output = Ins.model.dropout(valid_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b706db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(valid_output[0,:,0])\n",
    "print(valid_ids[0])\n",
    "print(raw_text[0])\n",
    "\n",
    "print(len(torch.nonzero(input_ids[0])))\n",
    "print(len(raw_text[0].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40403869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "input_ids = batch_0['input_ids']\n",
    "print(input_ids[0])\n",
    "print(valid_ids[0])\n",
    "print(batch_0['raw_text'][0])\n",
    "id_map = Ins.model.id_to_index_map\n",
    "print('Index of word \"do\": ', id_map[2079])\n",
    "\n",
    "cooc_matrix = Ins.model.cooc_matrix\n",
    "id_to_index_map = Ins.model.id_to_index_map\n",
    "\n",
    "adj = lex_test(input_ids, 16, 100, id_to_index_map, cooc_matrix)\n",
    "\n",
    "sentence = batch_0['raw_text'][0]\n",
    "\n",
    "print(f'Sentence has {len(sentence.split())} words')\n",
    "\n",
    "print(adj[0,:23,:23])\n",
    "\n",
    "for i, j in list(itertools.product(sentence.split(), sentence.split())):\n",
    "    if i in cooc and j in cooc:\n",
    "        print(f'{i}, {j}: {cooc[i][j]}')\n",
    "    elif not i in cooc:\n",
    "        print(f'{i} not in cooc matrix.')\n",
    "    else:\n",
    "        print(f'{j} not in cooc matrix.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea0064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lex_test(input_ids, batch_size, max_len, id_to_index_map, cooc_matrix):\n",
    "        # Initialize an empty adjacency tensor\n",
    "        adj_tensor = torch.zeros((batch_size, max_len, max_len))\n",
    "        \n",
    "        for i, id_sequence in enumerate(input_ids):\n",
    "            # Get word list\n",
    "            num_words = int(torch.sum(id_sequence != 0))\n",
    "\n",
    "            word_indices = []\n",
    "            \n",
    "            for word_id in id_sequence:\n",
    "                word_id_int = int(word_id) # conver from torch.tensor to int\n",
    "                index = id_to_index_map[word_id_int] if word_id_int in id_to_index_map else -1\n",
    "                word_indices.append(index)\n",
    "#             print('word indices: ', word_indices)\n",
    "#             print('num words: ', num_words)\n",
    "#             print('input ids: ', input_ids)\n",
    "            \n",
    "            if i==0:\n",
    "                print('\\nLex function test prints.')\n",
    "                print('id sequence for sentence 0:', id_sequence)\n",
    "                print(f'Num. words in lex function: {num_words}\\n')\n",
    "            \n",
    "            for j in range(num_words):\n",
    "                for k in range(num_words):\n",
    "                    if j != k:\n",
    "                        adj_tensor[i, j, k] = cooc_matrix[word_indices[j]][word_indices[k]]\n",
    "                    else:\n",
    "                        adj_tensor[i, j, k] = adj_tensor[i, j, k] / (2 * num_words)\n",
    "                \n",
    "        # Calculate the sums of rows for each matrix\n",
    "        row_sums = adj_tensor.sum(dim=2, keepdim=True).repeat(1, 1, max_len)\n",
    "\n",
    "        # Calculate the sums of columns for each matrix\n",
    "        column_sums = adj_tensor.sum(dim=1, keepdim=True).repeat(1, max_len, 1)\n",
    "\n",
    "        # Create a diagonal mask for each matrix\n",
    "        diagonal_mask = torch.eye(adj_tensor.size(-1)).bool().unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "\n",
    "        total_sum = row_sums + column_sums\n",
    "\n",
    "        # Set the diagonal entries to the sum of all the row and column entries (will be averaged later)\n",
    "        res = torch.where(diagonal_mask, total_sum, adj_tensor)\n",
    "        \n",
    "#         print('Res: ', res)\n",
    "        \n",
    "        adj_tensor = adj_tensor + res\n",
    "        \n",
    "        return adj_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaa9b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = batch_0['input_ids']\n",
    "print(input_ids)\n",
    "max_len = 100\n",
    "batch_size = 16\n",
    "\n",
    "adj = Ins.model.get_lex_adj(input_ids, batch_size, max_len)\n",
    "print(adj)\n",
    "\n",
    "id_map = Ins.model.id_to_index_map\n",
    "\n",
    "print(id_map[2320])\n",
    "\n",
    "print('Batch 0, sentence 0: ')\n",
    "for word_id in input_ids[0]:\n",
    "    print(int(word_id), int(word_id) in id_map)\n",
    "\n",
    "print(adj.shape)\n",
    "print(adj[0,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641d9d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Ins.tokenizer.tokenizer)\n",
    "id_map = {Ins.tokenizer.tokenizer.convert_tokens_to_ids(Ins.tokenizer.tokenizer.tokenize(w))[0]: i for i, w in enumerate(Ins.model.cooc.columns)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836f486e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(input_ids)\n",
    "for i, id_sequence in enumerate(input_ids):\n",
    "    print(id_sequence)\n",
    "    # Get word list\n",
    "    num_words = int(torch.sum(id_sequence != 0))\n",
    "    print(num_words)\n",
    "\n",
    "    word_indices = []\n",
    "\n",
    "    for word_id in id_sequence:\n",
    "        print(word_id)\n",
    "        word_id_int = int(word_id)\n",
    "        index = Ins.model.id_to_index_map[word_id_int] if word_id_int in Ins.model.id_to_index_map else -1\n",
    "        word_indices.append(index)\n",
    "    print('word indices: ', word_indices)\n",
    "    print('num words: ', num_words)\n",
    "    print('input ids: ', input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e78fb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current CUDA memory availability\n",
    "allocated_memory = torch.cuda.max_memory_allocated() / 1024**2  # Convert bytes to megabytes\n",
    "cached_memory = torch.cuda.max_memory_cached() / 1024**2  # Convert bytes to megabytes\n",
    "\n",
    "print(f\"Peak allocated memory: {allocated_memory:.2f} MB\")\n",
    "print(f\"Peak cached memory: {cached_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99865aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer4Bert(max_seq_len = 100, pretrained_bert_name = 'bert_large_uncased')\n",
    "print(tokenizer.tokenizer.convert_tokens_to_ids(tokenizer.tokenizer.tokenize(\"B\")))\n",
    "tokenizer.tokenizer.tokenize(\"i ate a good sammy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6569f9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer4Bert(max_seq_len = 100, pretrained_bert_name = 'bert_large_uncased')\n",
    "\n",
    "def ws_test(text):\n",
    "    tokens = []\n",
    "    valid_ids = []\n",
    "    for i, word in enumerate(text):\n",
    "        if len(text) <= 0:\n",
    "            continue\n",
    "        token = tokenizer.tokenizer.tokenize(word)\n",
    "        tokens.extend(token)\n",
    "        for m in range(len(token)):\n",
    "            if m == 0:\n",
    "                valid_ids.append(1)\n",
    "            else:\n",
    "                valid_ids.append(0)\n",
    "#     print(tokens)\n",
    "    token_ids = tokenizer.tokenizer.convert_tokens_to_ids(tokens)\n",
    "    return tokens, token_ids, valid_ids\n",
    "\n",
    "def create_feature_test(sentence, depinfo, print_sent = False):\n",
    "    text_left, text_right, aspect, polarity = sentence\n",
    "\n",
    "    cls_id = tokenizer.tokenizer.vocab[\"[CLS]\"]\n",
    "    sep_id = tokenizer.tokenizer.vocab[\"[SEP]\"]\n",
    "\n",
    "    doc = text_left + \" \" + aspect + \" \" + text_right\n",
    "\n",
    "    left_tokens, left_token_ids, left_valid_ids = ws_test(text_left.split(\" \"))\n",
    "    right_tokens, right_token_ids, right_valid_ids = ws_test(text_right.split(\" \"))\n",
    "    aspect_tokens, aspect_token_ids, aspect_valid_ids = ws_test(aspect.split(\" \"))\n",
    "    tokens = left_tokens + aspect_tokens + right_tokens\n",
    "    input_ids = [cls_id] + left_token_ids + aspect_token_ids + right_token_ids + [sep_id] + aspect_token_ids + [sep_id]\n",
    "    valid_ids = [1] + left_valid_ids + aspect_valid_ids + right_valid_ids + [1] + aspect_valid_ids + [1]\n",
    "    mem_valid_ids = [0] + [0] * len(left_tokens) + [1] * len(aspect_tokens) + [0] * len(right_tokens) # aspect terms mask\n",
    "    segment_ids = [0] * (len(tokens) + 2) + [1] * (len(aspect_tokens)+1)\n",
    "\n",
    "#     dep_instance_parser = DepInstanceParser(basicDependencies=depinfo, tokens=[])\n",
    "#     if self.dep_order == \"first\":\n",
    "#         dep_adj_matrix, dep_type_matrix = dep_instance_parser.get_first_order()\n",
    "#     elif self.dep_order == \"second\":\n",
    "#         dep_adj_matrix, dep_type_matrix = dep_instance_parser.get_second_order()\n",
    "#     elif self.dep_order == \"third\":\n",
    "#         dep_adj_matrix, dep_type_matrix = dep_instance_parser.get_third_order()\n",
    "#     else:\n",
    "#         raise ValueError()\n",
    "\n",
    "    token_head_list = []\n",
    "    for input_id, valid_id in zip(input_ids, valid_ids):\n",
    "        if input_id == cls_id:\n",
    "            continue\n",
    "        if input_id == sep_id:\n",
    "            break\n",
    "        if valid_id == 1:\n",
    "            token_head_list.append(input_id)\n",
    "         \n",
    "    print(input_ids)\n",
    "    print(token_head_list)\n",
    "    \n",
    "    input_ids = tokenizer.id_to_sequence(input_ids)\n",
    "    valid_ids = tokenizer.id_to_sequence(valid_ids)\n",
    "    segment_ids = tokenizer.id_to_sequence(segment_ids)\n",
    "    mem_valid_ids = tokenizer.id_to_sequence(mem_valid_ids)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7c80f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_id = tokenizer.tokenizer.vocab[\"[CLS]\"]\n",
    "sep_id = tokenizer.tokenizer.vocab[\"[SEP]\"]\n",
    "print(f'cls_id: {cls_id}\\nsep_id: {sep_id}')\n",
    "\n",
    "textdata = Ins.trainset.dataset.textdata\n",
    "depinfo_all = Ins.trainset.dataset.depinfo\n",
    "for sentence, depinfo in zip(textdata, depinfo_all):\n",
    "    text_left, text_right, aspect, polarity = sentence\n",
    "    left_tokens, left_token_ids, left_valid_ids = ws_test(text_left.split(\" \"))\n",
    "    right_tokens, right_token_ids, right_valid_ids = ws_test(text_right.split(\" \"))\n",
    "    aspect_tokens, aspect_token_ids, aspect_valid_ids = ws_test(aspect.split(\" \"))\n",
    "    tokens = left_tokens + aspect_tokens + right_tokens\n",
    "    input_ids = [cls_id] + left_token_ids + aspect_token_ids + right_token_ids + [sep_id] + aspect_token_ids + [sep_id]\n",
    "    valid_ids = [1] + left_valid_ids + aspect_valid_ids + right_valid_ids + [1] + aspect_valid_ids + [1]\n",
    "    mem_valid_ids = [0] + [0] * len(left_tokens) + [1] * len(aspect_tokens) + [0] * len(right_tokens) # aspect terms mask\n",
    "    segment_ids = [0] * (len(tokens) + 2) + [1] * (len(aspect_tokens)+1)\n",
    "    \n",
    "    print(tokens)\n",
    "    print(input_ids)\n",
    "    print(valid_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08400b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "\n",
    "sentence = list(textdata)[i]\n",
    "depinfo = list(depinfo_all)[i]\n",
    "print(sentence)\n",
    "print(depinfo)\n",
    "print(valid_ids)\n",
    "\n",
    "text_left, text_right, aspect, polarity = sentence\n",
    "left_tokens, left_token_ids, left_valid_ids = ws_test(text_left.split(\" \"))\n",
    "right_tokens, right_token_ids, right_valid_ids = ws_test(text_right.split(\" \"))\n",
    "aspect_tokens, aspect_token_ids, aspect_valid_ids = ws_test(aspect.split(\" \"))\n",
    "tokens = left_tokens + aspect_tokens + right_tokens\n",
    "input_ids = [cls_id] + left_token_ids + aspect_token_ids + right_token_ids + [sep_id] + aspect_token_ids + [sep_id]\n",
    "valid_ids = [1] + left_valid_ids + aspect_valid_ids + right_valid_ids + [1] + aspect_valid_ids + [1]\n",
    "mem_valid_ids = [0] + [0] * len(left_tokens) + [1] * len(aspect_tokens) + [0] * len(right_tokens) # aspect terms mask\n",
    "segment_ids = [0] * (len(tokens) + 2) + [1] * (len(aspect_tokens)+1)\n",
    "\n",
    "temp = input_ids[valid_ids == 1]\n",
    "print(input_ids)\n",
    "print(temp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
